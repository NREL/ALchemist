{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#_1","title":"Home","text":"<p>ALchemist: Active Learning Toolkit for Chemical and Materials Research</p> <p>ALchemist is a modular Python toolkit that brings active learning and Bayesian optimization to experimental design in chemical and materials research. It is designed for scientists and engineers who want to efficiently explore or optimize high-dimensional variable spaces\u2014using intuitive graphical interfaces, programmatic APIs, or autonomous optimization workflows.</p> <p>NLR Software Record: SWR-25-102</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li> <p>Flexible variable space definition: Real, integer, and categorical variables with bounds or discrete values.</p> </li> <li> <p>Probabilistic surrogate modeling: Gaussian process regression via BoTorch or scikit-learn backends.</p> </li> <li> <p>Advanced acquisition strategies: Efficient sampling using qEI, qPI, qUCB, and qNegIntegratedPosteriorVariance.</p> </li> <li> <p>Modern web interface: React-based UI with FastAPI backend for seamless active learning workflows.</p> </li> <li> <p>Desktop GUI: CustomTkinter desktop application for offline optimization.</p> </li> <li> <p>Session management: Save/load optimization sessions with audit logs for reproducibility.</p> </li> <li> <p>Multiple interfaces: No-code GUI, Python Session API, or REST API for different use cases.</p> </li> <li> <p>Autonomous optimization: Human-out-of-the-loop operation for real-time process control.</p> </li> <li> <p>Experiment tracking: CSV logging, reproducible random seeds, and comprehensive audit trails.</p> </li> <li> <p>Extensibility: Abstract interfaces for models and acquisition functions enable future backend and workflow expansion.</p> </li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<p>ALchemist is built on a clean, modular architecture:</p> <ul> <li> <p>Core Session API: Headless Bayesian optimization engine (<code>alchemist_core</code>) that powers all interfaces</p> </li> <li> <p>Desktop Application: CustomTkinter GUI using the Core Session API, designed for human-in-the-loop and offline optimization</p> </li> <li> <p>REST API: FastAPI server providing a thin wrapper around the Core Session API for remote access</p> </li> <li> <p>Web Application: React UI consuming the REST API, supporting both interactive and autonomous optimization workflows</p> </li> </ul> <p>Session Compatibility: Optimization sessions are fully interoperable between desktop and web applications. Session files (JSON format) can be created, edited, and loaded in either interface, enabling seamless workflow transitions.</p> <p>Use Cases:</p> <ul> <li> <p>Interactive Optimization: Use desktop or web GUI for manual experiment design and human-in-the-loop optimization</p> </li> <li> <p>Programmatic Workflows: Import the Session API in Python scripts or Jupyter notebooks for batch processing</p> </li> <li> <p>Autonomous Optimization: Use the REST API to integrate ALchemist with automated laboratory equipment for real-time process control</p> </li> <li> <p>Remote Monitoring: Web dashboard provides read-only monitoring mode when ALchemist is being remote-controlled</p> </li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>We recommend using Anaconda to manage your Python environments.</p> <p>Requirements: Python 3.11 or higher</p> <p>1. Create a new environment: </p><pre><code>conda create -n alchemist-env python=3.11\nconda activate alchemist-env\n</code></pre><p></p> <p>2. Install ALchemist:</p> <p>From PyPI (recommended): </p><pre><code>pip install alchemist-nrel\n</code></pre><p></p> <p>This installs the latest stable release with pre-built web application files. Both desktop and web applications are ready to use immediately.</p>"},{"location":"#running-alchemist","title":"Running ALchemist","text":"<p>Web Application: </p><pre><code>alchemist-web\n</code></pre> Open your browser to http://localhost:8000<p></p> <p>Desktop Application: </p><pre><code>alchemist\n</code></pre> The desktop GUI launches directly.<p></p>"},{"location":"#advanced-installation","title":"Advanced Installation","text":""},{"location":"#latest-development-version","title":"Latest Development Version","text":"<p>Desktop app only (GitHub install): </p><pre><code>pip install git+https://github.com/NatLabRockies/ALchemist.git\n</code></pre><p></p> <p>This installs the latest unreleased version. </p> <p>Note: The web application is not available with this method because static build files are not included in the repository.</p> <p>Web and desktop apps (clone and build): </p><pre><code>git clone https://github.com/NatLabRockies/ALchemist.git\ncd ALchemist\npip install -e .\ncd alchemist-web\nnpm install\nnpm run build\n</code></pre><p></p> <p>This builds the web application from source, giving you the latest unreleased version of both apps.</p>"},{"location":"#docker-deployment","title":"Docker Deployment","text":"<p>ALchemist can be deployed as a Docker container for production environments.</p> <p>Prerequisites: Docker Desktop installed and running</p> <p>Quick start: </p><pre><code>git clone https://github.com/NatLabRockies/ALchemist.git\ncd ALchemist/docker\ndocker-compose up -d\n</code></pre><p></p> <p>Access the application:</p> <ul> <li> <p>Web UI: http://localhost:8000/app</p> </li> <li> <p>API Docs: http://localhost:8000/docs</p> </li> <li> <p>Health Check: http://localhost:8000/health</p> </li> </ul> <p>Stop the container: </p><pre><code>docker-compose down\n</code></pre><p></p> <p>Use the sidebar to navigate through the documentation. See Getting Started to define your variable space and generate initial experiments.</p>"},{"location":"ISSUES_LOG/","title":"Troubleshooting","text":""},{"location":"ISSUES_LOG/#issues-troubleshooting-log","title":"Issues &amp; Troubleshooting Log","text":"<p>This log tracks known issues, user-reported bugs, and observations from internal testing for ALchemist. It is maintained by the development team.</p>"},{"location":"ISSUES_LOG/#how-to-report-an-issue","title":"How to Report an Issue","text":"<p>If you encounter a problem or have feedback, please open an issue on GitHub or email ccoatney@nrel.gov with the following information:</p> <ul> <li> <p>Brief description of the issue</p> </li> <li> <p>Steps to reproduce (if applicable)</p> </li> <li> <p>Your operating system and environment</p> </li> <li> <p>Any error messages or screenshots</p> </li> <li> <p>Date observed</p> </li> </ul>"},{"location":"ISSUES_LOG/#known-issues","title":"Known Issues","text":"Issue Date Reported Status Notes / Workarounds None currently - see resolved issues below - - -"},{"location":"ISSUES_LOG/#resolved-issues","title":"Resolved Issues","text":"Issue Date Reported Date Resolved Notes BoTorch kernel hyperparameters not shown in \"Next Point\" dialog 2024-06-16 2025-08-20 \u2705 RESOLVED: Enhanced hyperparameter extraction with recursive kernel traversal. Now properly displays ARD lengthscales, kernel types, noise parameters, and transform information for both SingleTaskGP and MixedSingleTaskGP models. Handles complex AdditiveKernel structures with categorical/continuous variables. GUI not displaying fully on macOS; windows may be cut off 2024-06-16 2025-06-29 Resolved as of latest testing; GUI now displays correctly on Mac without external monitor. Loading variables from CSV does not work; only JSON loads correctly 2025-06-29 2025-07-15 Fixed CSV parsing for integer min/max values and categorical value parsing. Saving variables as CSV and reloading does not restore variables 2025-06-29 2025-07-15 Fixed Integer variable population and main UI update after variable definition. Main UI \"Load Variables\" button fails with JSON error when loading CSV files 2025-07-15 2025-07-15 Fixed load_variables() function to properly detect and parse both JSON and CSV file formats. Categorical variables losing values when editing in variables setup 2025-07-15 2025-07-15 Enhanced categorical editor data filtering and improved Sheet widget data handling. Model Prediction Optimum tool: suggested experiment gives fractional value for integer variable (BoTorch backend) 2025-06-29 2025-07-15 Fixed by implementing integer rounding in optimization results. Note: BoTorch likely has native integer constraints - investigate optimize_acqf with integer_indices parameter for future improvement. Model Prediction Optimum tool: optimizing to maximum or minimum gives same suggested values (BoTorch backend) 2025-06-29 2025-07-15 Fixed by correcting acquisition panel to use find_optimum() method instead of select_next() method. <p>This log is updated as issues are reported and resolved.</p>"},{"location":"use_cases/","title":"Use Cases","text":""},{"location":"use_cases/#when-to-use-alchemist","title":"When to Use ALchemist","text":"<p>ALchemist is designed for researchers and engineers who need to optimize complex systems with expensive experiments, where reducing experimental overhead is critical.</p>"},{"location":"use_cases/#what-alchemist-can-be-used-for","title":"What ALchemist Can Be Used For","text":"<p>ALchemist is ideal when you're optimizing (or modeling) a system with many variables, where each experiment is time-consuming or expensive. It's particularly well-suited for:</p> <ul> <li> <p>Multi-variable optimization: 2-10+ input variables with complex interactions</p> </li> <li> <p>Expensive experiments: Each test costs significant time, money, or resources</p> </li> <li> <p>Black-box systems: You can measure outputs but don't have explicit mathematical models</p> </li> <li> <p>Uncertainty quantification: You need confidence estimates and risk management</p> </li> </ul> <p>Key Strengths:</p> <ul> <li> <p>Tracks your iterations: Complete audit logs and session management for reproducibility</p> </li> <li> <p>Detailed statistical insights: Uncertainty quantification, calibration diagnostics, and model performance metrics</p> </li> <li> <p>No advanced coding required: User-friendly desktop and web interfaces (though programmatic access is available)</p> </li> <li> <p>Free and open source: Unlike commercial tools like JMP, ALchemist is freely available with full source code</p> </li> </ul>"},{"location":"use_cases/#example-application-domains","title":"Example Application Domains","text":"Domain Example Variables Example Objectives Chemical Synthesis Temperature, pressure, catalyst loading, reagent ratios, reaction time Maximize yield, minimize byproducts, optimize selectivity Energy Materials &amp; Batteries Composition (cathode/anode/electrolyte), processing conditions, electrode thickness, charging protocols Maximize energy density, improve cycle life, reduce charging time Materials Discovery Elemental composition, heat treatment schedule, sintering parameters, microstructure Optimize mechanical properties, maximize conductivity, minimize defects Process Manufacturing Operating conditions (temp/flow/pressure), equipment settings, material ratios, timing Maximize throughput, minimize energy use, improve quality, reduce waste Biological Systems Growth conditions (temp/light/humidity), nutrient concentrations, treatment protocols Maximize yield, optimize metabolite production, improve growth rates Engineering Design Design parameters, control tuning (PID gains), calibration parameters, operating set points Minimize settling time, reduce overshoot, improve efficiency, match experimental data"},{"location":"use_cases/#workflow-modes","title":"Workflow Modes","text":""},{"location":"use_cases/#1-interactive-optimization","title":"1. Interactive Optimization","text":"<p>Best for: Exploratory research, learning Bayesian optimization, hands-on control</p> <p>How it works:</p> <ul> <li> <p>Use desktop GUI or web UI to manually review suggestions</p> </li> <li> <p>Visualize progress in real-time</p> </li> <li> <p>Decide when to run experiments</p> </li> </ul> <p>Example workflow: </p><pre><code>1. Define variable space (e.g., 3 variables: temp, pressure, catalyst)\n2. Generate initial experiments (Latin Hypercube sampling)\n3. Run experiments in lab\n4. Load results into ALchemist\n5. Train GP model, view diagnostics\n6. Run acquisition function (Expected Improvement)\n7. Review suggested next experiment\n8. Repeat steps 3-7 until satisfied\n</code></pre><p></p>"},{"location":"use_cases/#2-programmatic-workflows","title":"2. Programmatic Workflows","text":"<p>Best for: Batch processing, integration with analysis pipelines, reproducible research</p> <p>Example code: </p><pre><code>from alchemist_core.session import OptimizationSession\n\n# Create session\nsession = OptimizationSession()\n\n# Define variables\nsession.add_variable(\"temperature\", \"real\", min=200, max=400)\nsession.add_variable(\"pressure\", \"real\", min=1, max=10)\n\n# Load experimental data\nsession.load_experiments(\"data.csv\")\n\n# Train model\nsession.train_model(backend=\"botorch\", kernel=\"matern\")\n\n# Get next suggestion\nnext_point = session.suggest_next(strategy=\"qEI\", goal=\"maximize\")\n\n# Save session\nsession.save_session(\"my_optimization.json\")\n</code></pre><p></p>"},{"location":"use_cases/#3-autonomous-optimization","title":"3. Autonomous Optimization","text":"<p>Best for: Real-time process control, automated laboratory systems, self-driving experiments</p> <p>Example autonomous loop: </p><pre><code>import requests\n\nBASE_URL = \"http://localhost:8000\"\nSESSION_ID = \"abc-123\"\n\nwhile not converged:\n    # Get next experiment from ALchemist\n    response = requests.post(\n        f\"{BASE_URL}/sessions/{SESSION_ID}/acquisition/suggest\",\n        json={\"strategy\": \"qEI\", \"goal\": \"maximize\"}\n    )\n    next_point = response.json()[\"suggestions\"][0]\n\n    # Execute experiment on hardware\n    result = run_experiment_on_reactor(next_point)\n\n    # Send result back to ALchemist\n    requests.post(\n        f\"{BASE_URL}/sessions/{SESSION_ID}/experiments\",\n        params={\"auto_train\": True},\n        json={\"inputs\": next_point, \"output\": result}\n    )\n</code></pre><p></p>"},{"location":"use_cases/#interface-options","title":"Interface Options","text":"<p>ALchemist offers multiple interfaces for different workflows:</p> <ul> <li>Desktop GUI: Offline work, local files, ideal for learning (no coding required)</li> <li>Web UI: Collaboration, remote access, browser-based (no coding required)</li> <li>Python Session API: Batch processing, custom workflows, notebooks (Python scripting)</li> <li>REST API: Real-time control, automation, integration with other systems (API integration)</li> </ul> <p>For a detailed comparison, see the Python API Overview.</p>"},{"location":"use_cases/#getting-started","title":"Getting Started","text":"<p>First-Time Users:</p> <ol> <li>Start with desktop or web GUI for learning</li> <li>Try a simple optimization (2-3 variables, 20-50 experiments)</li> <li>Explore visualizations to understand model behavior</li> </ol> <p>Experienced Users:</p> <ol> <li>Use Session API for scripted workflows</li> <li>Leverage session save/load for reproducibility</li> <li>Consider REST API for automated systems</li> </ol>"},{"location":"use_cases/#next-steps","title":"Next Steps","text":"<ul> <li> <p>Bayesian Optimization Intro - Learn the theory</p> </li> <li> <p>Web UI Guide - Browser-based workflows</p> </li> <li> <p>Session API Guide - Programmatic workflows</p> </li> <li> <p>Variable Space Setup - Define your search space</p> </li> <li> <p>Model Training - Train Gaussian Process models</p> </li> </ul> <p>Questions? Open an issue on GitHub or contact ccoatney@nrel.gov</p>"},{"location":"acquisition/botorch/","title":"BoTorch Acquisition","text":""},{"location":"acquisition/botorch/#botorch-acquisition-functions","title":"BoTorch Acquisition Functions","text":"<p>The BoTorch backend in ALchemist provides a flexible and powerful interface for selecting the next experiment(s) using a variety of acquisition functions from the BoTorch library. This guide explains the available options, how to use them, and what each setting means.</p>"},{"location":"acquisition/botorch/#overview","title":"Overview","text":"<p>The Acquisition panel in ALchemist allows you to:</p> <ul> <li> <p>Choose between Regular, Batch, and Exploratory acquisition strategies.</p> </li> <li> <p>Select from several acquisition functions, each balancing exploration and exploitation in different ways.</p> </li> <li> <p>Customize parameters such as batch size and Monte Carlo integration points.</p> </li> <li> <p>Run the selected strategy to suggest the next experiment(s) based on your trained model.</p> </li> </ul>"},{"location":"acquisition/botorch/#important-note","title":"Important Note","text":"<p>You must first train your model using the BoTorch backend before running any BoTorch acquisition functions. See BoTorch Backend for details on model training.</p>"},{"location":"acquisition/botorch/#acquisition-types","title":"Acquisition Types","text":""},{"location":"acquisition/botorch/#1-regular-acquisition","title":"1. Regular Acquisition","text":"<ul> <li> <p>Expected Improvement (EI):   Suggests points with the highest expected improvement over the current best observed value.</p> </li> <li> <p>Log Expected Improvement (LogEI):   Numerically stable version of EI.</p> </li> <li> <p>Probability of Improvement (PI):   Selects points with the highest probability of improving over the current best value.</p> </li> <li> <p>Log Probability of Improvement (LogPI):   Numerically stable version of PI.</p> </li> <li> <p>Upper Confidence Bound (UCB):   Balances exploration and exploitation by selecting points with the highest upper confidence bound.</p> </li> </ul> <p>Customization: </p> <ul> <li>Choose to maximize or minimize your objective.</li> </ul>"},{"location":"acquisition/botorch/#2-batch-acquisition","title":"2. Batch Acquisition","text":"<ul> <li> <p>q-Expected Improvement (qEI):   Selects a batch of points that together maximize expected improvement.</p> </li> <li> <p>q-Upper Confidence Bound (qUCB):   Batch version of UCB.</p> </li> </ul> <p>Customization: </p> <ul> <li> <p>Set batch size (number of points to suggest at once, q).</p> </li> <li> <p>Monte Carlo samples (mc_samples) are used internally for batch methods.</p> </li> </ul>"},{"location":"acquisition/botorch/#3-exploratory-acquisition","title":"3. Exploratory Acquisition","text":"<ul> <li>Integrated Posterior Variance (qNIPV):   Selects points to reduce overall model uncertainty, focusing on exploration rather than optimization.</li> </ul> <p>Customization: </p> <ul> <li>Set the number of Monte Carlo integration points (higher values improve accuracy but increase computation time; 500\u20132000 is typical).</li> </ul>"},{"location":"acquisition/botorch/#how-to-use","title":"How to Use","text":"<ol> <li> <p>Train Model:    Train your model using the BoTorch backend. See BoTorch Backend for instructions.</p> </li> <li> <p>Open Acquisition Panel:    Go to the Acquisition panel. The BoTorch options will appear automatically.</p> </li> <li> <p>Choose Acquisition Type:    Use the segmented button to select Regular, Batch, or Exploratory.</p> </li> <li> <p>Configure Options: </p> </li> <li>Select the acquisition function from the dropdown.</li> <li>Adjust parameters (batch size, MC points) as needed.</li> <li> <p>Choose whether to maximize or minimize.</p> </li> <li> <p>Run Acquisition:    Click Run Acquisition Strategy to suggest the next experiment(s). Results, including predicted value and uncertainty, will be shown in a notification window and highlighted in the data table and plots.</p> </li> </ol>"},{"location":"acquisition/botorch/#model-optimum-finder","title":"Model Optimum Finder","text":"<p>In addition to acquisition functions, you can use the Model Prediction Optimum tool to find the point where the model predicts the best value (maximum or minimum). Note: This does not balance exploration and exploitation\u2014it simply finds the model's optimum prediction.</p>"},{"location":"acquisition/botorch/#tips-notes","title":"Tips &amp; Notes","text":"<ul> <li> <p>Batch Acquisition: Use batch mode to suggest multiple experiments at once, useful for parallel experimentation.</p> </li> <li> <p>Exploratory Mode: Use qNIPV when you want to reduce model uncertainty rather than optimize the objective.</p> </li> <li> <p>Parameter Tuning: Increase MC points for more accurate but slower exploratory acquisition.</p> </li> <li> <p>Publication Quality: All results and suggested points are integrated with ALchemist's visualization tools for easy analysis and export.</p> </li> </ul> <p>For more details on the underlying algorithms, see the BoTorch documentation</p>"},{"location":"acquisition/logging/","title":"Logging & Tracking","text":""},{"location":"acquisition/logging/#logging-tracking","title":"Logging &amp; Tracking","text":"<p>ALchemist provides robust logging and notification features to help you keep track of your optimization workflow, model settings, and acquisition strategies. This is essential for reproducibility, transparency, and effective reporting in scientific research.</p>"},{"location":"acquisition/logging/#notifications-immediate-feedback-for-each-acquisition","title":"Notifications: Immediate Feedback for Each Acquisition","text":"<p>Whenever you execute an acquisition strategy (e.g., suggest the next experiment), a notification window will automatically pop up. This window provides a detailed summary of the result, including:</p> <ul> <li> <p>Point Details:   The coordinates of the suggested next experiment (or batch of experiments), along with predicted values, uncertainties, and 95% confidence intervals.</p> </li> <li> <p>Model Info:   Information about the trained model, including backend (BoTorch or scikit-optimize), kernel type, learned hyperparameters, and recent performance metrics (e.g., RMSE, R\u00b2).</p> </li> <li> <p>Strategy Info:   Details about the acquisition strategy used, including the type (e.g., Expected Improvement, qEI), optimization goal (maximize/minimize), and any relevant parameters.</p> </li> <li> <p>Export Options:   You can export all of this information to a CSV file for record-keeping, publication, or further analysis.</p> </li> </ul> <p>This notification system ensures you always have a clear record of what was suggested and why.</p>"},{"location":"acquisition/logging/#logging-keeping-a-complete-experiment-record","title":"Logging: Keeping a Complete Experiment Record","text":"<p>ALchemist includes an experiment logger that tracks every key step in your workflow:</p> <ul> <li> <p>Model Training:   Logs backend, kernel, hyperparameters, and performance metrics each time you train or update your model.</p> </li> <li> <p>Acquisition Strategies:   Logs the details of every acquisition function execution, including the strategy, parameters, suggested points, predicted values, and uncertainties.</p> </li> <li> <p>Experiment Data:   Logs the current state of your experimental dataset, including variable names and summary statistics.</p> </li> <li> <p>Exported Logs:   All logs are saved in timestamped files in the <code>logs/</code> directory, making it easy to revisit or share your workflow history.</p> </li> </ul>"},{"location":"acquisition/logging/#best-practices","title":"Best Practices","text":"<ul> <li> <p>Log Every Iteration:   After each acquisition and model update, use the export and logging features to save your results. This ensures you can always trace back which model, kernel, and acquisition strategy led to each experimental suggestion.</p> </li> <li> <p>Reproducibility:   Keeping detailed logs allows you (and others) to reproduce your optimization process, which is essential for scientific rigor and publication.</p> </li> <li> <p>Transparency:   By exporting notifications and logs, you can clearly communicate your workflow, decisions, and results to collaborators, reviewers, or future users.</p> </li> </ul>"},{"location":"acquisition/logging/#summary","title":"Summary","text":"<p>ALchemist\u2019s notification and logging system is designed to make your active learning workflow transparent, reproducible, and easy to document. Make it a habit to export and log your results at each step\u2014this will pay dividends when you need to report, troubleshoot, or publish your work.</p>"},{"location":"acquisition/skopt/","title":"scikit-optimize Acquisition","text":""},{"location":"acquisition/skopt/#scikit-optimize-acquisition-functions","title":"scikit-optimize Acquisition Functions","text":"<p>The scikit-optimize (skopt) backend in ALchemist provides a range of acquisition functions for Bayesian optimization using the scikit-optimize library. This guide explains the available options, how to use them, and what each setting means.</p>"},{"location":"acquisition/skopt/#overview","title":"Overview","text":"<p>The Acquisition panel in ALchemist allows you to:</p> <ul> <li> <p>Choose from several acquisition functions, each balancing exploration and exploitation in different ways.</p> </li> <li> <p>Customize parameters such as exploration/exploitation trade-offs.</p> </li> <li> <p>Run the selected strategy to suggest the next experiment based on your trained model.</p> </li> </ul>"},{"location":"acquisition/skopt/#important-note","title":"Important Note","text":"<p>You must first train your model using the scikit-optimize backend before running any skopt acquisition functions. See scikit-optimize Backend for details on model training.</p>"},{"location":"acquisition/skopt/#acquisition-functions","title":"Acquisition Functions","text":"<ul> <li> <p>Expected Improvement (EI):   Balances exploration and exploitation by selecting points with the highest expected improvement over the current best value. Parameter: \u03be (xi) \u2014 higher values favor exploration.</p> </li> <li> <p>Upper Confidence Bound (UCB):   Selects points with the highest upper confidence bound, balancing exploration and exploitation. Parameter: \u03ba (kappa) \u2014 higher values increase exploration.</p> </li> <li> <p>Probability of Improvement (PI):   Selects points with the highest probability of improving over the current best value. Parameter: \u03be (xi) \u2014 higher values favor exploration.</p> </li> <li> <p>GP Hedge (Auto-balance):   Automatically balances between EI, UCB, and PI by adaptively selecting the best-performing strategy during optimization. Parameters: \u03be (xi) and \u03ba (kappa).</p> </li> </ul> <p>Customization: </p> <ul> <li> <p>Choose to maximize or minimize your objective.</p> </li> <li> <p>Adjust \u03be (xi) and \u03ba (kappa) parameters using sliders as appropriate for the selected acquisition function.</p> </li> </ul>"},{"location":"acquisition/skopt/#how-to-use","title":"How to Use","text":"<ol> <li> <p>Train Model:    Train your model using the scikit-optimize backend. See scikit-optimize Backend for instructions.</p> </li> <li> <p>Open Acquisition Panel:    Go to the Acquisition panel. The scikit-optimize options will appear automatically.</p> </li> <li> <p>Select Acquisition Function:    Use the dropdown menu to select the acquisition function (EI, UCB, PI, or GP Hedge).</p> </li> <li> <p>Configure Options: </p> </li> <li>Adjust \u03be (xi) and \u03ba (kappa) parameters as needed.</li> <li> <p>Choose whether to maximize or minimize.</p> </li> <li> <p>Run Acquisition:    Click Run Acquisition Strategy to suggest the next experiment. Results, including predicted value and uncertainty, will be shown in a notification window and highlighted in the data table and plots.</p> </li> </ol>"},{"location":"acquisition/skopt/#model-optimum-finder","title":"Model Optimum Finder","text":"<p>In addition to acquisition functions, you can use the Model Prediction Optimum tool to find the point where the model predicts the best value (maximum or minimum). Note: This does not balance exploration and exploitation\u2014it simply finds the model's optimum prediction.</p>"},{"location":"acquisition/skopt/#tips-notes","title":"Tips &amp; Notes","text":"<ul> <li>Parameter Tuning: </li> <li>Increase \u03be (xi) or \u03ba (kappa) for more exploration; decrease for more exploitation.</li> <li> <p>GP Hedge is useful if you are unsure which acquisition function to use.</p> </li> <li> <p>Publication Quality:   All results and suggested points are integrated with ALchemist's visualization tools for easy analysis and export.</p> </li> </ul> <p>For more details on the underlying algorithms,</p>"},{"location":"api/acquisition/","title":"Acquisition","text":""},{"location":"api/acquisition/#acquisition-functions","title":"Acquisition Functions","text":"<p>Acquisition functions guide the selection of next experiments in Bayesian optimization. They balance exploration (reducing uncertainty) and exploitation (targeting optimal regions).</p>"},{"location":"api/acquisition/#botorch-acquisition","title":"BoTorch Acquisition","text":"<p>PyTorch-based acquisition functions with batch support.</p>"},{"location":"api/acquisition/#alchemist_core.acquisition.botorch_acquisition.BoTorchAcquisition","title":"<code>alchemist_core.acquisition.botorch_acquisition.BoTorchAcquisition(search_space, model=None, acq_func='ucb', maximize=True, random_state=42, acq_func_kwargs=None, batch_size=1, ref_point=None, directions=None, objective_names=None, outcome_constraints=None)</code>","text":"<p>               Bases: <code>BaseAcquisition</code></p> <p>Acquisition function implementation using BoTorch.</p> <p>Supported acquisition functions: - 'ei': Expected Improvement - 'logei': Log Expected Improvement (numerically stable) - 'pi': Probability of Improvement - 'logpi': Log Probability of Improvement (numerically stable) - 'ucb': Upper Confidence Bound - 'qei': Batch Expected Improvement (for q&gt;1) - 'qucb': Batch Upper Confidence Bound (for q&gt;1) - 'qipv' or 'qnipv': q-Negative Integrated Posterior Variance (exploratory)</p> <p>Initialize the BoTorch acquisition function.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <p>The search space (SearchSpace object)</p> required <code>model</code> <p>A trained model (BoTorchModel)</p> <code>None</code> <code>acq_func</code> <p>Acquisition function type (see class docstring for options)</p> <code>'ucb'</code> <code>maximize</code> <p>Whether to maximize (True) or minimize (False) the objective</p> <code>True</code> <code>random_state</code> <p>Random state for reproducibility</p> <code>42</code> <code>acq_func_kwargs</code> <p>Dictionary of additional arguments for the acquisition function</p> <code>None</code> <code>batch_size</code> <p>Number of points to select at once (q)</p> <code>1</code> <code>ref_point</code> <p>Reference point for MOBO hypervolume (list of floats)</p> <code>None</code> <code>directions</code> <p>Per-objective direction list ('maximize'/'minimize')</p> <code>None</code> <code>objective_names</code> <p>List of objective names (for multi-objective)</p> <code>None</code> <code>outcome_constraints</code> <p>List of outcome constraint callables for MOBO</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If acq_func is not a valid acquisition function name</p>"},{"location":"api/acquisition/#alchemist_core.acquisition.botorch_acquisition.BoTorchAcquisition.select_next","title":"<code>select_next(candidate_points=None)</code>","text":"<p>Suggest the next experiment point(s) using BoTorch optimization.</p> <p>Parameters:</p> Name Type Description Default <code>candidate_points</code> <p>Candidate points to evaluate (optional)</p> <code>None</code> <p>Returns:</p> Type Description <p>Dictionary with the selected point or list of points</p>"},{"location":"api/acquisition/#alchemist_core.acquisition.botorch_acquisition.BoTorchAcquisition.find_optimum","title":"<code>find_optimum(model=None, maximize=None, random_state=None)</code>","text":"<p>Find the point where the model predicts the optimal value.</p> <p>This uses the same approach as regret plot predictions: generate a grid in the original variable space, predict using the model's standard pipeline, and find the argmax/argmin. This ensures categorical variables are handled correctly through proper encoding/decoding.</p>"},{"location":"api/acquisition/#skopt-acquisition","title":"Skopt Acquisition","text":"<p>Scikit-optimize based acquisition functions.</p>"},{"location":"api/acquisition/#alchemist_core.acquisition.skopt_acquisition.SkoptAcquisition","title":"<code>alchemist_core.acquisition.skopt_acquisition.SkoptAcquisition(search_space, model=None, acq_func='ei', maximize=True, random_state=42, acq_func_kwargs=None)</code>","text":"<p>               Bases: <code>BaseAcquisition</code></p> <p>Simple acquisition function implementation using scikit-optimize (skopt).</p> <p>Supported acquisition functions: - 'ei' or 'EI': Expected Improvement - 'pi' or 'PI': Probability of Improvement - 'ucb' or 'UCB': Upper Confidence Bound (mapped to LCB in skopt) - 'gp_hedge': GP-Hedge (portfolio of acquisition functions)</p> <p>Initialize the acquisition function.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <p>The search space (SearchSpace object or list of skopt dimensions)</p> required <code>model</code> <p>A trained model (SklearnModel or compatible)</p> <code>None</code> <code>acq_func</code> <p>Acquisition function ('ei', 'pi', 'ucb', or 'gp_hedge')</p> <code>'ei'</code> <code>maximize</code> <p>Whether to maximize (True) or minimize (False) the objective</p> <code>True</code> <code>random_state</code> <p>Random state for reproducibility</p> <code>42</code> <code>acq_func_kwargs</code> <p>Dictionary of additional arguments for the acquisition function</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If acq_func is not a valid acquisition function name</p>"},{"location":"api/acquisition/#alchemist_core.acquisition.skopt_acquisition.SkoptAcquisition.select_next","title":"<code>select_next(candidate_points=None, **kwargs)</code>","text":"<p>Suggest the next experiment point.</p>"},{"location":"api/acquisition/#alchemist_core.acquisition.skopt_acquisition.SkoptAcquisition.find_optimum","title":"<code>find_optimum(model, maximize=True, random_state=42)</code>","text":"<p>Find the point where the model predicts the optimal value.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Trained model with predict method</p> required <code>maximize</code> <p>Whether to maximize (True) or minimize (False) the objective</p> <code>True</code> <code>random_state</code> <p>Random seed for reproducibility</p> <code>42</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Contains 'x_opt' (optimal point), 'value' (predicted value),    'std' (standard deviation at optimum)</p>"},{"location":"api/acquisition/#available-strategies","title":"Available Strategies","text":""},{"location":"api/acquisition/#single-point-strategies","title":"Single-Point Strategies","text":"Strategy Code Description Best For Expected Improvement <code>'EI'</code> Balances exploration/exploitation General use Log EI <code>'LogEI'</code> Numerically stable EI Noisy data Probability of Improvement <code>'PI'</code> Conservative improvement Risk-averse Log PI <code>'LogPI'</code> Numerically stable PI Noisy data Upper Confidence Bound <code>'UCB'</code> More exploratory Unknown spaces"},{"location":"api/acquisition/#batch-strategies","title":"Batch Strategies","text":"Strategy Code Description Best For Batch Expected Improvement <code>'qEI'</code> Parallel experiments Lab workflows Batch UCB <code>'qUCB'</code> Parallel + exploratory Exploration Negative Integrated Posterior Variance <code>'qNIPV'</code> Pure exploration Model improvement"},{"location":"api/acquisition/#see-also","title":"See Also","text":"<ul> <li>OptimizationSession - High-level acquisition interface</li> <li>BoTorch Acquisition Guide - Detailed strategy explanations</li> <li>Models - Model training for acquisition</li> </ul>"},{"location":"api/models/","title":"Models","text":""},{"location":"api/models/#models","title":"Models","text":"<p>Gaussian Process regression models for Bayesian optimization. ALchemist supports two backends: BoTorch (PyTorch-based) and scikit-learn.</p>"},{"location":"api/models/#botorch-model","title":"BoTorch Model","text":"<p>PyTorch-based Gaussian Process implementation with GPU support and advanced features.</p>"},{"location":"api/models/#alchemist_core.models.botorch_model.BoTorchModel","title":"<code>alchemist_core.models.botorch_model.BoTorchModel(training_iter=50, random_state=42, kernel_options=None, cat_dims=None, search_space=None, input_transform_type='none', output_transform_type='none')</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Initialize the BoTorchModel with custom options.</p> <p>Parameters:</p> Name Type Description Default <code>training_iter</code> <p>Maximum iterations for model optimization.</p> <code>50</code> <code>random_state</code> <p>Random seed for reproducibility.</p> <code>42</code> <code>kernel_options</code> <code>dict</code> <p>Dictionary with kernel options like \"cont_kernel_type\" and \"matern_nu\".</p> <code>None</code> <code>cat_dims</code> <code>list[int] | None</code> <p>List of column indices that are categorical.</p> <code>None</code> <code>search_space</code> <code>list</code> <p>Optional search space list.</p> <code>None</code> <code>input_transform_type</code> <code>str</code> <p>Type of input scaling (\"none\", \"normalize\", \"standardize\")</p> <code>'none'</code> <code>output_transform_type</code> <code>str</code> <p>Type of output scaling (\"none\", \"standardize\")</p> <code>'none'</code>"},{"location":"api/models/#alchemist_core.models.botorch_model.BoTorchModel.train","title":"<code>train(exp_manager, **kwargs)</code>","text":"<p>Train the model using an ExperimentManager instance.</p>"},{"location":"api/models/#alchemist_core.models.botorch_model.BoTorchModel.predict","title":"<code>predict(X, return_std=False, **kwargs)</code>","text":"<p>Make predictions using the trained model.</p>"},{"location":"api/models/#alchemist_core.models.botorch_model.BoTorchModel.get_hyperparameters","title":"<code>get_hyperparameters()</code>","text":"<p>Get model hyperparameters.</p>"},{"location":"api/models/#sklearn-model","title":"Sklearn Model","text":"<p>Scikit-learn based Gaussian Process implementation for CPU-only workflows.</p>"},{"location":"api/models/#alchemist_core.models.sklearn_model.SklearnModel","title":"<code>alchemist_core.models.sklearn_model.SklearnModel(kernel_options, n_restarts_optimizer=30, random_state=42, optimizer='L-BFGS-B', input_transform_type='none', output_transform_type='none')</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Initialize the SklearnModel with kernel options and scaling transforms.</p> <p>Parameters:</p> Name Type Description Default <code>kernel_options</code> <code>dict</code> <p>Dictionary with keys: - \"kernel_type\": one of \"RBF\", \"Matern\", \"RationalQuadratic\"  - If \"Matern\" is selected, a key \"matern_nu\" should be provided.</p> required <code>n_restarts_optimizer</code> <p>Number of restarts for the optimizer.</p> <code>30</code> <code>random_state</code> <p>Random state for reproducibility.</p> <code>42</code> <code>optimizer</code> <p>Optimization method for hyperparameter tuning.</p> <code>'L-BFGS-B'</code> <code>input_transform_type</code> <code>str</code> <p>Type of input scaling (\"none\", \"standard\", \"minmax\", \"robust\")</p> <code>'none'</code> <code>output_transform_type</code> <code>str</code> <p>Type of output scaling (\"none\", \"standard\")</p> <code>'none'</code>"},{"location":"api/models/#alchemist_core.models.sklearn_model.SklearnModel.train","title":"<code>train(experiment_manager, **kwargs)</code>","text":"<p>Train the model using the ExperimentManager.</p>"},{"location":"api/models/#alchemist_core.models.sklearn_model.SklearnModel.predict","title":"<code>predict(X, return_std=False, **kwargs)</code>","text":"<p>Make predictions using the trained model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>Input features</p> required <code>return_std</code> <p>Whether to return standard deviations</p> <code>False</code> <p>Returns:</p> Type Description <p>If return_std is False: numpy array of predictions</p> <p>If return_std is True: tuple of (predictions, standard deviations)</p>"},{"location":"api/models/#alchemist_core.models.sklearn_model.SklearnModel.get_hyperparameters","title":"<code>get_hyperparameters()</code>","text":""},{"location":"api/models/#see-also","title":"See Also","text":"<ul> <li>OptimizationSession - High-level model training interface</li> <li>BoTorch Guide - Detailed BoTorch configuration</li> <li>Sklearn Guide - Detailed sklearn configuration</li> </ul>"},{"location":"api/python_overview/","title":"Overview","text":""},{"location":"api/python_overview/#python-api-overview","title":"Python API Overview","text":"<p>The ALchemist Python API (<code>alchemist_core</code>) is the core library for Bayesian optimization in Python. Import it directly in scripts, Jupyter notebooks, or Python applications for offline, programmatic optimization workflows.</p>"},{"location":"api/python_overview/#python-api-vs-rest-api-vs-webdesktop-app","title":"Python API vs REST API vs Web/Desktop App","text":"Feature Python API REST API Web/Desktop App Interface Python import HTTP requests Browser/GUI Use Case Scripts, notebooks Remote access, web apps No-code interface Requires Server No Yes Yes (web) / No (desktop) Language Python only Any (HTTP) N/A Offline Yes No Desktop only Best For Automation, analysis Integration, remote Interactive exploration"},{"location":"api/python_overview/#core-classes","title":"Core Classes","text":"<p>The Python API is organized around these main classes:</p> <p>Orchestration: - OptimizationSession - Main class coordinating all optimization workflows</p> <p>Data &amp; Space: - SearchSpace - Variable space definition and management - ExperimentManager - Experimental data storage</p> <p>Modeling: - BoTorchModel - PyTorch-based Gaussian Process models - SklearnModel - Scikit-learn Gaussian Process models</p> <p>Acquisition: - BoTorchAcquisition - BoTorch acquisition functions - SkoptAcquisition - Scikit-optimize acquisition functions</p>"},{"location":"api/python_overview/#quick-example","title":"Quick Example","text":"<pre><code>from alchemist_core import OptimizationSession\n\nsession = OptimizationSession()\nsession.add_variable('temperature', 'real', bounds=(20, 100))\nsession.add_variable('pressure', 'real', bounds=(1, 10))\n\n# Generate design, add data, train, optimize\npoints = session.generate_initial_design('lhs', n_points=15)\n# ... run experiments and add results ...\nsession.train_model(backend='botorch', kernel='Matern')\ncandidates = session.suggest_next(strategy='EI', n_suggestions=5, goal='maximize')\n</code></pre> <p>For complete workflows and detailed examples, see the OptimizationSession API Reference.</p>"},{"location":"api/python_overview/#documentation-structure","title":"Documentation Structure","text":"<ul> <li>OptimizationSession - Auto-generated API reference from docstrings</li> <li>SearchSpace - Variable management reference</li> <li>Models - Gaussian Process model reference</li> <li>Acquisition - Acquisition function reference</li> </ul>"},{"location":"api/python_overview/#key-differences-from-rest-api","title":"Key Differences from REST API","text":"Aspect Python API REST API Import <code>from alchemist_core import OptimizationSession</code> Not applicable Session Python object in memory Server-side, accessed by ID Method Call <code>session.add_variable(...)</code> <code>POST /sessions/{id}/variables</code> Return Type Python objects (DataFrame, dict) JSON over HTTP Concurrency Single process Multi-client via HTTP"},{"location":"api/rest/","title":"Overview","text":""},{"location":"api/rest/#rest-api-reference","title":"REST API Reference","text":"<p>The ALchemist REST API provides HTTP access to the Core Session API, enabling language-agnostic integration with Bayesian optimization workflows. Built with FastAPI, the API offers interactive documentation, type validation, and WebSocket support for real-time updates.</p> <p>Python API vs REST API</p> <ul> <li>Python API: Direct Python import (<code>from alchemist_core import OptimizationSession</code>) for notebooks and scripts. See Python API docs</li> <li>REST API: HTTP endpoints for web applications, remote access, and non-Python clients (this page)</li> </ul>"},{"location":"api/rest/#overview","title":"Overview","text":"<p>Base URL: <code>http://localhost:8000/api/v1</code> (default) API Version: v1 Content Type: <code>application/json</code> Interactive Docs: http://localhost:8000/api/docs (Swagger UI) Alternative Docs: http://localhost:8000/api/redoc (ReDoc) Web Application: http://localhost:8000</p> <p>Architecture: </p><pre><code>HTTP Client \u2192 FastAPI REST API \u2192 Core Session API \u2192 BoTorch/Sklearn\n</code></pre><p></p>"},{"location":"api/rest/#getting-started","title":"Getting Started","text":""},{"location":"api/rest/#starting-the-api-server","title":"Starting the API Server","text":"<p>Development mode (auto-reload): </p><pre><code>python -m api.run_api\n</code></pre><p></p> <p>Production mode (with uvicorn): </p><pre><code>uvicorn api.main:app --host 0.0.0.0 --port 8000 --workers 4\n</code></pre><p></p> <p>With custom settings: </p><pre><code>HOST=0.0.0.0 PORT=8080 python -m api.run_api\n</code></pre><p></p>"},{"location":"api/rest/#testing-the-api","title":"Testing the API","text":"<p>Browser:</p> <ul> <li> <p>API Documentation: http://localhost:8000/docs</p> </li> <li> <p>Web Application: http://localhost:8000/app</p> </li> <li> <p>Use interactive Swagger UI to test endpoints</p> </li> </ul> <p>cURL: </p><pre><code># Health check\ncurl http://localhost:8000/health\n\n# Create session\ncurl -X POST http://localhost:8000/api/v1/sessions \\\n  -H \"Content-Type: application/json\"\n</code></pre><p></p> <p>Python: </p><pre><code>import requests\n\nBASE_URL = \"http://localhost:8000/api/v1\"\n\n# Health check\nresponse = requests.get(\"http://localhost:8000/health\")\nprint(response.json())  # {\"status\": \"healthy\"}\n\n# Create session\nresponse = requests.post(f\"{BASE_URL}/sessions\")\nsession_id = response.json()[\"session_id\"]\nprint(f\"Created session: {session_id}\")\n</code></pre><p></p>"},{"location":"api/rest/#authentication","title":"Authentication","text":"<p>Current version: No authentication (local development)</p> <p>Future versions: Support for:</p> <ul> <li> <p>API keys (header-based)</p> </li> <li> <p>OAuth2 (for web deployment)</p> </li> <li> <p>JWT tokens</p> </li> </ul> <p>For production: Use reverse proxy (nginx) with authentication layer</p>"},{"location":"api/rest/#common-patterns","title":"Common Patterns","text":""},{"location":"api/rest/#error-handling","title":"Error Handling","text":"<p>HTTP Status Codes:</p> Code Meaning When Used 200 OK Successful GET/PATCH request 201 Created Successful POST creating resource 204 No Content Successful DELETE 400 Bad Request Invalid input data 404 Not Found Session or resource doesn't exist 422 Unprocessable Entity Validation error (Pydantic) 500 Internal Server Error Server-side error <p>Error Response Format: </p><pre><code>{\n  \"detail\": \"Session abc123 not found\"\n}\n</code></pre><p></p> <p>Validation Error (422): </p><pre><code>{\n  \"detail\": [\n    {\n      \"loc\": [\"body\", \"temperature\"],\n      \"msg\": \"field required\",\n      \"type\": \"value_error.missing\"\n    }\n  ]\n}\n</code></pre><p></p>"},{"location":"api/rest/#pagination","title":"Pagination","text":"<p>Large result sets (experiments, sessions):</p> <p>Request: </p><pre><code>GET /sessions/{session_id}/experiments?skip=0&amp;limit=50\n</code></pre><p></p> <p>Parameters:</p> <ul> <li> <p><code>skip</code>: Number of items to skip (default: 0)</p> </li> <li> <p><code>limit</code>: Maximum items to return (default: 100, max: 1000)</p> </li> </ul> <p>Response: </p><pre><code>{\n  \"items\": [...],\n  \"total\": 500,\n  \"skip\": 0,\n  \"limit\": 50\n}\n</code></pre><p></p>"},{"location":"api/rest/#filtering-and-sorting","title":"Filtering and Sorting","text":"<p>Experiments endpoint: </p><pre><code>GET /sessions/{session_id}/experiments?sort_by=yield&amp;order=desc\n</code></pre><p></p> <p>Parameters:</p> <ul> <li> <p><code>sort_by</code>: Column name to sort by</p> </li> <li> <p><code>order</code>: <code>asc</code> or <code>desc</code></p> </li> <li> <p><code>filter</code>: JSON filter expression (advanced)</p> </li> </ul>"},{"location":"api/rest/#sessions-api","title":"Sessions API","text":""},{"location":"api/rest/#create-session","title":"Create Session","text":"<p>Endpoint: <code>POST /sessions</code></p> <p>Purpose: Create a new optimization session</p> <p>Request Body (optional): </p><pre><code>{\n  \"metadata\": {\n    \"name\": \"Catalyst Screening\",\n    \"description\": \"Pd catalyst optimization\",\n    \"tags\": [\"catalysis\", \"suzuki\"]\n  }\n}\n</code></pre><p></p> <p>Response (201 Created): </p><pre><code>{\n  \"session_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"created_at\": \"2025-12-12T10:00:00Z\",\n  \"expires_at\": \"2025-12-13T10:00:00Z\"\n}\n</code></pre><p></p> <p>Example: </p><pre><code>response = requests.post(f\"{BASE_URL}/sessions\")\nsession_id = response.json()[\"session_id\"]\n</code></pre><p></p>"},{"location":"api/rest/#get-session-details","title":"Get Session Details","text":"<p>Endpoint: <code>GET /sessions/{session_id}</code></p> <p>Purpose: Get complete session information</p> <p>Response (200 OK): </p><pre><code>{\n  \"session_id\": \"550e8400-...\",\n  \"metadata\": {...},\n  \"search_space\": {\n    \"n_variables\": 3,\n    \"variables\": [...]\n  },\n  \"experiments\": {\n    \"count\": 25,\n    \"target_column\": \"yield\"\n  },\n  \"model\": {\n    \"trained\": true,\n    \"backend\": \"botorch\",\n    \"kernel\": \"Matern\"\n  }\n}\n</code></pre><p></p>"},{"location":"api/rest/#save-session","title":"Save Session","text":"<p>Endpoint: <code>POST /sessions/{session_id}/save</code></p> <p>Purpose: Persist session to server-side storage</p> <p>Response (200 OK): </p><pre><code>{\n  \"message\": \"Session persisted to server storage\"\n}\n</code></pre><p></p>"},{"location":"api/rest/#export-session","title":"Export Session","text":"<p>Endpoint: <code>GET /sessions/{session_id}/export</code></p> <p>Purpose: Download session as JSON file</p> <p>Response: JSON file with <code>Content-Disposition: attachment</code></p> <p>Example: </p><pre><code>response = requests.get(f\"{BASE_URL}/sessions/{session_id}/export\")\nwith open('my_session.json', 'wb') as f:\n    f.write(response.content)\n</code></pre><p></p>"},{"location":"api/rest/#import-session","title":"Import Session","text":"<p>Endpoint: <code>POST /sessions/import</code></p> <p>Purpose: Upload and create session from JSON file</p> <p>Request: Multipart form data with file</p> <p>Response (201 Created): </p><pre><code>{\n  \"session_id\": \"new-session-id\",\n  \"created_at\": \"2025-12-12T10:00:00Z\",\n  \"expires_at\": \"2025-12-13T10:00:00Z\"\n}\n</code></pre><p></p>"},{"location":"api/rest/#delete-session","title":"Delete Session","text":"<p>Endpoint: <code>DELETE /sessions/{session_id}</code></p> <p>Purpose: Remove session from memory</p> <p>Response (204 No Content)</p>"},{"location":"api/rest/#variables-api","title":"Variables API","text":""},{"location":"api/rest/#add-variable","title":"Add Variable","text":"<p>Endpoint: <code>POST /sessions/{session_id}/variables</code></p> <p>Purpose: Add variable to search space</p> <p>Request Body (continuous): </p><pre><code>{\n  \"name\": \"temperature\",\n  \"type\": \"real\",\n  \"bounds\": [20.0, 100.0],\n  \"unit\": \"\u00b0C\"\n}\n</code></pre><p></p> <p>Request Body (categorical): </p><pre><code>{\n  \"name\": \"solvent\",\n  \"type\": \"categorical\",\n  \"categories\": [\"THF\", \"DMF\", \"toluene\"]\n}\n</code></pre><p></p> <p>Response (201 Created): </p><pre><code>{\n  \"name\": \"temperature\",\n  \"type\": \"real\",\n  \"bounds\": [20.0, 100.0],\n  \"unit\": \"\u00b0C\"\n}\n</code></pre><p></p> <p>Example: </p><pre><code># Add continuous variable\nrequests.post(\n    f\"{BASE_URL}/sessions/{session_id}/variables\",\n    json={\n        \"name\": \"temperature\",\n        \"type\": \"real\",\n        \"bounds\": [20, 100],\n        \"unit\": \"\u00b0C\"\n    }\n)\n\n# Add categorical variable\nrequests.post(\n    f\"{BASE_URL}/sessions/{session_id}/variables\",\n    json={\n        \"name\": \"solvent\",\n        \"type\": \"categorical\",\n        \"categories\": [\"THF\", \"DMF\", \"toluene\"]\n    }\n)\n</code></pre><p></p>"},{"location":"api/rest/#get-all-variables","title":"Get All Variables","text":"<p>Endpoint: <code>GET /sessions/{session_id}/variables</code></p> <p>Response (200 OK): </p><pre><code>{\n  \"variables\": [\n    {\n      \"name\": \"temperature\",\n      \"type\": \"real\",\n      \"bounds\": [20.0, 100.0],\n      \"unit\": \"\u00b0C\"\n    },\n    {\n      \"name\": \"solvent\",\n      \"type\": \"categorical\",\n      \"categories\": [\"THF\", \"DMF\", \"toluene\"]\n    }\n  ],\n  \"count\": 2\n}\n</code></pre><p></p>"},{"location":"api/rest/#delete-variable","title":"Delete Variable","text":"<p>Endpoint: <code>DELETE /sessions/{session_id}/variables/{variable_name}</code></p> <p>Response (204 No Content)</p>"},{"location":"api/rest/#experiments-api","title":"Experiments API","text":""},{"location":"api/rest/#add-experiment","title":"Add Experiment","text":"<p>Endpoint: <code>POST /sessions/{session_id}/experiments</code></p> <p>Purpose: Add single experiment with results</p> <p>Request Body: </p><pre><code>{\n  \"inputs\": {\n    \"temperature\": 60.0,\n    \"catalyst_loading\": 2.5,\n    \"solvent\": \"THF\"\n  },\n  \"output\": 85.3,\n  \"noise\": 1.2\n}\n</code></pre><p></p> <p>Response (201 Created): </p><pre><code>{\n  \"experiment_id\": 1,\n  \"inputs\": {...},\n  \"output\": 85.3,\n  \"added_at\": \"2025-12-12T10:15:00\"\n}\n</code></pre><p></p>"},{"location":"api/rest/#add-multiple-experiments","title":"Add Multiple Experiments","text":"<p>Endpoint: <code>POST /sessions/{session_id}/experiments/batch</code></p> <p>Purpose: Add multiple experiments at once</p> <p>Request Body: </p><pre><code>{\n  \"experiments\": [\n    {\n      \"inputs\": {\"temperature\": 60, \"solvent\": \"THF\"},\n      \"output\": 85.3\n    },\n    {\n      \"inputs\": {\"temperature\": 80, \"solvent\": \"DMF\"},\n      \"output\": 72.1\n    }\n  ]\n}\n</code></pre><p></p> <p>Response (201 Created): </p><pre><code>{\n  \"added\": 2,\n  \"total_experiments\": 27\n}\n</code></pre><p></p>"},{"location":"api/rest/#get-all-experiments","title":"Get All Experiments","text":"<p>Endpoint: <code>GET /sessions/{session_id}/experiments</code></p> <p>Query Parameters:</p> <ul> <li> <p><code>skip</code>: Pagination offset (default: 0)</p> </li> <li> <p><code>limit</code>: Max results (default: 100)</p> </li> </ul> <p>Response (200 OK): </p><pre><code>{\n  \"experiments\": [\n    {\n      \"temperature\": 60.0,\n      \"catalyst_loading\": 2.5,\n      \"solvent\": \"THF\",\n      \"yield\": 85.3\n    }\n  ],\n  \"total\": 25,\n  \"target_column\": \"yield\"\n}\n</code></pre><p></p>"},{"location":"api/rest/#upload-csv","title":"Upload CSV","text":"<p>Endpoint: <code>POST /sessions/{session_id}/experiments/upload</code></p> <p>Purpose: Upload experiments from CSV file</p> <p>Request: Multipart form data</p> <p>cURL Example: </p><pre><code>curl -X POST \\\n  http://localhost:8000/api/v1/sessions/{session_id}/experiments/upload \\\n  -F \"file=@experiments.csv\" \\\n  -F \"target_column=yield\"\n</code></pre><p></p> <p>Python Example: </p><pre><code>with open('experiments.csv', 'rb') as f:\n    response = requests.post(\n        f\"{BASE_URL}/sessions/{session_id}/experiments/upload\",\n        files={'file': f},\n        data={'target_column': 'yield'}\n    )\n</code></pre><p></p> <p>Response (200 OK): </p><pre><code>{\n  \"added\": 50,\n  \"total_experiments\": 50,\n  \"message\": \"Successfully uploaded 50 experiments\"\n}\n</code></pre><p></p>"},{"location":"api/rest/#export-experiments","title":"Export Experiments","text":"<p>Endpoint: <code>GET /sessions/{session_id}/experiments/export</code></p> <p>Purpose: Download experiments as CSV</p> <p>Response: CSV file with <code>Content-Disposition: attachment</code></p> <p>Example: </p><pre><code>response = requests.get(\n    f\"{BASE_URL}/sessions/{session_id}/experiments/export\"\n)\nwith open('experiments.csv', 'wb') as f:\n    f.write(response.content)\n</code></pre><p></p>"},{"location":"api/rest/#delete-all-experiments","title":"Delete All Experiments","text":"<p>Endpoint: <code>DELETE /sessions/{session_id}/experiments</code></p> <p>Response (204 No Content)</p>"},{"location":"api/rest/#staged-experiments-api","title":"Staged Experiments API","text":"<p>Staged experiments provide a workflow queue for autonomous optimization. Use these endpoints to track which experiments are pending execution.</p>"},{"location":"api/rest/#stage-experiment","title":"Stage Experiment","text":"<p>Endpoint: <code>POST /sessions/{session_id}/experiments/staged</code></p> <p>Purpose: Queue an experiment for later execution</p> <p>Request Body: </p><pre><code>{\n  \"inputs\": {\n    \"temperature\": 375.2,\n    \"catalyst_loading\": 3.1,\n    \"solvent\": \"DMF\"\n  },\n  \"reason\": \"qEI\"\n}\n</code></pre><p></p> <p>Response (200 OK): </p><pre><code>{\n  \"message\": \"Experiment staged successfully\",\n  \"n_staged\": 1,\n  \"staged_inputs\": {\"temperature\": 375.2, \"catalyst_loading\": 3.1, \"solvent\": \"DMF\"}\n}\n</code></pre><p></p>"},{"location":"api/rest/#stage-multiple-experiments","title":"Stage Multiple Experiments","text":"<p>Endpoint: <code>POST /sessions/{session_id}/experiments/staged/batch</code></p> <p>Purpose: Queue multiple experiments at once (e.g., from batch acquisition)</p> <p>Request Body: </p><pre><code>{\n  \"experiments\": [\n    {\"temperature\": 375.2, \"catalyst_loading\": 3.1, \"solvent\": \"DMF\"},\n    {\"temperature\": 412.5, \"catalyst_loading\": 1.8, \"solvent\": \"THF\"}\n  ],\n  \"reason\": \"qEI batch\"\n}\n</code></pre><p></p> <p>Response (200 OK): </p><pre><code>{\n  \"experiments\": [\n    {\"temperature\": 375.2, \"catalyst_loading\": 3.1, \"solvent\": \"DMF\", \"_reason\": \"qEI batch\"},\n    {\"temperature\": 412.5, \"catalyst_loading\": 1.8, \"solvent\": \"THF\", \"_reason\": \"qEI batch\"}\n  ],\n  \"n_staged\": 2\n}\n</code></pre><p></p>"},{"location":"api/rest/#get-staged-experiments","title":"Get Staged Experiments","text":"<p>Endpoint: <code>GET /sessions/{session_id}/experiments/staged</code></p> <p>Purpose: Retrieve all experiments awaiting execution</p> <p>Response (200 OK): </p><pre><code>{\n  \"experiments\": [\n    {\"temperature\": 375.2, \"catalyst_loading\": 3.1, \"solvent\": \"DMF\", \"_reason\": \"qEI\"}\n  ],\n  \"n_staged\": 1\n}\n</code></pre><p></p>"},{"location":"api/rest/#clear-staged-experiments","title":"Clear Staged Experiments","text":"<p>Endpoint: <code>DELETE /sessions/{session_id}/experiments/staged</code></p> <p>Purpose: Remove all staged experiments</p> <p>Response (200 OK): </p><pre><code>{\n  \"message\": \"Staged experiments cleared\",\n  \"n_cleared\": 2\n}\n</code></pre><p></p>"},{"location":"api/rest/#complete-staged-experiments","title":"Complete Staged Experiments","text":"<p>Endpoint: <code>POST /sessions/{session_id}/experiments/staged/complete</code></p> <p>Purpose: Finalize staged experiments by providing output values</p> <p>Query Parameters:</p> <ul> <li><code>auto_train</code>: Auto-retrain model (default: false)</li> <li><code>training_backend</code>: \"sklearn\" or \"botorch\"</li> <li><code>training_kernel</code>: Kernel type</li> </ul> <p>Request Body: </p><pre><code>{\n  \"outputs\": [85.3, 78.9],\n  \"noises\": [1.2, 0.8],\n  \"iteration\": 5,\n  \"reason\": \"qEI\"\n}\n</code></pre><p></p> <p>Response (200 OK): </p><pre><code>{\n  \"message\": \"Staged experiments completed and added to dataset\",\n  \"n_added\": 2,\n  \"n_experiments\": 27,\n  \"model_trained\": true,\n  \"training_metrics\": {\n    \"rmse\": 2.3,\n    \"r2\": 0.94,\n    \"backend\": \"botorch\"\n  }\n}\n</code></pre><p></p>"},{"location":"api/rest/#autonomous-workflow-example","title":"Autonomous Workflow Example","text":"<pre><code>import requests\n\nBASE_URL = \"http://localhost:8000/api/v1\"\nsession_id = \"your-session-id\"\n\n# 1. Get suggestion from acquisition\nresponse = requests.post(\n    f\"{BASE_URL}/sessions/{session_id}/acquisition/suggest\",\n    json={\"strategy\": \"qEI\", \"goal\": \"maximize\", \"n_suggestions\": 2}\n)\nsuggestions = response.json()[\"suggestions\"]\n\n# 2. Stage suggestions\nfor suggestion in suggestions:\n    requests.post(\n        f\"{BASE_URL}/sessions/{session_id}/experiments/staged\",\n        json={\"inputs\": suggestion, \"reason\": \"qEI\"}\n    )\n\n# 3. Get staged experiments (e.g., for hardware controller)\nresponse = requests.get(f\"{BASE_URL}/sessions/{session_id}/experiments/staged\")\npending = response.json()[\"experiments\"]\n\n# 4. Execute experiments and collect outputs\noutputs = [run_experiment(**exp) for exp in pending]\n\n# 5. Complete staged experiments with outputs\nresponse = requests.post(\n    f\"{BASE_URL}/sessions/{session_id}/experiments/staged/complete\",\n    json={\"outputs\": outputs},\n    params={\"auto_train\": True}\n)\n</code></pre>"},{"location":"api/rest/#models-api","title":"Models API","text":""},{"location":"api/rest/#train-model","title":"Train Model","text":"<p>Endpoint: <code>POST /sessions/{session_id}/model/train</code></p> <p>Purpose: Train Gaussian Process model</p> <p>Request Body: </p><pre><code>{\n  \"backend\": \"botorch\",\n  \"kernel\": \"Matern\",\n  \"kernel_params\": {\n    \"nu\": 2.5\n  },\n  \"target_column\": \"yield\",\n  \"goal\": \"maximize\"\n}\n</code></pre><p></p> <p>Response (200 OK): </p><pre><code>{\n  \"success\": true,\n  \"backend\": \"botorch\",\n  \"kernel\": \"Matern\",\n  \"hyperparameters\": {\n    \"lengthscales\": [0.15, 0.22, 0.18],\n    \"outputscale\": 1.45,\n    \"noise\": 0.05\n  },\n  \"metrics\": {\n    \"cv_rmse\": 3.2,\n    \"cv_r2\": 0.93,\n    \"cv_mae\": 2.1,\n    \"mean_z\": 0.02,\n    \"std_z\": 1.05\n  },\n  \"training_time_s\": 2.34\n}\n</code></pre><p></p> <p>Example: </p><pre><code>response = requests.post(\n    f\"{BASE_URL}/sessions/{session_id}/models/train\",\n    json={\n        \"backend\": \"botorch\",\n        \"kernel\": \"Matern\",\n        \"target_column\": \"yield\",\n        \"goal\": \"maximize\"\n    }\n)\nmetrics = response.json()[\"metrics\"]\nprint(f\"R\u00b2 = {metrics['cv_r2']:.3f}\")\n</code></pre><p></p>"},{"location":"api/rest/#get-model-info","title":"Get Model Info","text":"<p>Endpoint: <code>GET /sessions/{session_id}/model</code></p> <p>Response (200 OK): </p><pre><code>{\n  \"trained\": true,\n  \"backend\": \"botorch\",\n  \"kernel\": \"Matern\",\n  \"hyperparameters\": {...},\n  \"metrics\": {...},\n  \"trained_at\": \"2025-12-12T10:30:00\"\n}\n</code></pre><p></p>"},{"location":"api/rest/#get-predictions","title":"Get Predictions","text":"<p>Endpoint: <code>POST /sessions/{session_id}/model/predict</code></p> <p>Purpose: Make predictions at new points</p> <p>Request Body: </p><pre><code>{\n  \"inputs\": [\n    {\"temperature\": 65, \"catalyst_loading\": 3.0, \"solvent\": \"THF\"},\n    {\"temperature\": 75, \"catalyst_loading\": 2.0, \"solvent\": \"DMF\"}\n  ]\n}\n</code></pre><p></p> <p>Response (200 OK): </p><pre><code>{\n  \"predictions\": [\n    {\n      \"inputs\": {\"temperature\": 65, ...},\n      \"mean\": 87.5,\n      \"std\": 2.3,\n      \"confidence_interval_95\": [82.9, 92.1]\n    },\n    {\n      \"inputs\": {\"temperature\": 75, ...},\n      \"mean\": 79.2,\n      \"std\": 3.1,\n      \"confidence_interval_95\": [73.1, 85.3]\n    }\n  ]\n}\n</code></pre><p></p>"},{"location":"api/rest/#acquisition-api","title":"Acquisition API","text":""},{"location":"api/rest/#suggest-next-experiments","title":"Suggest Next Experiments","text":"<p>Endpoint: <code>POST /sessions/{session_id}/acquisition/suggest</code></p> <p>Purpose: Generate next experiment candidates</p> <p>Request Body: </p><pre><code>{\n  \"strategy\": \"EI\",\n  \"n_suggestions\": 5,\n  \"goal\": \"maximize\",\n  \"xi\": 0.01\n}\n</code></pre><p></p> <p>Available Strategies:</p> <ul> <li> <p><code>EI</code> - Expected Improvement</p> </li> <li> <p><code>PI</code> - Probability of Improvement</p> </li> <li> <p><code>UCB</code> - Upper Confidence Bound</p> </li> <li> <p><code>qEI</code> - Batch Expected Improvement</p> </li> <li> <p><code>qUCB</code> - Batch Upper Confidence Bound</p> </li> <li> <p><code>qNIPV</code> - Negative Integrated Posterior Variance (exploration)</p> </li> </ul> <p>Response (200 OK): </p><pre><code>{\n  \"suggestions\": [\n    {\n      \"temperature\": 75.3,\n      \"catalyst_loading\": 2.8,\n      \"solvent\": \"THF\"\n    }\n  ],\n  \"n_suggestions\": 1\n}\n</code></pre><p></p> <p>Example: </p><pre><code>response = requests.post(\n    f\"{BASE_URL}/sessions/{session_id}/acquisition/suggest\",\n    json={\n        \"strategy\": \"EI\",\n        \"n_suggestions\": 3,\n        \"goal\": \"maximize\"\n    }\n)\nsuggestions = response.json()[\"suggestions\"]\nfor i, cand in enumerate(candidates, 1):\n    print(f\"Candidate {i}: {cand}\")\n</code></pre><p></p>"},{"location":"api/rest/#visualizations-api","title":"Visualizations API","text":""},{"location":"api/rest/#get-parity-plot-data","title":"Get Parity Plot Data","text":"<p>Endpoint: <code>GET /sessions/{session_id}/visualizations/parity</code></p> <p>Response (200 OK): </p><pre><code>{\n  \"actual\": [85.3, 72.1, 90.2, ...],\n  \"predicted\": [86.1, 71.5, 89.8, ...],\n  \"std\": [2.3, 3.1, 2.0, ...],\n  \"metrics\": {\n    \"rmse\": 3.2,\n    \"mae\": 2.1,\n    \"r2\": 0.93\n  }\n}\n</code></pre><p></p>"},{"location":"api/rest/#get-q-q-plot-data","title":"Get Q-Q Plot Data","text":"<p>Endpoint: <code>GET /sessions/{session_id}/visualizations/qq_plot</code></p> <p>Response (200 OK): </p><pre><code>{\n  \"theoretical_quantiles\": [-2.5, -2.0, ...],\n  \"sample_quantiles\": [-2.4, -1.9, ...],\n  \"mean_z\": 0.02,\n  \"std_z\": 1.05,\n  \"calibration_status\": \"well_calibrated\"\n}\n</code></pre><p></p>"},{"location":"api/rest/#get-calibration-curve-data","title":"Get Calibration Curve Data","text":"<p>Endpoint: <code>GET /sessions/{session_id}/visualizations/calibration</code></p> <p>Response (200 OK): </p><pre><code>{\n  \"confidence_levels\": [0.68, 0.90, 0.95, 0.99],\n  \"observed_coverage\": [0.70, 0.89, 0.94, 0.98],\n  \"confidence_bands_lower\": [0.62, 0.85, 0.90, 0.96],\n  \"confidence_bands_upper\": [0.78, 0.94, 0.98, 0.99]\n}\n</code></pre><p></p>"},{"location":"api/rest/#websocket-api","title":"WebSocket API","text":""},{"location":"api/rest/#real-time-updates","title":"Real-Time Updates","text":"<p>Endpoint: <code>WS /ws/{session_id}</code></p> <p>Purpose: Subscribe to session events</p> <p>Events:</p> <ul> <li> <p><code>experiment_added</code>: New experimental data</p> </li> <li> <p><code>model_trained</code>: Model training completed</p> </li> <li> <p><code>acquisition_generated</code>: New candidates available</p> </li> </ul> <p>Example (Python with websockets): </p><pre><code>import websockets\nimport asyncio\n\nasync def listen_to_session(session_id):\n    uri = f\"ws://localhost:8000/ws/{session_id}\"\n    async with websockets.connect(uri) as websocket:\n        while True:\n            message = await websocket.recv()\n            event = json.loads(message)\n            print(f\"Event: {event['type']}\")\n            print(f\"Data: {event['data']}\")\n\nasyncio.run(listen_to_session(session_id))\n</code></pre><p></p>"},{"location":"api/rest/#rate-limiting","title":"Rate Limiting","text":"<p>Current: No rate limiting (local development)</p> <p>Future: Configurable rate limits for production:</p> <ul> <li> <p>100 requests/minute per IP (general)</p> </li> <li> <p>10 model training requests/hour per session</p> </li> <li> <p>1000 requests/hour for read-only endpoints</p> </li> </ul>"},{"location":"api/rest/#cors-configuration","title":"CORS Configuration","text":"<p>Default (development): </p><pre><code>allow_origins=[\"http://localhost:5173\", \"http://localhost:3000\"]\n</code></pre><p></p> <p>Production: Configure allowed origins in environment: </p><pre><code>ALLOWED_ORIGINS=https://myapp.com,https://app.mycompany.com\n</code></pre><p></p>"},{"location":"api/rest/#best-practices","title":"Best Practices","text":""},{"location":"api/rest/#session-lifecycle","title":"Session Lifecycle","text":"<p>Recommended pattern: </p><pre><code># 1. Create session\nsession = requests.post(f\"{BASE_URL}/sessions\").json()\nsession_id = session[\"session_id\"]\n\n# 2. Define variables\nfor var in variables:\n    requests.post(f\"{BASE_URL}/sessions/{session_id}/variables\", json=var)\n\n# 3. Add experiments\nrequests.post(f\"{BASE_URL}/sessions/{session_id}/experiments/batch\", json=experiments)\n\n# 4. Train model\nrequests.post(f\"{BASE_URL}/sessions/{session_id}/model/train\", json=model_config)\n\n# 5. Get suggestions\nsuggestions = requests.post(\n    f\"{BASE_URL}/sessions/{session_id}/acquisition/suggest\",\n    json={\"strategy\": \"EI\", \"n_suggestions\": 5}\n).json()\n\n# 6. Save session\nrequests.post(f\"{BASE_URL}/sessions/{session_id}/save\")\n</code></pre><p></p>"},{"location":"api/rest/#error-handling_1","title":"Error Handling","text":"<p>Always check status codes: </p><pre><code>response = requests.post(url, json=data)\nif response.status_code == 201:\n    result = response.json()\nelif response.status_code == 422:\n    errors = response.json()[\"detail\"]\n    print(f\"Validation errors: {errors}\")\nelse:\n    print(f\"Error: {response.status_code}\")\n    print(response.text)\n</code></pre><p></p>"},{"location":"api/rest/#performance-optimization","title":"Performance Optimization","text":"<p>Batch operations:</p> <ul> <li> <p>Use <code>/experiments/batch</code> instead of multiple single adds</p> </li> <li> <p>Upload CSV for large datasets</p> </li> <li> <p>Minimize round-trips</p> </li> </ul> <p>Caching:</p> <ul> <li> <p>Cache session_id after creation</p> </li> <li> <p>Reuse trained models</p> </li> <li> <p>Store experiment data locally</p> </li> </ul>"},{"location":"api/rest/#migration-from-core-session-api","title":"Migration from Core Session API","text":"<p>Convert Session API code to REST API:</p> <p>Before (Core Session API): </p><pre><code>from alchemist_core import OptimizationSession\n\nsession = OptimizationSession()\nsession.add_variable('temp', 'real', bounds=(20, 100))\nsession.add_experiment({'temp': 60}, output=85.3)\nsession.train_model(backend='botorch')\nnext_point = session.suggest_next(strategy='EI')\n</code></pre><p></p> <p>After (REST API): </p><pre><code>import requests\n\nBASE_URL = \"http://localhost:8000\"\n\n# Create session\nr = requests.post(f\"{BASE_URL}/sessions\")\nsession_id = r.json()[\"session_id\"]\n\n# Add variable\nrequests.post(\n    f\"{BASE_URL}/sessions/{session_id}/variables\",\n    json={\"name\": \"temp\", \"type\": \"real\", \"bounds\": [20, 100]}\n)\n\n# Add experiment\nrequests.post(\n    f\"{BASE_URL}/sessions/{session_id}/experiments\",\n    json={\"inputs\": {\"temp\": 60}, \"output\": 85.3}\n)\n\n# Train model\nrequests.post(\n    f\"{BASE_URL}/sessions/{session_id}/model/train\",\n    json={\"backend\": \"botorch\"}\n)\n\n# Get suggestions\nr = requests.post(\n    f\"{BASE_URL}/sessions/{session_id}/acquisition/suggest\",\n    json={\"strategy\": \"EI\", \"n_suggestions\": 1}\n)\nnext_point = r.json()[\"suggestions\"][0]\n</code></pre><p></p>"},{"location":"api/rest/#further-reading","title":"Further Reading","text":"<ul> <li>Core Session API - Python interface (no HTTP)</li> <li>Web Application - Browser interface using this API</li> <li>Session Management - Session lifecycle and storage</li> <li>Audit Logs - Reproducibility tracking</li> </ul> <p>Key Takeaway: The REST API provides language-agnostic access to ALchemist's Bayesian optimization capabilities. Use it for web applications, remote integrations, or non-Python clients.</p>"},{"location":"api/search_space/","title":"SearchSpace","text":""},{"location":"api/search_space/#searchspace","title":"SearchSpace","text":"<p>Manages the variable space for Bayesian optimization, including real, integer, and categorical variables.</p>"},{"location":"api/search_space/#class-reference","title":"Class Reference","text":""},{"location":"api/search_space/#alchemist_core.data.search_space.SearchSpace","title":"<code>alchemist_core.data.search_space.SearchSpace()</code>","text":"<p>Class for storing and managing the search space in a consistent way across backends. Provides methods for conversions to different formats required by different backends.</p>"},{"location":"api/search_space/#alchemist_core.data.search_space.SearchSpace.add_variable","title":"<code>add_variable(name, var_type, **kwargs)</code>","text":"<p>Add a variable to the search space.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Variable name</p> required <code>var_type</code> <code>str</code> <p>\"real\", \"integer\", or \"categorical\"</p> required <code>**kwargs</code> <p>Additional parameters (min, max, values)</p> <code>{}</code>"},{"location":"api/search_space/#alchemist_core.data.search_space.SearchSpace.from_dict","title":"<code>from_dict(data)</code>","text":"<p>Load search space from a list of dictionaries (used with JSON/CSV loading).</p>"},{"location":"api/search_space/#alchemist_core.data.search_space.SearchSpace.from_skopt","title":"<code>from_skopt(dimensions)</code>","text":"<p>Load search space from skopt dimensions.</p>"},{"location":"api/search_space/#alchemist_core.data.search_space.SearchSpace.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert search space to a list of dictionaries.</p>"},{"location":"api/search_space/#alchemist_core.data.search_space.SearchSpace.to_skopt","title":"<code>to_skopt()</code>","text":"<p>Get skopt dimensions for scikit-learn.</p>"},{"location":"api/search_space/#alchemist_core.data.search_space.SearchSpace.to_ax_space","title":"<code>to_ax_space()</code>","text":"<p>Convert to Ax parameter format.</p>"},{"location":"api/search_space/#alchemist_core.data.search_space.SearchSpace.to_botorch_bounds","title":"<code>to_botorch_bounds()</code>","text":"<p>Create bounds in BoTorch format.</p>"},{"location":"api/search_space/#alchemist_core.data.search_space.SearchSpace.get_variable_names","title":"<code>get_variable_names()</code>","text":"<p>Get list of all variable names.</p>"},{"location":"api/search_space/#alchemist_core.data.search_space.SearchSpace.get_categorical_variables","title":"<code>get_categorical_variables()</code>","text":"<p>Get list of categorical variable names.</p>"},{"location":"api/search_space/#alchemist_core.data.search_space.SearchSpace.get_integer_variables","title":"<code>get_integer_variables()</code>","text":"<p>Get list of integer variable names.</p>"},{"location":"api/search_space/#alchemist_core.data.search_space.SearchSpace.save_to_json","title":"<code>save_to_json(filepath)</code>","text":"<p>Save search space to a JSON file.</p>"},{"location":"api/search_space/#alchemist_core.data.search_space.SearchSpace.load_from_json","title":"<code>load_from_json(filepath)</code>","text":"<p>Load search space from a JSON file.</p>"},{"location":"api/search_space/#alchemist_core.data.search_space.SearchSpace.from_json","title":"<code>from_json(filepath)</code>  <code>classmethod</code>","text":"<p>Class method to create a SearchSpace from a JSON file.</p>"},{"location":"api/search_space/#alchemist_core.data.search_space.SearchSpace.add_constraint","title":"<code>add_constraint(constraint_type, coefficients, rhs, name=None)</code>","text":"<p>Add linear input constraint.</p> <p>Parameters:</p> Name Type Description Default <code>constraint_type</code> <code>str</code> <p>'inequality' (sum(coeff_i * x_i) &lt;= rhs) or              'equality' (sum(coeff_i * x_i) == rhs)</p> required <code>coefficients</code> <code>Dict[str, float]</code> <p>{variable_name: coefficient} mapping</p> required <code>rhs</code> <code>float</code> <p>right-hand side value</p> required <code>name</code> <code>Optional[str]</code> <p>optional human-readable name</p> <code>None</code>"},{"location":"api/search_space/#alchemist_core.data.search_space.SearchSpace.get_constraints","title":"<code>get_constraints()</code>","text":"<p>Return list of constraint dicts.</p>"},{"location":"api/search_space/#alchemist_core.data.search_space.SearchSpace.to_botorch_constraints","title":"<code>to_botorch_constraints(feature_names)</code>","text":"<p>Convert to BoTorch format for optimize_acqf.</p> <p>Each constraint is a tuple (indices_tensor, coefficients_tensor, rhs_float). BoTorch convention: inequality means sum(coeff_i * x_i) - rhs &lt;= 0.</p> <p>Parameters:</p> Name Type Description Default <code>feature_names</code> <code>List[str]</code> <p>ordered list of feature column names matching model input</p> required <p>Returns:</p> Type Description <code>Optional[List]</code> <p>(inequality_constraints, equality_constraints) \u2014 each is a list of tuples</p> <code>Optional[List]</code> <p>or None if no constraints of that type exist.</p>"},{"location":"api/search_space/#see-also","title":"See Also","text":"<ul> <li>OptimizationSession - Uses SearchSpace for variable management</li> </ul>"},{"location":"api/session/","title":"Usage Guide","text":""},{"location":"api/session/#optimizationsession-usage-guide","title":"OptimizationSession Usage Guide","text":"<p>Complete usage guide for <code>alchemist_core.OptimizationSession</code> - the main Python class for Bayesian optimization workflows.</p> <p>Auto-Generated Reference</p> <p>For class/method signatures auto-generated from docstrings, see OptimizationSession API Reference.</p>"},{"location":"api/session/#quick-start","title":"Quick Start","text":"<p>Complete workflow example: </p><pre><code>from alchemist_core import OptimizationSession\n\n# 1. Create session\nsession = OptimizationSession()\n\n# 2. Define search space\nsession.add_variable('temperature', 'real', bounds=(20, 100), unit='\u00b0C')\nsession.add_variable('pressure', 'real', bounds=(1, 10), unit='bar')\nsession.add_variable('catalyst', 'categorical', categories=['A', 'B', 'C'])\n\n# 3. Generate initial design\ninitial_points = session.generate_initial_design(method='lhs', n_points=15)\n\n# 4. Run experiments and add data\nfor point in initial_points:\n    output = my_experiment_function(**point)  # Your experiment\n    session.add_experiment(point, output=output)\n\n# 5. Train model\nsession.train_model(backend='botorch', kernel='Matern')\n\n# 6. Generate next candidates\ncandidates = session.suggest_next(strategy='EI', n_suggestions=5, goal='maximize')\n\n# 7. Save session\nsession.save_session('cache/sessions/')\n</code></pre><p></p>"},{"location":"api/session/#session-initialization","title":"Session Initialization","text":""},{"location":"api/session/#basic-initialization","title":"Basic Initialization","text":"<p>Create empty session: </p><pre><code>from alchemist_core import OptimizationSession\n\nsession = OptimizationSession()\n</code></pre><p></p> <p>With metadata: </p><pre><code>session = OptimizationSession()\nsession.metadata.name = \"Catalyst Screening\"\nsession.metadata.description = \"Optimization of Pd catalyst loading\"\nsession.metadata.tags = [\"catalysis\", \"suzuki\", \"2025\"]\nsession.metadata.author = \"Jane Researcher\"\n</code></pre><p></p>"},{"location":"api/session/#loading-existing-session","title":"Loading Existing Session","text":"<p>From file: </p><pre><code>session = OptimizationSession.load_session('cache/sessions/my_session.json')\n\n# Access session data\nprint(f\"Session: {session.metadata.name}\")\nprint(f\"Variables: {len(session.search_space.variables)}\")\nprint(f\"Experiments: {len(session.experiment_manager.data)}\")\n</code></pre><p></p> <p>With custom components: </p><pre><code>from alchemist_core.data.search_space import SearchSpace\nfrom alchemist_core.events import EventEmitter\n\n# Pre-configure components\nspace = SearchSpace()\nspace.add_variable('temp', 'real', min=20, max=100)\n\nevents = EventEmitter()\nevents.on('experiment_added', lambda data: print(f\"Added: {data}\"))\n\n# Initialize with components\nsession = OptimizationSession(\n    search_space=space,\n    event_emitter=events\n)\n</code></pre><p></p>"},{"location":"api/session/#search-space-management","title":"Search Space Management","text":""},{"location":"api/session/#adding-variables","title":"Adding Variables","text":"<p>Continuous (real) variables: </p><pre><code>session.add_variable(\n    name='temperature',\n    var_type='real',\n    bounds=(20.0, 100.0),\n    unit='\u00b0C'\n)\n\n# Alternative syntax\nsession.add_variable('pressure', 'real', bounds=(1.0, 10.0), unit='bar')\n</code></pre><p></p> <p>Discrete (integer) variables: </p><pre><code>session.add_variable(\n    name='n_stages',\n    var_type='integer',\n    bounds=(1, 10)\n)\n</code></pre><p></p> <p>Categorical variables: </p><pre><code>session.add_variable(\n    name='catalyst',\n    var_type='categorical',\n    categories=['A', 'B', 'C', 'D']\n)\n\n# Alternative: use 'values' parameter\nsession.add_variable('solvent', 'categorical', values=['THF', 'DMF', 'toluene'])\n</code></pre><p></p>"},{"location":"api/session/#search-space-summary","title":"Search Space Summary","text":"<p>Get variable information: </p><pre><code>summary = session.get_search_space_summary()\n\nprint(f\"Number of variables: {summary['n_variables']}\")\nfor var in summary['variables']:\n    print(f\"{var['name']}: {var['type']}, bounds={var['bounds']}\")\n</code></pre><p></p> <p>Example output: </p><pre><code>{\n    'n_variables': 3,\n    'variables': [\n        {\n            'name': 'temperature',\n            'type': 'real',\n            'bounds': [20.0, 100.0],\n            'unit': '\u00b0C'\n        },\n        {\n            'name': 'catalyst',\n            'type': 'categorical',\n            'categories': ['A', 'B', 'C']\n        }\n    ],\n    'categorical_variables': ['catalyst']\n}\n</code></pre><p></p>"},{"location":"api/session/#initial-design-generation","title":"Initial Design Generation","text":""},{"location":"api/session/#design-of-experiments-doe","title":"Design of Experiments (DOE)","text":"<p>Latin Hypercube Sampling (recommended): </p><pre><code>points = session.generate_initial_design(\n    method='lhs',\n    n_points=20,\n    random_seed=42\n)\n# Returns list of dicts: [{'temp': 45.2, 'pressure': 3.1, 'catalyst': 'A'}, ...]\n</code></pre><p></p> <p>Other methods: </p><pre><code># Random sampling\npoints = session.generate_initial_design('random', n_points=15)\n\n# Sobol sequences (low discrepancy)\npoints = session.generate_initial_design('sobol', n_points=32)\n\n# Halton sequences\npoints = session.generate_initial_design('halton', n_points=25)\n</code></pre><p></p> <p>LHS with criteria: </p><pre><code>points = session.generate_initial_design(\n    method='lhs',\n    n_points=20,\n    lhs_criterion='maximin',  # 'maximin', 'correlation', 'ratio'\n    random_seed=42\n)\n</code></pre><p></p>"},{"location":"api/session/#running-initial-experiments","title":"Running Initial Experiments","text":"<p>Evaluate and add results: </p><pre><code># Generate design\ninitial_points = session.generate_initial_design('lhs', n_points=15)\n\n# Run experiments\nfor point in initial_points:\n    # Your experiment function\n    output = run_experiment(\n        temperature=point['temperature'],\n        pressure=point['pressure'],\n        catalyst=point['catalyst']\n    )\n\n    # Add to session\n    session.add_experiment(point, output=output, reason='LHS initial design')\n\nprint(f\"Completed {len(initial_points)} initial experiments\")\n</code></pre><p></p>"},{"location":"api/session/#data-management","title":"Data Management","text":""},{"location":"api/session/#adding-experiments","title":"Adding Experiments","text":"<p>Single experiment: </p><pre><code>session.add_experiment(\n    inputs={'temperature': 60, 'pressure': 5, 'catalyst': 'A'},\n    output=85.3,\n    noise=1.2,  # Optional measurement uncertainty\n    reason='Manual entry'\n)\n</code></pre><p></p> <p>Batch addition: </p><pre><code>experiments = [\n    {'temperature': 60, 'pressure': 5, 'catalyst': 'A'},\n    {'temperature': 80, 'pressure': 3, 'catalyst': 'B'},\n    {'temperature': 40, 'pressure': 7, 'catalyst': 'C'}\n]\noutputs = [85.3, 72.1, 68.9]\n\nfor inputs, output in zip(experiments, outputs):\n    session.add_experiment(inputs, output=output)\n</code></pre><p></p>"},{"location":"api/session/#loading-from-csv","title":"Loading from CSV","text":"<p>Simple load: </p><pre><code>session.load_data('experiments.csv', target_column='yield')\n</code></pre><p></p> <p>With noise column: </p><pre><code>session.load_data(\n    filepath='experiments.csv',\n    target_column='yield',\n    noise_column='std_dev'\n)\n</code></pre><p></p> <p>CSV format: </p><pre><code>temperature,pressure,catalyst,yield,std_dev\n60,5,A,85.3,1.2\n80,3,B,72.1,0.9\n40,7,C,68.9,1.5\n</code></pre><p></p>"},{"location":"api/session/#data-summary","title":"Data Summary","text":"<p>Get statistics: </p><pre><code>summary = session.get_data_summary()\n\nprint(f\"Number of experiments: {summary['n_experiments']}\")\nprint(f\"Target range: {summary['target_stats']['min']:.2f} - {summary['target_stats']['max']:.2f}\")\nprint(f\"Target mean: {summary['target_stats']['mean']:.2f}\")\nprint(f\"Has noise data: {summary['has_noise']}\")\n</code></pre><p></p>"},{"location":"api/session/#model-training","title":"Model Training","text":""},{"location":"api/session/#basic-training","title":"Basic Training","text":"<p>BoTorch backend (recommended): </p><pre><code>results = session.train_model(\n    backend='botorch',\n    kernel='Matern',\n    kernel_params={'nu': 2.5}\n)\n\nprint(f\"R\u00b2 = {results['metrics']['cv_r2']:.3f}\")\nprint(f\"RMSE = {results['metrics']['cv_rmse']:.3f}\")\n</code></pre><p></p> <p>Scikit-learn backend: </p><pre><code>results = session.train_model(\n    backend='sklearn',\n    kernel='Matern',\n    kernel_params={'nu': 2.5}\n)\n</code></pre><p></p>"},{"location":"api/session/#kernel-options","title":"Kernel Options","text":"<p>Matern kernels (most versatile): </p><pre><code># Matern \u03bd=1.5 (less smooth, more flexible)\nsession.train_model(backend='botorch', kernel='Matern', kernel_params={'nu': 1.5})\n\n# Matern \u03bd=2.5 (smooth, good default)\nsession.train_model(backend='botorch', kernel='Matern', kernel_params={'nu': 2.5})\n</code></pre><p></p> <p>RBF kernel (infinitely smooth): </p><pre><code>session.train_model(backend='botorch', kernel='RBF')\n</code></pre><p></p> <p>Rational Quadratic (mixture of lengthscales): </p><pre><code>session.train_model(backend='botorch', kernel='RationalQuadratic')\n</code></pre><p></p>"},{"location":"api/session/#advanced-training-options","title":"Advanced Training Options","text":"<p>BoTorch with transforms: </p><pre><code>results = session.train_model(\n    backend='botorch',\n    kernel='Matern',\n    kernel_params={'nu': 2.5},\n    input_transform_type='normalize',      # Auto-applied by default\n    output_transform_type='standardize',   # Auto-applied by default\n    calibration_enabled=True               # Apply automatic calibration\n)\n</code></pre><p></p> <p>Sklearn with transforms: </p><pre><code>results = session.train_model(\n    backend='sklearn',\n    kernel='Matern',\n    kernel_params={'nu': 2.5},\n    input_transform_type='minmax',    # 'minmax', 'standard', 'robust', 'none'\n    output_transform_type='standard',  # 'standard' or 'none'\n    n_restarts=10                      # Hyperparameter optimization restarts\n)\n</code></pre><p></p>"},{"location":"api/session/#training-results","title":"Training Results","text":"<p>Inspect results: </p><pre><code>results = session.train_model(backend='botorch', kernel='Matern')\n\n# Cross-validation metrics\nprint(\"Cross-Validation Metrics:\")\nprint(f\"  R\u00b2 = {results['metrics']['cv_r2']:.4f}\")\nprint(f\"  RMSE = {results['metrics']['cv_rmse']:.4f}\")\nprint(f\"  MAE = {results['metrics']['cv_mae']:.4f}\")\n\n# Calibration diagnostics\nprint(\"\\nCalibration:\")\nprint(f\"  Mean(z) = {results['metrics']['mean_z']:.4f}\")\nprint(f\"  Std(z) = {results['metrics']['std_z']:.4f}\")\n\n# Hyperparameters\nprint(\"\\nHyperparameters:\")\nprint(f\"  Lengthscales: {results['hyperparameters']['lengthscales']}\")\nprint(f\"  Outputscale: {results['hyperparameters']['outputscale']:.4f}\")\nprint(f\"  Noise: {results['hyperparameters']['noise']:.6f}\")\n</code></pre><p></p>"},{"location":"api/session/#acquisition-functions","title":"Acquisition Functions","text":""},{"location":"api/session/#generate-candidates","title":"Generate Candidates","text":"<p>Expected Improvement (EI): </p><pre><code>candidates = session.suggest_next(\n    strategy='EI',\n    n_candidates=5,\n    goal='maximize',\n    xi=0.01  # Exploration parameter\n)\n\n# Returns DataFrame with candidates\nprint(candidates)\n</code></pre><p></p> <p>Upper Confidence Bound (UCB): </p><pre><code>candidates = session.suggest_next(\n    strategy='UCB',\n    n_candidates=3,\n    goal='maximize',\n    kappa=2.0  # Exploration weight\n)\n</code></pre><p></p> <p>Probability of Improvement (PI): </p><pre><code>candidates = session.suggest_next(\n    strategy='PI',\n    n_candidates=5,\n    goal='maximize',\n    xi=0.01\n)\n</code></pre><p></p> <p>Thompson Sampling (TS): </p><pre><code>candidates = session.suggest_next(\n    strategy='ThompsonSampling',\n    n_candidates=1,\n    goal='maximize'\n)\n</code></pre><p></p>"},{"location":"api/session/#minimization-vs-maximization","title":"Minimization vs Maximization","text":"<p>Maximize (e.g., yield): </p><pre><code>candidates = session.suggest_next(strategy='EI', goal='maximize', n_candidates=3)\n</code></pre><p></p> <p>Minimize (e.g., cost, error): </p><pre><code>candidates = session.suggest_next(strategy='EI', goal='minimize', n_candidates=3)\n</code></pre><p></p>"},{"location":"api/session/#working-with-candidates","title":"Working with Candidates","text":"<p>Extract candidate values: </p><pre><code>candidates = session.suggest_next('EI', n_candidates=3, goal='maximize')\n\nfor i, row in candidates.iterrows():\n    print(f\"Candidate {i+1}:\")\n    print(f\"  Temperature: {row['temperature']:.2f}\")\n    print(f\"  Pressure: {row['pressure']:.2f}\")\n    print(f\"  Catalyst: {row['catalyst']}\")\n    print()\n</code></pre><p></p> <p>Convert to list of dicts: </p><pre><code>candidates = session.suggest_next('EI', n_candidates=3, goal='maximize')\ncandidate_dicts = candidates.to_dict('records')\n\n# Each element is a dict: {'temperature': 75.3, 'pressure': 4.2, 'catalyst': 'A'}\nfor point in candidate_dicts:\n    output = run_experiment(**point)\n    session.add_experiment(point, output=output, reason='EI')\n</code></pre><p></p>"},{"location":"api/session/#staged-experiments-workflow","title":"Staged Experiments Workflow","text":"<p>Purpose: Manage experiments awaiting evaluation</p> <p>Pattern: </p><pre><code># 1. Generate and stage candidates\ncandidates = session.suggest_next('EI', n_candidates=5, goal='maximize')\nfor _, row in candidates.iterrows():\n    session.add_staged_experiment(row.to_dict())\n\n# 2. Get staged experiments\nstaged = session.get_staged_experiments()\nprint(f\"{len(staged)} experiments staged\")\n\n# 3. Run experiments\noutputs = []\nfor point in staged:\n    output = run_experiment(**point)\n    outputs.append(output)\n\n# 4. Move to dataset in batch\nsession.move_staged_to_experiments(\n    outputs=outputs,\n    reason='Expected Improvement - Batch 3'\n)\n\nprint(f\"Added {len(outputs)} experiments to dataset\")\n</code></pre><p></p>"},{"location":"api/session/#predictions","title":"Predictions","text":""},{"location":"api/session/#make-predictions","title":"Make Predictions","text":"<p>Single point: </p><pre><code># Must train model first\nsession.train_model(backend='botorch', kernel='Matern')\n\n# Predict\npoint = {'temperature': 65, 'pressure': 4, 'catalyst': 'A'}\nmean, std = session.predict(point)\n\nprint(f\"Predicted: {mean:.2f} \u00b1 {std:.2f}\")\n</code></pre><p></p> <p>Multiple points: </p><pre><code>points = [\n    {'temperature': 65, 'pressure': 4, 'catalyst': 'A'},\n    {'temperature': 75, 'pressure': 5, 'catalyst': 'B'}\n]\n\nmeans, stds = session.predict_batch(points)\n\nfor point, mean, std in zip(points, means, stds):\n    print(f\"{point}: {mean:.2f} \u00b1 {std:.2f}\")\n</code></pre><p></p>"},{"location":"api/session/#prediction-with-confidence-intervals","title":"Prediction with Confidence Intervals","text":"<p>Custom confidence level: </p><pre><code>from scipy.stats import norm\n\nmean, std = session.predict(point)\n\n# 95% confidence interval\nz_95 = 1.96\nci_lower = mean - z_95 * std\nci_upper = mean + z_95 * std\n\nprint(f\"Prediction: {mean:.2f}\")\nprint(f\"95% CI: [{ci_lower:.2f}, {ci_upper:.2f}]\")\n</code></pre><p></p>"},{"location":"api/session/#session-persistence","title":"Session Persistence","text":""},{"location":"api/session/#saving-sessions","title":"Saving Sessions","text":"<p>Simple save: </p><pre><code>filepath = session.save_session()\nprint(f\"Saved to: {filepath}\")\n</code></pre><p></p> <p>Custom location: </p><pre><code>session.save_session(directory='cache/sessions/', filename='my_optimization.json')\n</code></pre><p></p> <p>With directory creation: </p><pre><code>import os\nos.makedirs('results/sessions', exist_ok=True)\nsession.save_session('results/sessions/')\n</code></pre><p></p>"},{"location":"api/session/#loading-sessions","title":"Loading Sessions","text":"<p>Load from file: </p><pre><code>session = OptimizationSession.load_session('cache/sessions/my_optimization.json')\n</code></pre><p></p> <p>Check session contents: </p><pre><code>session = OptimizationSession.load_session('path/to/session.json')\n\nprint(f\"Session: {session.metadata.name}\")\nprint(f\"Created: {session.metadata.created_at}\")\nprint(f\"Variables: {session.get_search_space_summary()['n_variables']}\")\nprint(f\"Experiments: {session.get_data_summary()['n_experiments']}\")\nprint(f\"Model trained: {session.model is not None}\")\n</code></pre><p></p>"},{"location":"api/session/#audit-logs","title":"Audit Logs","text":""},{"location":"api/session/#locking-decisions","title":"Locking Decisions","text":"<p>Lock data: </p><pre><code># Add experimental data\nsession.load_data('experiments.csv', target_column='yield')\n\n# Lock data state\nsession.lock_data(notes=\"Initial dataset after LHS design\")\n</code></pre><p></p> <p>Lock model: </p><pre><code># Train model\nsession.train_model(backend='botorch', kernel='Matern')\n\n# Lock model state\nsession.lock_model(notes=\"Production model for batch 3\")\n</code></pre><p></p> <p>Lock acquisition: </p><pre><code># Generate candidates\ncandidates = session.suggest_next('EI', n_candidates=5, goal='maximize')\n\n# Lock acquisition decision\nsession.lock_acquisition(\n    strategy='EI',\n    candidates=candidates,\n    notes=\"Batch 3 - targeting optimal region\"\n)\n</code></pre><p></p>"},{"location":"api/session/#viewing-audit-log","title":"Viewing Audit Log","text":"<p>Get all entries: </p><pre><code>entries = session.audit_log.entries\n\nfor entry in entries:\n    print(f\"{entry.timestamp}: {entry.entry_type}\")\n    print(f\"  Notes: {entry.notes}\")\n    print(f\"  Hash: {entry.hash[:16]}...\")\n</code></pre><p></p> <p>Filter by type: </p><pre><code>model_entries = [e for e in session.audit_log.entries if e.entry_type == 'model_locked']\nprint(f\"Found {len(model_entries)} model lock entries\")\n</code></pre><p></p> <p>Verify integrity: </p><pre><code>is_valid = session.audit_log.verify_integrity()\nif is_valid:\n    print(\"\u2713 Audit log verified - no tampering detected\")\nelse:\n    print(\"\u2717 Audit log corrupted - integrity check failed\")\n</code></pre><p></p>"},{"location":"api/session/#event-handling","title":"Event Handling","text":""},{"location":"api/session/#subscribe-to-events","title":"Subscribe to Events","text":"<p>Listen for events: </p><pre><code>def on_experiment_added(data):\n    print(f\"New experiment: {data['inputs']} \u2192 {data['output']}\")\n\ndef on_model_trained(data):\n    print(f\"Model trained: R\u00b2 = {data['metrics']['cv_r2']:.3f}\")\n\nsession.events.on('experiment_added', on_experiment_added)\nsession.events.on('model_trained', on_model_trained)\n\n# Now events will trigger callbacks\nsession.add_experiment({'temp': 60}, output=85)\nsession.train_model(backend='botorch')\n</code></pre><p></p> <p>Available events:</p> <ul> <li> <p><code>variable_added</code></p> </li> <li> <p><code>data_loaded</code></p> </li> <li> <p><code>experiment_added</code></p> </li> <li> <p><code>initial_design_generated</code></p> </li> <li> <p><code>model_trained</code></p> </li> <li> <p><code>acquisition_generated</code></p> </li> <li> <p><code>staged_experiments_cleared</code></p> </li> </ul>"},{"location":"api/session/#complete-example-workflows","title":"Complete Example Workflows","text":""},{"location":"api/session/#basic-optimization-loop","title":"Basic Optimization Loop","text":"<pre><code>from alchemist_core import OptimizationSession\n\n# Initialize\nsession = OptimizationSession()\nsession.metadata.name = \"Process Optimization\"\n\n# Define variables\nsession.add_variable('temperature', 'real', bounds=(20, 100))\nsession.add_variable('pressure', 'real', bounds=(1, 10))\n\n# Initial design\ninitial_points = session.generate_initial_design('lhs', n_points=10)\nfor point in initial_points:\n    output = my_experiment(**point)\n    session.add_experiment(point, output=output)\n\n# Optimization loop\nfor iteration in range(10):\n    # Train model\n    session.train_model(backend='botorch', kernel='Matern')\n\n    # Get candidates\n    candidates = session.suggest_next('EI', n_candidates=3, goal='maximize')\n\n    # Evaluate\n    for _, row in candidates.iterrows():\n        point = row.to_dict()\n        output = my_experiment(**point)\n        session.add_experiment(point, output=output, iteration=iteration+1)\n\n    # Check progress\n    summary = session.get_data_summary()\n    best = summary['target_stats']['max']\n    print(f\"Iteration {iteration+1}: Best = {best:.2f}\")\n\n# Save final session\nsession.save_session()\n</code></pre>"},{"location":"api/session/#batch-processing-with-staging","title":"Batch Processing with Staging","text":"<pre><code># Generate batch of candidates\nbatch_size = 10\ncandidates = session.suggest_next('UCB', n_candidates=batch_size, goal='maximize')\n\n# Stage all candidates\nfor _, row in candidates.iterrows():\n    session.add_staged_experiment(row.to_dict())\n\n# Run experiments (possibly in parallel)\nstaged = session.get_staged_experiments()\noutputs = [run_experiment(**point) for point in staged]\n\n# Add all results at once\nsession.move_staged_to_experiments(\n    outputs=outputs,\n    reason=f'UCB Batch {batch_number}'\n)\n\n# Re-train with updated data\nsession.train_model(backend='botorch', kernel='Matern')\n</code></pre>"},{"location":"api/session/#reproducible-research-workflow","title":"Reproducible Research Workflow","text":"<pre><code># Set random seed for reproducibility\nrandom_seed = 42\n\n# Create session with metadata\nsession = OptimizationSession()\nsession.metadata.name = \"Manuscript Optimization\"\nsession.metadata.description = \"Results for Journal of X\"\nsession.metadata.author = \"Jane Researcher\"\nsession.metadata.tags = [\"publication\", \"2025\"]\n\n# Define search space\nsession.add_variable('var1', 'real', bounds=(0, 1))\nsession.add_variable('var2', 'real', bounds=(0, 1))\n\n# Generate reproducible initial design\ninitial = session.generate_initial_design('lhs', n_points=20, random_seed=random_seed)\n\n# Add data and lock\nfor point in initial:\n    output = my_experiment(**point)\n    session.add_experiment(point, output=output)\nsession.lock_data(notes=\"Initial LHS design, n=20, seed=42\")\n\n# Train model and lock\nresults = session.train_model(backend='botorch', kernel='Matern')\nsession.lock_model(notes=f\"Production model, R\u00b2={results['metrics']['cv_r2']:.3f}\")\n\n# Generate candidates and lock\ncandidates = session.suggest_next('EI', n_candidates=5, goal='maximize')\nsession.lock_acquisition(strategy='EI', candidates=candidates, notes=\"Batch 1\")\n\n# Save with audit trail\nsession.save_session('manuscript_data/sessions/')\nsession.audit_log.export('manuscript_data/audit_log.json')\n</code></pre>"},{"location":"api/session/#configuration-options","title":"Configuration Options","text":""},{"location":"api/session/#session-configuration","title":"Session Configuration","text":"<p>Access config: </p><pre><code># View current config\nprint(session.config)\n\n# Modify settings\nsession.config['random_state'] = 123\nsession.config['verbose'] = True\n</code></pre><p></p> <p>Available options: </p><pre><code>{\n    'random_state': 42,           # Random seed\n    'verbose': True,              # Logging verbosity\n    'auto_train': False,          # Auto-train after adding data\n    'auto_train_threshold': 5     # Min experiments before auto-train\n}\n</code></pre><p></p>"},{"location":"api/session/#best-practices","title":"Best Practices","text":""},{"location":"api/session/#recommended-workflow","title":"Recommended Workflow","text":"<ol> <li>Define complete search space before generating designs</li> <li>Generate space-filling initial design (LHS with 5-10\u00d7 dimensions)</li> <li>Lock data before training production models</li> <li>Compare multiple kernels during exploration</li> <li>Use BoTorch backend for most applications</li> <li>Monitor calibration (Std(z) \u2248 1.0)</li> <li>Save sessions frequently at milestones</li> <li>Lock decisions for reproducibility</li> </ol>"},{"location":"api/session/#performance-tips","title":"Performance Tips","text":"<p>For large datasets (&gt; 100 points):</p> <ul> <li> <p>Use BoTorch backend (GPU acceleration if available)</p> </li> <li> <p>Consider subset for cross-validation</p> </li> <li> <p>Batch candidate generation</p> </li> </ul> <p>For many variables (&gt; 10):</p> <ul> <li> <p>Use ARD lengthscales (enabled by default)</p> </li> <li> <p>Increase initial design size (10-15\u00d7 dimensions)</p> </li> <li> <p>Consider variable screening</p> </li> </ul> <p>For expensive experiments:</p> <ul> <li> <p>Start with smaller initial design</p> </li> <li> <p>Use conservative acquisition (low \u03be/\u03ba)</p> </li> <li> <p>Validate model calibration carefully</p> </li> </ul>"},{"location":"api/session/#further-reading","title":"Further Reading","text":"<ul> <li>REST API - HTTP interface to Session API</li> <li>Session Management - Save/load sessions</li> <li>Audit Logs - Reproducibility tracking</li> <li>Web Application - Browser interface</li> <li>BoTorch Backend - Model training options</li> </ul> <p>Key Takeaway: The Core Session API provides complete programmatic control over Bayesian optimization workflows. Use it for automation, custom integrations, and reproducible research.</p>"},{"location":"api/session_class/","title":"OptimizationSession","text":""},{"location":"api/session_class/#optimizationsession","title":"OptimizationSession","text":"<p>The main class for Bayesian optimization workflows. Orchestrates variable space definition, data management, model training, and acquisition function execution.</p>"},{"location":"api/session_class/#class-reference","title":"Class Reference","text":""},{"location":"api/session_class/#alchemist_core.session.OptimizationSession","title":"<code>alchemist_core.session.OptimizationSession(search_space=None, experiment_manager=None, event_emitter=None, session_metadata=None)</code>","text":"<p>High-level interface for Bayesian optimization workflows.</p> <p>This class orchestrates the complete optimization loop: 1. Define search space 2. Load/add experimental data 3. Train surrogate model 4. Run acquisition to suggest next experiments 5. Iterate</p> Example <p>from alchemist_core import OptimizationSession</p> <p>Initialize optimization session.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <code>Optional[SearchSpace]</code> <p>Pre-configured SearchSpace object (optional)</p> <code>None</code> <code>experiment_manager</code> <code>Optional[ExperimentManager]</code> <p>Pre-configured ExperimentManager (optional)</p> <code>None</code> <code>event_emitter</code> <code>Optional[EventEmitter]</code> <p>EventEmitter for progress notifications (optional)</p> <code>None</code> <code>session_metadata</code> <code>Optional[SessionMetadata]</code> <p>Pre-configured session metadata (optional)</p> <code>None</code>"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession--create-session-with-search-space","title":"Create session with search space","text":"<p>session = OptimizationSession() session.add_variable('temperature', 'real', bounds=(300, 500)) session.add_variable('pressure', 'real', bounds=(1, 10)) session.add_variable('catalyst', 'categorical', categories=['A', 'B', 'C'])</p>"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession--load-experimental-data","title":"Load experimental data","text":"<p>session.load_data('experiments.csv', target_columns='yield')</p>"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession--train-model","title":"Train model","text":"<p>session.train_model(backend='botorch', kernel='Matern')</p>"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession--suggest-next-experiment","title":"Suggest next experiment","text":"<p>next_point = session.suggest_next(strategy='EI', goal='maximize') print(next_point)</p>"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession.add_variable","title":"<code>add_variable(name, var_type, **kwargs)</code>","text":"<p>Add a variable to the search space.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Variable name</p> required <code>var_type</code> <code>str</code> <p>Type ('real', 'integer', 'categorical')</p> required <code>**kwargs</code> <p>Type-specific parameters: - For 'real'/'integer': bounds=(min, max) or min=..., max=... - For 'categorical': categories=[list of values] or values=[list]</p> <code>{}</code> Example <p>session.add_variable('temp', 'real', bounds=(300, 500)) session.add_variable('catalyst', 'categorical', categories=['A', 'B'])</p>"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession.get_search_space_summary","title":"<code>get_search_space_summary()</code>","text":"<p>Get summary of current search space.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with variable information</p>"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession.generate_initial_design","title":"<code>generate_initial_design(method='lhs', n_points=10, random_seed=None, **kwargs)</code>","text":"<p>Generate initial experimental design (Design of Experiments).</p> <p>Creates a set of experimental conditions to evaluate before starting Bayesian optimization. This does NOT add the experiments to the session - you must evaluate them and add the results using add_experiment().</p> <p>Supported methods: - 'random': Uniform random sampling - 'lhs': Latin Hypercube Sampling (recommended, good space-filling properties) - 'sobol': Sobol quasi-random sequences (low discrepancy) - 'halton': Halton sequences - 'hammersly': Hammersly sequences (low discrepancy)</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>Sampling strategy to use</p> <code>'lhs'</code> <code>n_points</code> <code>int</code> <p>Number of points to generate</p> <code>10</code> <code>random_seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility</p> <code>None</code> <code>**kwargs</code> <p>Additional method-specific parameters: - lhs_criterion: For LHS method (\"maximin\", \"correlation\", \"ratio\")</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of dictionaries with variable names and values (no outputs)</p> Example"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession.generate_initial_design--generate-initial-design","title":"Generate initial design","text":"<p>points = session.generate_initial_design('lhs', n_points=10)</p>"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession.generate_initial_design--run-experiments-and-add-results","title":"Run experiments and add results","text":"<p>for point in points:     output = run_experiment(**point)  # Your experiment function     session.add_experiment(point, output=output)</p>"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession.generate_initial_design--now-ready-to-train-model","title":"Now ready to train model","text":"<p>session.train_model()</p>"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession.load_data","title":"<code>load_data(filepath, target_columns='Output', noise_column=None)</code>","text":"<p>Load experimental data from CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to CSV file</p> required <code>target_columns</code> <code>Union[str, List[str]]</code> <p>Target column name(s). Can be: - String for single-objective: 'yield' - List for multi-objective: ['yield', 'selectivity'] Default: 'Output'</p> <code>'Output'</code> <code>noise_column</code> <code>Optional[str]</code> <p>Optional column with measurement noise/uncertainty</p> <code>None</code> <p>Examples:</p> <p>Single-objective:</p> <pre><code>&gt;&gt;&gt; session.load_data('experiments.csv', target_columns='yield')\n&gt;&gt;&gt; session.load_data('experiments.csv', target_columns=['yield'])  # also works\n</code></pre> <p>Multi-objective:</p> <pre><code>&gt;&gt;&gt; session.load_data('experiments.csv', target_columns=['yield', 'selectivity'])\n</code></pre> Note <p>If the CSV doesn't have columns matching target_columns, an error will be raised. Target columns will be preserved with their original names internally.</p>"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession.add_experiment","title":"<code>add_experiment(inputs, output, noise=None, iteration=None, reason=None)</code>","text":"<p>Add a single experiment to the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Dict[str, Any]</code> <p>Dictionary mapping variable names to values</p> required <code>output</code> <code>float</code> <p>Target/output value</p> required <code>noise</code> <code>Optional[float]</code> <p>Optional measurement uncertainty</p> <code>None</code> <code>iteration</code> <code>Optional[int]</code> <p>Iteration number (auto-assigned if None)</p> <code>None</code> <code>reason</code> <code>Optional[str]</code> <p>Reason for this experiment (e.g., 'Manual', 'Expected Improvement')</p> <code>None</code> Example <p>session.add_experiment( ...     inputs={'temperature': 350, 'catalyst': 'A'}, ...     output=0.85, ...     reason='Manual' ... )</p>"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession.get_data_summary","title":"<code>get_data_summary()</code>","text":"<p>Get summary of current experimental data.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with data statistics</p>"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession.add_staged_experiment","title":"<code>add_staged_experiment(inputs)</code>","text":"<p>Add an experiment to the staging area (awaiting evaluation).</p> <p>Staged experiments are typically suggested by acquisition functions but not yet evaluated. They can be retrieved, evaluated externally, and then added to the dataset with add_experiment().</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Dict[str, Any]</code> <p>Dictionary mapping variable names to values</p> required Example"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession.add_staged_experiment--generate-suggestions-and-stage-them","title":"Generate suggestions and stage them","text":"<p>suggestions = session.suggest_next(n_suggestions=3) for point in suggestions.to_dict('records'):     session.add_staged_experiment(point)</p>"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession.add_staged_experiment--later-evaluate-and-add","title":"Later, evaluate and add","text":"<p>staged = session.get_staged_experiments() for point in staged:     output = run_experiment(**point)     session.add_experiment(point, output=output) session.clear_staged_experiments()</p>"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession.get_staged_experiments","title":"<code>get_staged_experiments()</code>","text":"<p>Get all staged experiments awaiting evaluation.</p> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of experiment input dictionaries</p>"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession.clear_staged_experiments","title":"<code>clear_staged_experiments()</code>","text":"<p>Clear all staged experiments.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of experiments cleared</p>"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession.move_staged_to_experiments","title":"<code>move_staged_to_experiments(outputs, noises=None, iteration=None, reason=None)</code>","text":"<p>Evaluate staged experiments and add them to the dataset in batch.</p> <p>Convenience method that pairs staged inputs with outputs and adds them all to the experiment manager, then clears the staging area.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>List[float]</code> <p>List of output values (must match length of staged experiments)</p> required <code>noises</code> <code>Optional[List[float]]</code> <p>Optional list of measurement uncertainties</p> <code>None</code> <code>iteration</code> <code>Optional[int]</code> <p>Iteration number for all experiments (auto-assigned if None)</p> <code>None</code> <code>reason</code> <code>Optional[str]</code> <p>Reason for these experiments (e.g., 'Expected Improvement')</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of experiments added</p> Example"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession.move_staged_to_experiments--stage-some-experiments","title":"Stage some experiments","text":"<p>session.add_staged_experiment({'x': 1.0, 'y': 2.0}) session.add_staged_experiment({'x': 3.0, 'y': 4.0})</p>"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession.move_staged_to_experiments--evaluate-them","title":"Evaluate them","text":"<p>outputs = [run_experiment(**point) for point in session.get_staged_experiments()]</p>"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession.move_staged_to_experiments--add-to-dataset-and-clear-staging","title":"Add to dataset and clear staging","text":"<p>session.move_staged_to_experiments(outputs, reason='LogEI')</p>"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession.train_model","title":"<code>train_model(backend='sklearn', kernel='Matern', kernel_params=None, **kwargs)</code>","text":"<p>Train surrogate model on current data.</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>str</code> <p>'sklearn' or 'botorch'</p> <code>'sklearn'</code> <code>kernel</code> <code>str</code> <p>Kernel type ('RBF', 'Matern', 'RationalQuadratic')</p> <code>'Matern'</code> <code>kernel_params</code> <code>Optional[Dict]</code> <p>Additional kernel parameters (e.g., {'nu': 2.5} for Matern)</p> <code>None</code> <code>**kwargs</code> <p>Backend-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with training results and hyperparameters</p> Example <p>results = session.train_model(backend='botorch', kernel='Matern') print(results['metrics'])</p>"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession.get_model_summary","title":"<code>get_model_summary()</code>","text":"<p>Get summary of trained model.</p> <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Dictionary with model information, or None if no model trained</p>"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession.suggest_next","title":"<code>suggest_next(strategy='EI', goal='maximize', n_suggestions=1, ref_point=None, **kwargs)</code>","text":"<p>Suggest next experiment(s) using acquisition function.</p> <p>Parameters:</p> Name Type Description Default <code>strategy</code> <code>str</code> <p>Acquisition strategy - 'EI': Expected Improvement - 'PI': Probability of Improvement - 'UCB': Upper Confidence Bound - 'LogEI': Log Expected Improvement (BoTorch only) - 'LogPI': Log Probability of Improvement (BoTorch only) - 'qEI', 'qUCB', 'qIPV': Batch acquisition (BoTorch only) - 'qEHVI', 'qNEHVI': Multi-objective acquisition (BoTorch only)</p> <code>'EI'</code> <code>goal</code> <code>Union[str, List[str]]</code> <p>'maximize' or 'minimize' (str), or list of per-objective directions</p> <code>'maximize'</code> <code>n_suggestions</code> <code>int</code> <p>Number of suggestions (batch acquisition)</p> <code>1</code> <code>ref_point</code> <code>Optional[List[float]]</code> <p>Reference point for MOBO hypervolume (list of floats, optional)</p> <code>None</code> <code>**kwargs</code> <p>Strategy-specific parameters:</p> <p>Sklearn backend: - xi (float): Exploration parameter for EI/PI (default: 0.01) - kappa (float): Exploration parameter for UCB (default: 1.96)</p> <p>BoTorch backend: - beta (float): Exploration parameter for UCB (default: 0.5) - mc_samples (int): Monte Carlo samples for batch acquisition (default: 128)</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with suggested experiment(s)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Single-objective\n&gt;&gt;&gt; next_point = session.suggest_next(strategy='EI', goal='maximize')\n</code></pre> <pre><code>&gt;&gt;&gt; # Multi-objective\n&gt;&gt;&gt; suggestions = session.suggest_next(\n...     strategy='qNEHVI',\n...     goal=['maximize', 'maximize'],\n...     ref_point=[0.0, 0.0]\n... )\n</code></pre>"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession.predict","title":"<code>predict(inputs)</code>","text":"<p>Make predictions at new points.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>DataFrame</code> <p>DataFrame with input features</p> required <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, ndarray], Dict[str, Tuple[ndarray, ndarray]]]</code> <p>Single-objective: Tuple of (predictions, uncertainties)</p> <code>Union[Tuple[ndarray, ndarray], Dict[str, Tuple[ndarray, ndarray]]]</code> <p>Multi-objective: dict[str, tuple[ndarray, ndarray]] keyed by objective name</p>"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession.save_session","title":"<code>save_session(filepath)</code>","text":"<p>Save complete session state to JSON file.</p> <p>Saves all session data including: - Session metadata (name, description, tags) - Search space definition - Experimental data - Trained model state (if available) - Complete audit log</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to save session file (.json extension recommended)</p> required Example <p>session.save_session(\"~/ALchemist_Sessions/catalyst_study_nov2025.json\")</p>"},{"location":"api/session_class/#alchemist_core.session.OptimizationSession.load_session","title":"<code>load_session(filepath=None, retrain_on_load=True)</code>","text":"<p>Load session from JSON file.</p> <p>This method works both as a static method (creating a new session) and as an instance method (loading into existing session):</p> <p>Static usage (returns new session):     &gt; session = OptimizationSession.load_session(\"my_session.json\")</p> <p>Instance usage (loads into existing session):     &gt; session = OptimizationSession()     &gt; session.load_session(\"my_session.json\")     &gt; # session.experiment_manager.df is now populated</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to session file (required when called as static method,      can be self when called as instance method)</p> <code>None</code> <code>retrain_on_load</code> <code>bool</code> <p>Whether to retrain model if config exists (default: True)</p> <code>True</code> <p>Returns:</p> Type Description <code>OptimizationSession</code> <p>OptimizationSession (new or modified instance)</p>"},{"location":"api/session_class/#related-classes","title":"Related Classes","text":""},{"location":"api/session_class/#experimentmanager","title":"ExperimentManager","text":"<p>Manages experimental data storage and retrieval.</p>"},{"location":"api/session_class/#alchemist_core.data.experiment_manager.ExperimentManager","title":"<code>alchemist_core.data.experiment_manager.ExperimentManager(search_space=None, target_columns=None)</code>","text":"<p>Class for storing and managing experimental data in a consistent way across backends. Provides methods for data access, saving/loading, and conversion to formats needed by different backends.</p> <p>Supports both single-objective and multi-objective optimization: - Single-objective: Uses single target column (default: 'Output', but configurable) - Multi-objective: Uses multiple target columns specified in target_columns attribute</p> <p>The target_column parameter allows flexible column naming to support various CSV formats.</p>"},{"location":"api/session_class/#alchemist_core.data.experiment_manager.ExperimentManager.add_experiment","title":"<code>add_experiment(point_dict, output_value=None, noise_value=None, iteration=None, reason=None)</code>","text":"<p>Add a single experiment point.</p> <p>Parameters:</p> Name Type Description Default <code>point_dict</code> <code>Dict[str, Union[float, str, int]]</code> <p>Dictionary with variable names as keys and values</p> required <code>output_value</code> <code>Optional[float]</code> <p>The experiment output/target value (if known)</p> <code>None</code> <code>noise_value</code> <code>Optional[float]</code> <p>Optional observation noise/uncertainty value for regularization</p> <code>None</code> <code>iteration</code> <code>Optional[int]</code> <p>Iteration number (auto-assigned if None)</p> <code>None</code> <code>reason</code> <code>Optional[str]</code> <p>Reason for this experiment (e.g., 'Initial Design (LHS)', 'Expected Improvement')</p> <code>None</code>"},{"location":"api/session_class/#alchemist_core.data.experiment_manager.ExperimentManager.get_data","title":"<code>get_data()</code>","text":"<p>Get the raw experiment data.</p>"},{"location":"api/session_class/#alchemist_core.data.experiment_manager.ExperimentManager.get_features_and_target","title":"<code>get_features_and_target()</code>","text":"<p>Get features (X) and target (y) separated.</p> <p>Returns:</p> Name Type Description <code>X</code> <code>DataFrame</code> <p>Features DataFrame</p> <code>y</code> <code>Series</code> <p>Target Series</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configured target column is not found in data</p>"},{"location":"api/session_class/#alchemist_core.data.experiment_manager.ExperimentManager.has_noise_data","title":"<code>has_noise_data()</code>","text":"<p>Check if the experiment data includes noise values.</p>"},{"location":"api/session_class/#alchemist_core.data.experiment_manager.ExperimentManager.from_csv","title":"<code>from_csv(filepath, search_space=None)</code>  <code>classmethod</code>","text":"<p>Class method to create an ExperimentManager from a CSV file.</p>"},{"location":"api/session_class/#see-also","title":"See Also","text":"<ul> <li>SearchSpace - Variable space management</li> <li>Models - Gaussian Process models</li> <li>Acquisition - Acquisition functions</li> </ul>"},{"location":"background/bayesian_optimization/","title":"Bayesian Optimization","text":""},{"location":"background/bayesian_optimization/#introduction-to-bayesian-optimization","title":"Introduction to Bayesian Optimization","text":"<p>Bayesian Optimization (BO) is a method for efficiently optimizing complex systems, particularly when experiments or evaluations are expensive, time-consuming, or resource-intensive. It is widely used in scientific and engineering research to identify optimal conditions or parameters with minimal experimentation. To understand how BO works and why it is so effective, we need to explore its foundation in probabilistic modeling and how it leverages uncertainty to guide decision-making.</p>"},{"location":"background/bayesian_optimization/#why-use-bayesian-optimization","title":"Why Use Bayesian Optimization?","text":"<p>In many real-world problems, the objective function (the thing you want to optimize) is:</p> <ul> <li> <p>Expensive to evaluate: Each experiment or simulation may require significant time, materials, or computational resources.</p> </li> <li> <p>Black-box in nature: You may not have an explicit mathematical formula for the objective function, only the ability to measure its output for given inputs.</p> </li> <li> <p>Noisy: Experimental results may vary due to measurement errors or uncontrollable factors.</p> </li> </ul> <p>Traditional optimization methods, such as grid search or brute-force sampling, are inefficient in these scenarios because they require a large number of evaluations. Bayesian Optimization, on the other hand, is designed to minimize the number of evaluations by intelligently selecting the most informative experiments to perform.</p>"},{"location":"background/bayesian_optimization/#the-core-idea-probabilistic-modeling","title":"The Core Idea: Probabilistic Modeling","text":"<p>At the heart of Bayesian Optimization is a probabilistic model of the objective function. Unlike many machine learning models (e.g., neural networks or decision trees) that provide a single prediction for a given input, probabilistic models output a distribution of possible values. This distribution captures both the predicted value and the uncertainty in that prediction.</p>"},{"location":"background/bayesian_optimization/#gaussian-processes-gps","title":"Gaussian Processes (GPs)","text":"<p>The most common probabilistic model used in Bayesian Optimization is the Gaussian Process (GP). A GP is a flexible and powerful tool for modeling unknown functions. It assumes that the objective function can be described as a random process, where any finite set of points follows a multivariate Gaussian distribution.</p>"},{"location":"background/bayesian_optimization/#key-features-of-gaussian-processes","title":"Key Features of Gaussian Processes","text":"<ol> <li> <p>Mean Function: Represents the model's best guess for the objective function at any given point \\(x\\), denoted as \\(\\mu(x)\\).</p> </li> <li> <p>Covariance Function (Kernel): Describes how points in the input space are related, denoted as \\(k(x, x')\\). For example, points closer together are often assumed to have similar objective values. Common kernels include the Radial Basis Function (RBF) and Mat\u00e9rn kernels.</p> </li> <li> <p>Uncertainty Quantification: For each input \\(x\\), the GP provides both a predicted mean \\(\\mu(x)\\) and a standard deviation \\(\\sigma(x)\\), giving a full probability distribution for the objective value: \\(f(x) \\sim \\mathcal{N}(\\mu(x), \\sigma^2(x))\\).</p> </li> </ol> <p>This ability to quantify uncertainty is what sets GPs apart from many other machine learning models, such as neural networks or support vector machines, which typically provide only point estimates.</p>"},{"location":"background/bayesian_optimization/#how-bayesian-optimization-works","title":"How Bayesian Optimization Works","text":"<p>Bayesian Optimization uses the probabilistic model (e.g., a GP) to guide the search for the optimal input parameters. Here's how it works step by step:</p>"},{"location":"background/bayesian_optimization/#1-define-the-variable-space","title":"1. Define the Variable Space","text":"<p>The first step is to define the input variables (e.g., temperature, pressure, composition) and their ranges. These variables form the \"search space\" for optimization.</p>"},{"location":"background/bayesian_optimization/#2-build-the-surrogate-model","title":"2. Build the Surrogate Model","text":"<p>The surrogate model (e.g., a GP) is trained on a small set of initial data points. This model approximates the objective function and provides predictions with associated uncertainties.</p>"},{"location":"background/bayesian_optimization/#3-evaluate-the-acquisition-function","title":"3. Evaluate the Acquisition Function","text":"<p>The acquisition function is a mathematical rule that determines the next point to evaluate. It uses the surrogate model's predictions and uncertainties to balance two competing goals:</p> <ul> <li> <p>Exploration: Testing regions of the search space with high uncertainty to learn more about the objective function.</p> </li> <li> <p>Exploitation: Testing regions likely to yield high objective values based on current knowledge.</p> </li> </ul>"},{"location":"background/bayesian_optimization/#common-acquisition-functions","title":"Common Acquisition Functions","text":"<p>Expected Improvement (EI): Measures the expected improvement over the current best objective value. EI favors points with high predicted values and/or high uncertainty.</p> \\[ \\mathrm{EI}(x) = \\mathbb{E} \\left[ \\max(0, f(x) - f_\\text{best}) \\right] \\] <p>Probability of Improvement (PI): Focuses on the probability that a point will improve upon the current best value, where \\(\\Phi\\) is the cumulative distribution function of the standard normal distribution.</p> \\[ \\mathrm{PI}(x) = \\Phi \\left( \\frac{\\mu(x) - f_\\text{best}}{\\sigma(x)} \\right) \\] <p>Upper Confidence Bound (UCB): Balances exploration and exploitation by considering both the predicted mean and uncertainty, where \\(\\kappa\\) is a tunable parameter that controls the exploration-exploitation tradeoff.</p> \\[ \\mathrm{UCB}(x) = \\mu(x) + \\kappa \\cdot \\sigma(x) \\]"},{"location":"background/bayesian_optimization/#4-perform-the-experiment","title":"4. Perform the Experiment","text":"<p>The next experiment is conducted at the point suggested by the acquisition function, and the result is added to the dataset.</p>"},{"location":"background/bayesian_optimization/#5-update-the-model","title":"5. Update the Model","text":"<p>The surrogate model is retrained with the new data, refining its predictions and uncertainties.</p>"},{"location":"background/bayesian_optimization/#6-repeat-until-convergence","title":"6. Repeat Until Convergence","text":"<p>Steps 3\u20135 are repeated until the objective is optimized or resources are exhausted.</p>"},{"location":"background/bayesian_optimization/#why-probabilistic-modeling-matters","title":"Why Probabilistic Modeling Matters","text":"<p>The use of probabilistic models like GPs is what makes Bayesian Optimization so effective. By modeling uncertainty, GPs allow the optimization process to:</p> <ul> <li> <p>Focus on promising regions: Exploit areas likely to yield high objective values.</p> </li> <li> <p>Explore unknown regions: Avoid getting stuck in local optima by testing areas with high uncertainty.</p> </li> <li> <p>Adapt to noisy data: Account for variability in experimental results.</p> </li> </ul> <p>This probabilistic approach ensures that every experiment contributes valuable information, making Bayesian Optimization highly efficient.</p>"},{"location":"background/bayesian_optimization/#example-optimizing-a-chemical-reaction","title":"Example: Optimizing a Chemical Reaction","text":"<p>Imagine you are optimizing a chemical reaction to maximize yield. The input variables are temperature, pressure, and catalyst loading, and the objective is the reaction yield. Each experiment is expensive, so you want to minimize the number of trials.</p> <ol> <li>Initial Data: Start with a few experiments at random conditions.</li> <li>Surrogate Model: Use a GP to model the relationship between the input variables and yield.</li> <li>Acquisition Function: Evaluate the acquisition function to select the next set of conditions to test.</li> <li>Experiment: Conduct the experiment and measure the yield.</li> <li>Update: Add the new data to the GP and repeat.</li> </ol> <p>Over time, Bayesian Optimization will focus on the most promising conditions, efficiently identifying the optimal reaction parameters.</p>"},{"location":"background/bayesian_optimization/#learn-more","title":"Learn More","text":"<p>For a visual and engaging overview of Bayesian Optimization, watch this excellent video by Taylor Sparks, Professor of Materials Science &amp; Engineering at the University of Utah: </p>"},{"location":"background/bayesian_optimization/#summary","title":"Summary","text":"<p>Bayesian Optimization is a powerful framework for optimizing complex systems with minimal experimentation. By leveraging probabilistic models like Gaussian Processes, it intelligently balances exploration and exploitation, making it ideal for applications where experiments are costly or time-consuming. Whether you're optimizing a chemical reaction, designing a new material, or tuning a process, Bayesian Optimization can help you achieve your goals more efficiently and with fewer resources.</p>"},{"location":"background/interpreting_calibration/","title":"Calibration Curves","text":""},{"location":"background/interpreting_calibration/#interpreting-calibration-curves-for-uncertainty-assessment","title":"Interpreting Calibration Curves for Uncertainty Assessment","text":"<p>A calibration curve (also called a reliability diagram) is a diagnostic tool that helps you evaluate whether your Gaussian Process model's predicted confidence intervals have the correct coverage. In ALchemist, the calibration curve complements the Q-Q plot to provide a comprehensive view of your model's uncertainty quality.</p>"},{"location":"background/interpreting_calibration/#what-is-coverage-calibration","title":"What is Coverage Calibration?","text":"<p>When a Gaussian Process provides a prediction with uncertainty, it defines confidence intervals at various levels:</p> <ul> <li> <p>68% confidence interval: \u03bc \u00b1 1\u03c3 should contain 68% of true values</p> </li> <li> <p>95% confidence interval: \u03bc \u00b1 1.96\u03c3 should contain 95% of true values</p> </li> <li> <p>99% confidence interval: \u03bc \u00b1 2.58\u03c3 should contain 99% of true values</p> </li> </ul> <p>Well-calibrated coverage means that the empirical (observed) coverage matches the nominal (claimed) coverage. For example, if your model claims 95% confidence, then 95% of experimental observations should actually fall within those bounds.</p>"},{"location":"background/interpreting_calibration/#why-coverage-calibration-matters","title":"Why Coverage Calibration Matters","text":"<p>Accurate coverage is essential for:</p> <ul> <li> <p>Reliable decision-making: Knowing when predictions are trustworthy</p> </li> <li> <p>Risk management: Avoiding over-confident predictions in safety-critical applications</p> </li> <li> <p>Efficient exploration: Balancing exploration and exploitation in optimization</p> </li> <li> <p>Experimental planning: Determining when more data is needed vs. when to act on predictions</p> </li> </ul>"},{"location":"background/interpreting_calibration/#understanding-the-calibration-curve","title":"Understanding the Calibration Curve","text":""},{"location":"background/interpreting_calibration/#components-of-the-visualization","title":"Components of the Visualization","text":"<p>ALchemist's calibration curve display includes:</p> <ol> <li>Line plot: Nominal coverage (x-axis) vs. empirical coverage (y-axis)</li> <li>Perfect calibration line: Diagonal reference (y = x)</li> <li>Metrics table: Coverage values at standard confidence levels</li> <li>Color-coded status: Visual indicators for calibration quality</li> </ol>"},{"location":"background/interpreting_calibration/#reading-the-plot","title":"Reading the Plot","text":"<p>X-axis (Nominal Coverage): The confidence level claimed by the model (e.g., 0.68 = 68% confidence)</p> <p>Y-axis (Empirical Coverage): The actual fraction of observations that fall within the predicted intervals</p> <p>Perfect calibration: Points lie on the diagonal line (y = x)</p>"},{"location":"background/interpreting_calibration/#interpreting-coverage-patterns","title":"Interpreting Coverage Patterns","text":""},{"location":"background/interpreting_calibration/#well-calibrated-model","title":"Well-Calibrated Model","text":"<p>What it looks like:</p> <ul> <li> <p>Points closely follow the diagonal line</p> </li> <li> <p>Empirical \u2248 Nominal at all confidence levels</p> </li> <li> <p>Status indicators show \"Good\" (green)</p> </li> </ul> <p>Metrics example: </p><pre><code>Confidence   Nominal   Empirical   Status\n68%          0.68      0.67        \u2713 Good\n95%          0.95      0.94        \u2713 Good\n99%          0.99      0.98        \u2713 Good\n99.7%        0.997     0.995       \u2713 Good\n</code></pre><p></p> <p>What it means:</p> <ul> <li> <p>Model uncertainties accurately reflect prediction errors</p> </li> <li> <p>Confidence intervals have correct coverage</p> </li> <li> <p>Safe to trust model predictions and uncertainties</p> </li> <li> <p>Acquisition functions will make optimal decisions</p> </li> </ul>"},{"location":"background/interpreting_calibration/#over-confident-model","title":"Over-Confident Model","text":"<p>What it looks like:</p> <ul> <li> <p>Curve below the diagonal line</p> </li> <li> <p>Empirical coverage &lt; Nominal coverage</p> </li> <li> <p>Status indicators show \"Under-conf\" (orange/red)</p> </li> </ul> <p>Metrics example: </p><pre><code>Confidence   Nominal   Empirical   Status\n68%          0.68      0.55        Under-conf\n95%          0.95      0.82        Under-conf\n99%          0.99      0.91        Under-conf\n99.7%        0.997     0.95        Under-conf\n</code></pre><p></p> <p>What it means:</p> <ul> <li> <p>Model is too confident in its predictions</p> </li> <li> <p>Claimed \"95% confidence\" only captures 82% of observations</p> </li> <li> <p>Prediction intervals are too narrow</p> </li> <li> <p>Risk of missing optimal regions by over-exploiting</p> </li> </ul> <p>Why it happens:</p> <ul> <li> <p>Model underestimates noise in the data</p> </li> <li> <p>Kernel is too restrictive (overfit to training data)</p> </li> <li> <p>Insufficient data for problem complexity</p> </li> <li> <p>Lengthscales too small (overfitting local variations)</p> </li> </ul> <p>How to fix: 1. Increase noise parameter: If using noise column, increase values 2. Regularization: Add explicit noise term to model 3. Change kernel: Try more flexible kernel (Matern \u03bd=1.5 instead of \u03bd=2.5) 4. Collect more data: Especially in high-variance regions 5. Apply calibration: ALchemist automatically applies calibration corrections 6. Check data quality: Look for outliers or measurement errors</p>"},{"location":"background/interpreting_calibration/#under-confident-model","title":"Under-Confident Model","text":"<p>What it looks like:</p> <ul> <li> <p>Curve above the diagonal line</p> </li> <li> <p>Empirical coverage &gt; Nominal coverage</p> </li> <li> <p>Status indicators show \"Over-conf\" (blue)</p> </li> </ul> <p>Metrics example: </p><pre><code>Confidence   Nominal   Empirical   Status\n68%          0.68      0.78        Over-conf\n95%          0.95      0.99        Over-conf\n99%          0.99      1.00        Over-conf\n99.7%        0.997     1.00        Over-conf\n</code></pre><p></p> <p>What it means:</p> <ul> <li> <p>Model is too cautious with its predictions</p> </li> <li> <p>Claimed \"95% confidence\" actually captures 99% of observations</p> </li> <li> <p>Prediction intervals are too wide</p> </li> <li> <p>Risk of over-exploring, wasting experiments on unnecessary regions</p> </li> </ul> <p>Why it happens:</p> <ul> <li> <p>Model overestimates noise in the data</p> </li> <li> <p>Kernel is too flexible (underfit)</p> </li> <li> <p>Lengthscales too large (over-smoothing)</p> </li> <li> <p>Prior distributions too broad</p> </li> </ul> <p>How to fix: 1. Decrease noise parameter: Reduce explicit noise values 2. Tighter kernel: Try less flexible kernel (Matern \u03bd=2.5 or RBF) 3. Optimize hyperparameters: Ensure lengthscales are optimized, not fixed too large 4. Check preprocessing: Ensure data is properly scaled 5. More aggressive optimization: Increase training iterations</p> <p>Note: Under-confidence is generally less problematic than over-confidence, but wastes experimental resources.</p>"},{"location":"background/interpreting_calibration/#mixed-calibration-issues","title":"Mixed Calibration Issues \ud83d\udd04","text":"<p>What it looks like:</p> <ul> <li> <p>Curve crosses the diagonal line</p> </li> <li> <p>Some confidence levels over-confident, others under-confident</p> </li> <li> <p>Inconsistent status indicators</p> </li> </ul> <p>Metrics example: </p><pre><code>Confidence   Nominal   Empirical   Status\n68%          0.68      0.62        Under-conf\n95%          0.95      0.96        Good\n99%          0.99      1.00        Over-conf\n99.7%        0.997     1.00        Over-conf\n</code></pre><p></p> <p>What it means:</p> <ul> <li> <p>Uncertainty estimates have non-normal distribution</p> </li> <li> <p>May indicate model misspecification</p> </li> <li> <p>Different behavior in tails vs. center of distribution</p> </li> </ul> <p>How to fix: 1. Check data distribution: Look for outliers or bimodality 2. Transform outputs: Consider log or Box-Cox transformation 3. Different kernel: Experiment with alternative kernel families 4. Stratified sampling: Ensure training data covers full range</p>"},{"location":"background/interpreting_calibration/#sample-size-considerations","title":"Sample Size Considerations","text":""},{"location":"background/interpreting_calibration/#small-datasets-n-30","title":"Small Datasets (N &lt; 30) \ud83d\udd0d","text":"<ul> <li> <p>High variability in empirical coverage estimates</p> </li> <li> <p>Coverage metrics less reliable</p> </li> <li> <p>\u00b110% deviation from nominal is common</p> </li> <li> <p>Focus on overall trends rather than exact values</p> </li> </ul> <p>Interpretation guidance: </p><pre><code>Empirical coverage of 0.85 for nominal 0.95 is acceptable with N=20\nSame coverage would be concerning with N=100\n</code></pre><p></p>"},{"location":"background/interpreting_calibration/#medium-datasets-30-n-100","title":"Medium Datasets (30 &lt; N &lt; 100)","text":"<ul> <li> <p>Moderate reliability in coverage estimates</p> </li> <li> <p>\u00b15% deviation becoming significant</p> </li> <li> <p>Clear patterns indicate real issues</p> </li> </ul>"},{"location":"background/interpreting_calibration/#large-datasets-n-100","title":"Large Datasets (N &gt; 100) \ud83c\udfaf","text":"<ul> <li> <p>High confidence in calibration assessment</p> </li> <li> <p>Even small deviations (\u00b13%) may indicate issues</p> </li> <li> <p>Coverage metrics are highly reliable</p> </li> </ul> <p>ALchemist displays a warning for N &lt; 30: </p><pre><code>Note: Small sample size (N=25). Coverage estimates may be unreliable.\n</code></pre><p></p>"},{"location":"background/interpreting_calibration/#calibration-status-indicators","title":"Calibration Status Indicators","text":"<p>ALchemist uses color-coded status indicators for quick assessment:</p> Status Color Criterion Interpretation \u2713 Good Green |Empirical - Nominal| &lt; 0.05 Well-calibrated Under-conf Orange Empirical &lt; Nominal - 0.05 Too confident (narrow intervals) Over-conf Blue Empirical &gt; Nominal + 0.05 Too cautious (wide intervals) <p>These thresholds are adjustable based on sample size and application requirements.</p>"},{"location":"background/interpreting_calibration/#using-calibration-with-q-q-plots","title":"Using Calibration with Q-Q Plots","text":"<p>The calibration curve and Q-Q plot provide complementary information:</p>"},{"location":"background/interpreting_calibration/#q-q-plot-strengths","title":"Q-Q Plot Strengths:","text":"<ul> <li> <p>Tests normality assumption</p> </li> <li> <p>Detects bias (Mean(z) \u2260 0)</p> </li> <li> <p>Shows over/under-confidence via Std(z)</p> </li> <li> <p>Visual pattern recognition</p> </li> </ul>"},{"location":"background/interpreting_calibration/#calibration-curve-strengths","title":"Calibration Curve Strengths:","text":"<ul> <li> <p>Quantifies coverage at specific confidence levels</p> </li> <li> <p>Easier to interpret numerically</p> </li> <li> <p>Less sensitive to distributional assumptions</p> </li> <li> <p>Direct link to decision-making thresholds</p> </li> </ul>"},{"location":"background/interpreting_calibration/#combined-analysis","title":"Combined Analysis:","text":"<p>Both good: Model is well-calibrated and uncertainties are reliable</p> <p>Q-Q bad, Calibration good: Non-normal errors but coverage is correct (acceptable for many applications)</p> <p>Q-Q good, Calibration bad: Distribution is normal but variance is miscalibrated (systematic scaling issue)</p> <p>Both bad: Significant model misspecification (investigate data and model choices)</p>"},{"location":"background/interpreting_calibration/#practical-guidelines","title":"Practical Guidelines","text":""},{"location":"background/interpreting_calibration/#when-to-trust-your-model","title":"When to Trust Your Model","text":"<p>Proceed with confidence if:</p> <ul> <li> <p>All coverage metrics within \u00b15% of nominal (for N &gt; 30)</p> </li> <li> <p>Status indicators show \"Good\" or mild \"Over-conf\"</p> </li> <li> <p>Calibration curve closely follows diagonal</p> </li> <li> <p>Q-Q plot also shows good calibration</p> </li> </ul>"},{"location":"background/interpreting_calibration/#when-to-improve-calibration","title":"When to Improve Calibration","text":"<p>Take corrective action if:</p> <ul> <li> <p>Multiple \"Under-conf\" indicators (model too confident)</p> </li> <li> <p>Large deviations (&gt;10%) from nominal coverage</p> </li> <li> <p>Consistent pattern across all confidence levels</p> </li> <li> <p>Sample size is adequate (N &gt; 30) for reliable assessment</p> </li> </ul>"},{"location":"background/interpreting_calibration/#when-to-collect-more-data","title":"When to Collect More Data","text":"<p>Consider more experiments if:</p> <ul> <li> <p>Sample size is small (N &lt; 30) with unclear patterns</p> </li> <li> <p>High variance in coverage estimates</p> </li> <li> <p>Model appears underfit (high RMSE, low R\u00b2)</p> </li> <li> <p>Coverage is acceptable but prediction accuracy is poor</p> </li> </ul>"},{"location":"background/interpreting_calibration/#calibration-in-active-learning-context","title":"Calibration in Active Learning Context","text":"<p>During Bayesian optimization, calibration affects:</p>"},{"location":"background/interpreting_calibration/#acquisition-function-performance","title":"Acquisition Function Performance:","text":"<ul> <li> <p>Expected Improvement: Relies on accurate \u03c3 for exploration/exploitation balance</p> </li> <li> <p>UCB: Directly uses \u03c3 in the acquisition formula</p> </li> <li> <p>Probability of Improvement: Needs correct uncertainty quantification</p> </li> </ul>"},{"location":"background/interpreting_calibration/#optimization-strategy","title":"Optimization Strategy:","text":"<ul> <li> <p>Over-confident model: May converge prematurely to local optima (too much exploitation)</p> </li> <li> <p>Under-confident model: May waste experiments exploring known regions (too much exploration)</p> </li> <li> <p>Well-calibrated model: Optimal balance, efficient convergence</p> </li> </ul>"},{"location":"background/interpreting_calibration/#stopping-criteria","title":"Stopping Criteria:","text":"<ul> <li> <p>Calibrated uncertainties help determine when optimization has converged</p> </li> <li> <p>Under-confident models may never reach stopping criteria</p> </li> <li> <p>Over-confident models may stop too early</p> </li> </ul>"},{"location":"background/interpreting_calibration/#alchemists-automatic-calibration","title":"ALchemist's Automatic Calibration","text":"<p>ALchemist implements automatic uncertainty calibration:</p> <ol> <li>Cross-validation: Computes z-scores from CV predictions</li> <li>Calibration factor: Calculates <code>s = Std(z)</code> from residuals</li> <li>Scaling: Multiplies predicted \u03c3 by calibration factor</li> <li>Application: Automatically applied to future predictions</li> </ol> <p>Effect:</p> <ul> <li> <p>If Std(z) = 1.5 (over-confident), future \u03c3 predictions are scaled by 1.5\u00d7</p> </li> <li> <p>If Std(z) = 0.7 (under-confident), future \u03c3 predictions are scaled by 0.7\u00d7</p> </li> <li> <p>Brings model toward better calibration without retraining</p> </li> </ul> <p>Toggle:</p> <ul> <li> <p>Calibrated vs. uncalibrated results viewable in visualization panel</p> </li> <li> <p>Compare to see calibration impact on your specific dataset</p> </li> </ul>"},{"location":"background/interpreting_calibration/#summary","title":"Summary","text":"Pattern Coverage vs. Diagonal Issue Primary Fix On diagonal Aligned \u2713 None - Below diagonal Empirical &lt; Nominal Over-confident Increase noise/uncertainty Above diagonal Empirical &gt; Nominal Under-confident Reduce noise, tighter kernel Crosses diagonal Mixed Model misspecification Check data, try different kernel High scatter Variable Small sample Collect more data"},{"location":"background/interpreting_calibration/#further-reading","title":"Further Reading","text":"<ul> <li> <p>Q-Q Plot Interpretation - Complementary diagnostic for normality</p> </li> <li> <p>Model Performance - Overall model quality assessment</p> </li> <li> <p>Calibration Curve Visualization - How to generate and use the plot in ALchemist</p> </li> </ul> <p>For theoretical background on uncertainty calibration:</p> <ul> <li> <p>Guo et al. (2017), \"On Calibration of Modern Neural Networks\"</p> </li> <li> <p>Kuleshov et al. (2018), \"Accurate Uncertainties for Deep Learning Using Calibrated Regression\"</p> </li> <li> <p>DeGroot &amp; Fienberg (1983), \"The Comparison and Evaluation of Forecasters\"</p> </li> </ul>"},{"location":"background/interpreting_qqplot/","title":"Q-Q Plots","text":""},{"location":"background/interpreting_qqplot/#interpreting-q-q-plots-for-uncertainty-calibration","title":"Interpreting Q-Q Plots for Uncertainty Calibration","text":"<p>A Q-Q plot (quantile-quantile plot) is a diagnostic tool for assessing whether the uncertainty estimates from your Gaussian Process model are well-calibrated. In ALchemist, the Q-Q plot helps you determine if your model's predicted uncertainties accurately reflect the true prediction errors.</p>"},{"location":"background/interpreting_qqplot/#what-is-uncertainty-calibration","title":"What is Uncertainty Calibration?","text":"<p>When a Gaussian Process predicts an output value, it also provides a measure of uncertainty (standard deviation). Well-calibrated uncertainty means that:</p> <ul> <li> <p>If the model says there's a 68% chance the true value is within \u00b11\u03c3, then approximately 68% of predictions should fall within that range</p> </li> <li> <p>If the model says there's a 95% chance the true value is within \u00b12\u03c3, then approximately 95% should fall within that range</p> </li> </ul> <p>Calibration is critical for:</p> <ul> <li> <p>Decision-making: Reliable uncertainties help determine when to trust predictions vs. run more experiments</p> </li> <li> <p>Acquisition functions: Methods like Expected Improvement and UCB rely on accurate uncertainty estimates</p> </li> <li> <p>Risk assessment: Understanding prediction confidence for safety-critical applications</p> </li> </ul>"},{"location":"background/interpreting_qqplot/#understanding-the-q-q-plot","title":"Understanding the Q-Q Plot","text":""},{"location":"background/interpreting_qqplot/#what-the-plot-shows","title":"What the Plot Shows","text":"<p>The Q-Q plot in ALchemist displays:</p> <ul> <li> <p>X-axis: Theoretical quantiles from a standard normal distribution \\(\\mathcal{N}(0, 1)\\)</p> </li> <li> <p>Y-axis: Standardized residuals (z-scores) from cross-validation predictions</p> </li> <li> <p>Diagonal reference line: Perfect calibration (y = x)</p> </li> <li> <p>Confidence band (for small samples, N &lt; 100): Expected deviation range due to finite sample size</p> </li> <li> <p>Diagnostic metrics: Mean(z) and Std(z) displayed on the plot</p> </li> </ul>"},{"location":"background/interpreting_qqplot/#standardized-residuals-z-scores","title":"Standardized Residuals (Z-scores)","text":"<p>For each cross-validation prediction, the z-score is calculated as:</p> \\[ z_i = \\frac{y_i^{\\text{true}} - y_i^{\\text{pred}}}{\\sigma_i} \\] <p>Where:</p> <ul> <li> <p>\\(y_i^{\\text{true}}\\) = actual experimental value</p> </li> <li> <p>\\(y_i^{\\text{pred}}\\) = model prediction</p> </li> <li> <p>\\(\\sigma_i\\) = predicted standard deviation</p> </li> </ul> <p>If uncertainties are well-calibrated, these z-scores should follow a standard normal distribution \\(\\mathcal{N}(0, 1)\\).</p>"},{"location":"background/interpreting_qqplot/#interpreting-the-plot","title":"Interpreting the Plot","text":""},{"location":"background/interpreting_qqplot/#perfect-calibration","title":"Perfect Calibration \ud83c\udfaf","text":"<p>What it looks like:</p> <ul> <li> <p>Points closely follow the diagonal line (y = x)</p> </li> <li> <p>Mean(z) \u2248 0.0</p> </li> <li> <p>Std(z) \u2248 1.0</p> </li> <li> <p>Points within confidence band</p> </li> </ul> <p>What it means:</p> <ul> <li> <p>Model uncertainties accurately reflect prediction errors</p> </li> <li> <p>68% of predictions within \u00b11\u03c3, 95% within \u00b12\u03c3 (as expected)</p> </li> <li> <p>Acquisition functions will make optimal decisions</p> </li> </ul> <p>Example: </p><pre><code>Mean(z) = 0.02\nStd(z) = 0.98\nStatus: \u2713 Well-calibrated\n</code></pre><p></p>"},{"location":"background/interpreting_qqplot/#over-confident-predictions","title":"Over-Confident Predictions","text":"<p>What it looks like:</p> <ul> <li> <p>Points systematically above the diagonal line</p> </li> <li> <p>Std(z) &gt; 1.0 (e.g., 1.5, 2.0, or higher)</p> </li> <li> <p>Residuals are larger than predicted uncertainties</p> </li> </ul> <p>What it means:</p> <ul> <li> <p>Model is too confident in its predictions</p> </li> <li> <p>Actual errors are larger than the model thinks they are</p> </li> <li> <p>The model predicts \u03c3 = 2, but actual error is \u03c3 = 4</p> </li> <li> <p>Risk of over-exploiting regions that may not be optimal</p> </li> </ul> <p>Why it happens:</p> <ul> <li> <p>Insufficient model complexity (kernel too simple)</p> </li> <li> <p>Underestimated noise in the data</p> </li> <li> <p>Not enough data for the problem complexity</p> </li> <li> <p>Overfitting to training data</p> </li> </ul> <p>How to fix:</p> <ul> <li> <p>Try a more flexible kernel (e.g., Matern with lower \u03bd)</p> </li> <li> <p>Increase model noise parameter (if using noise column)</p> </li> <li> <p>Collect more training data</p> </li> <li> <p>Apply uncertainty calibration (ALchemist does this automatically)</p> </li> </ul> <p>Example: </p><pre><code>Mean(z) = -0.05\nStd(z) = 1.45\nStatus: Over-confident (model uncertainties too small)\n</code></pre><p></p>"},{"location":"background/interpreting_qqplot/#under-confident-predictions","title":"Under-Confident Predictions","text":"<p>What it looks like:</p> <ul> <li> <p>Points systematically below the diagonal line</p> </li> <li> <p>Std(z) &lt; 1.0 (e.g., 0.6, 0.7, 0.8)</p> </li> <li> <p>Residuals are smaller than predicted uncertainties</p> </li> </ul> <p>What it means:</p> <ul> <li> <p>Model is too uncertain about its predictions</p> </li> <li> <p>The model predicts \u03c3 = 4, but actual error is \u03c3 = 2</p> </li> <li> <p>Predictions are more accurate than the model believes</p> </li> <li> <p>Risk of over-exploring, wasting experiments on unnecessary regions</p> </li> </ul> <p>Why it happens:</p> <ul> <li> <p>Model is overly conservative</p> </li> <li> <p>Noise parameter set too high</p> </li> <li> <p>Kernel lengthscales too large (oversmoothing)</p> </li> <li> <p>Small dataset with conservative priors</p> </li> </ul> <p>How to fix:</p> <ul> <li> <p>Try a more restrictive kernel (e.g., Matern with higher \u03bd)</p> </li> <li> <p>Reduce model noise parameter</p> </li> <li> <p>Optimize kernel hyperparameters more aggressively</p> </li> <li> <p>Collect more data to reduce inherent uncertainty</p> </li> </ul> <p>Example: </p><pre><code>Mean(z) = 0.08\nStd(z) = 0.72\nStatus: Under-confident (model uncertainties too large)\n</code></pre><p></p>"},{"location":"background/interpreting_qqplot/#systematic-bias","title":"Systematic Bias \ud83d\udd34","text":"<p>What it looks like:</p> <ul> <li> <p>Mean(z) significantly different from 0 (e.g., |Mean(z)| &gt; 0.3)</p> </li> <li> <p>Points shifted up or down from the diagonal</p> </li> <li> <p>Consistent over- or under-prediction</p> </li> </ul> <p>What it means:</p> <ul> <li> <p>Model has systematic bias in predictions</p> </li> <li> <p>Not just a calibration issue\u2014predictions are consistently off</p> </li> <li> <p>Mean(z) &gt; 0: Model consistently under-predicts</p> </li> <li> <p>Mean(z) &lt; 0: Model consistently over-predicts</p> </li> </ul> <p>How to fix:</p> <ul> <li> <p>Check data quality and units</p> </li> <li> <p>Try different kernel types</p> </li> <li> <p>Check for data preprocessing issues</p> </li> <li> <p>Ensure input/output transforms are appropriate</p> </li> <li> <p>Consider adding a mean function or trend</p> </li> </ul> <p>Example: </p><pre><code>Mean(z) = 0.45\nStd(z) = 1.02\nStatus: \ud83d\udd34 Systematic bias (consistent under-prediction)\n</code></pre><p></p>"},{"location":"background/interpreting_qqplot/#sample-size-considerations","title":"Sample Size Considerations","text":""},{"location":"background/interpreting_qqplot/#small-datasets-n-30","title":"Small Datasets (N &lt; 30)","text":"<ul> <li> <p>Expect more scatter around the diagonal</p> </li> <li> <p>Confidence bands are wider</p> </li> <li> <p>Std(z) can deviate from 1.0 more easily</p> </li> <li> <p>Don't over-interpret minor deviations</p> </li> </ul>"},{"location":"background/interpreting_qqplot/#medium-datasets-30-n-100","title":"Medium Datasets (30 &lt; N &lt; 100)","text":"<ul> <li> <p>Narrower confidence bands</p> </li> <li> <p>More reliable calibration assessment</p> </li> <li> <p>Moderate deviations indicate real issues</p> </li> </ul>"},{"location":"background/interpreting_qqplot/#large-datasets-n-100","title":"Large Datasets (N &gt; 100)","text":"<ul> <li> <p>Tight confidence bands</p> </li> <li> <p>High confidence in calibration assessment</p> </li> <li> <p>Even small deviations may indicate issues</p> </li> </ul>"},{"location":"background/interpreting_qqplot/#practical-guidelines","title":"Practical Guidelines","text":""},{"location":"background/interpreting_qqplot/#when-to-worry","title":"When to Worry \ud83d\udea8","text":"<p>Take action if you see:</p> <ul> <li> <p>Std(z) &gt; 1.3 or &lt; 0.7 (with N &gt; 30)</p> </li> <li> <p>|Mean(z)| &gt; 0.3</p> </li> <li> <p>Clear systematic pattern in deviations</p> </li> <li> <p>Points consistently outside confidence band</p> </li> </ul>"},{"location":"background/interpreting_qqplot/#when-not-to-worry","title":"When Not to Worry","text":"<p>Don't be concerned if:</p> <ul> <li> <p>Minor scatter with Std(z) between 0.9 and 1.1</p> </li> <li> <p>Mean(z) between -0.1 and 0.1</p> </li> <li> <p>Points within confidence band (especially for N &lt; 30)</p> </li> <li> <p>Random scatter without systematic pattern</p> </li> </ul>"},{"location":"background/interpreting_qqplot/#relationship-to-calibration-curve","title":"Relationship to Calibration Curve","text":"<p>The Q-Q plot and Calibration Curve are complementary:</p> <ul> <li> <p>Q-Q Plot: Tests if residuals follow normal distribution (are z-scores ~ N(0,1)?)</p> </li> <li> <p>Calibration Curve: Tests if confidence intervals have correct coverage (do 95% intervals contain 95% of points?)</p> </li> </ul> <p>Use both together for comprehensive uncertainty assessment:</p> <ul> <li> <p>Q-Q plot reveals over/under-confidence and bias</p> </li> <li> <p>Calibration curve quantifies coverage at specific confidence levels</p> </li> </ul>"},{"location":"background/interpreting_qqplot/#summary","title":"Summary","text":"Observation Mean(z) Std(z) Interpretation Action Points on diagonal \u2248 0 \u2248 1.0 \u2713 Well-calibrated None needed Points above diagonal \u2248 0 &gt; 1.0 Over-confident Increase uncertainty Points below diagonal \u2248 0 &lt; 1.0 Under-confident Reduce uncertainty Points shifted up &gt; 0 any \ud83d\udd34 Under-predicting Check data/model Points shifted down &lt; 0 any \ud83d\udd34 Over-predicting Check data/model"},{"location":"background/interpreting_qqplot/#further-reading","title":"Further Reading","text":"<ul> <li> <p>Calibration Curve Interpretation - Complementary calibration diagnostic</p> </li> <li> <p>Model Performance - General model assessment guidance</p> </li> <li> <p>Error Metrics - RMSE, MAE, R\u00b2 interpretation</p> </li> </ul> <p>For the mathematical foundations of uncertainty calibration in Gaussian Processes, see:</p> <ul> <li> <p>Kuleshov et al. (2018), \"Accurate Uncertainties for Deep Learning Using Calibrated Regression\"</p> </li> <li> <p>Gneiting et al. (2007), \"Strictly Proper Scoring Rules, Prediction, and Estimation\"</p> </li> </ul>"},{"location":"background/kernels/","title":"Kernels","text":""},{"location":"background/kernels/#kernel-deep-dive-understanding-kernels-in-gaussian-processes","title":"Kernel Deep Dive: Understanding Kernels in Gaussian Processes","text":"<p>Kernels (also called covariance functions) are a fundamental component of Gaussian Processes (GPs). They encode our assumptions about the underlying function we are modeling and play a central role in determining the GP's predictions and uncertainty.</p>"},{"location":"background/kernels/#what-is-a-kernel","title":"What Is a Kernel?","text":"<p>Mathematically, a kernel is a function \\(k(x, x')\\) that defines the similarity or correlation between two points \\(x\\) and \\(x'\\) in the input space. In a GP, the kernel determines the covariance matrix \\(\\mathbf{K}\\) for all pairs of input points, which in turn defines the joint distribution over function values.</p> <p>Given a set of input points \\(\\mathbf{X} = [x_1, x_2, ..., x_n]\\), the GP prior is:</p> \\[ f(\\mathbf{X}) \\sim \\mathcal{N}(\\mu(\\mathbf{X}), \\mathbf{K}) \\] <p>where \\(\\mathbf{K}_{ij} = k(x_i, x_j)\\).</p> <p>Functionally, the kernel controls:</p> <ul> <li> <p>The smoothness and complexity of the functions the GP can model.</p> </li> <li> <p>How information from observed data points influences predictions at new points.</p> </li> <li> <p>The ability to capture periodicity, trends, or other structural properties.</p> </li> </ul>"},{"location":"background/kernels/#common-kernels","title":"Common Kernels","text":""},{"location":"background/kernels/#1-radial-basis-function-rbf-squared-exponential-gaussian-kernel","title":"1. Radial Basis Function (RBF) / Squared Exponential / Gaussian Kernel","text":"<p>The RBF kernel is the most widely used kernel and assumes the function is infinitely smooth.</p> \\[ k_{\\text{RBF}}(x, x') = \\sigma^2 \\exp\\left( -\\frac{||x - x'||^2}{2\\ell^2} \\right) \\] <ul> <li> <p>\\(\\sigma^2\\) is the signal variance (controls overall scale).</p> </li> <li> <p>\\(\\ell\\) is the lengthscale (controls how quickly correlation decays with distance).</p> </li> </ul> <p>Properties:</p> <ul> <li> <p>Produces very smooth functions.</p> </li> <li> <p>Good default for many problems.</p> </li> <li> <p>Implemented in both scikit-optimize and BoTorch.</p> </li> </ul> <p>References: sklearn RBF kernel BoTorch RBF kernel</p>"},{"location":"background/kernels/#2-matern-kernel","title":"2. Matern Kernel","text":"<p>The Matern kernel is a generalization of the RBF kernel with an additional parameter \\(\\nu\\) that controls smoothness.</p> \\[ k_{\\text{Matern}}(x, x') = \\sigma^2 \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left( \\frac{\\sqrt{2\\nu} ||x - x'||}{\\ell} \\right)^\\nu K_\\nu \\left( \\frac{\\sqrt{2\\nu} ||x - x'||}{\\ell} \\right) \\] <ul> <li> <p>\\(\\nu\\) (nu): Smoothness parameter. Common values are 0.5, 1.5, 2.5, and \\(\\infty\\).</p> <ul> <li>\\(\\nu = 0.5\\): Exponential kernel (less smooth, rougher functions)</li> <li>\\(\\nu = 1.5\\): Once differentiable</li> <li>\\(\\nu = 2.5\\): Twice differentiable</li> <li>\\(\\nu \\to \\infty\\): Recovers the RBF kernel</li> </ul> </li> <li> <p>\\(K_\\nu\\) is a modified Bessel function.</p> </li> </ul> <p>Properties:</p> <ul> <li> <p>Allows control over function roughness.</p> </li> <li> <p>Lower \\(\\nu\\) allows modeling rougher, less smooth functions.</p> </li> <li> <p>Implemented in both scikit-optimize and BoTorch.</p> </li> </ul> <p>References: sklearn Matern kernel BoTorch Matern kernel</p>"},{"location":"background/kernels/#3-rational-quadratic-kernel","title":"3. Rational Quadratic Kernel","text":"<p>The Rational Quadratic kernel can be seen as a scale mixture of RBF kernels with different lengthscales.</p> \\[ k_{\\text{RQ}}(x, x') = \\sigma^2 \\left( 1 + \\frac{||x - x'||^2}{2\\alpha \\ell^2} \\right)^{-\\alpha} \\] <ul> <li> <p>\\(\\alpha\\) controls the relative weighting of large-scale and small-scale variations.</p> </li> <li> <p>As \\(\\alpha \\to \\infty\\), the kernel approaches the RBF kernel.</p> </li> </ul> <p>Properties:</p> <ul> <li> <p>Can model functions with varying smoothness.</p> </li> <li> <p>Useful when the function exhibits both short- and long-range correlations.</p> </li> <li> <p>Currently implemented in the scikit-optimize backend.</p> </li> </ul> <p>References: sklearn RationalQuadratic kernel</p>"},{"location":"background/kernels/#anisotropic-kernels-and-automatic-relevance-determination-ard","title":"Anisotropic Kernels and Automatic Relevance Determination (ARD)","text":""},{"location":"background/kernels/#isotropic-vs-anisotropic","title":"Isotropic vs. Anisotropic","text":"<ul> <li> <p>Isotropic kernel: Uses a single lengthscale \\(\\ell\\) for all input dimensions.  </p> <p>\\(k(x, x') = k(||x - x'||)\\)</p> </li> <li> <p>Anisotropic kernel: Uses a separate lengthscale \\(\\ell_d\\) for each input dimension \\(d\\).  </p> <p>\\(k(x, x') = \\exp\\left( -\\sum_{d=1}^D \\frac{(x_d - x'_d)^2}{2\\ell_d^2} \\right)\\)</p> </li> </ul>"},{"location":"background/kernels/#automatic-relevance-determination-ard","title":"Automatic Relevance Determination (ARD)","text":"<p>ARD refers to the process where the model learns a separate lengthscale for each input variable. If a variable is not relevant to the output, its lengthscale will become very large, effectively reducing its influence on the model.</p> <p>Benefits:</p> <ul> <li> <p>Helps identify which variables are important for predicting the output.</p> </li> <li> <p>Improves interpretability and can lead to more efficient optimization.</p> </li> </ul> <p>Both scikit-optimize and BoTorch support anisotropic kernels and ARD by default.</p>"},{"location":"background/kernels/#choosing-a-kernel","title":"Choosing a Kernel","text":"<ul> <li> <p>RBF: Good default for smooth, well-behaved functions.</p> </li> <li> <p>Matern: Use when you expect the function to be less smooth or want to control smoothness. Lower \\(\\nu\\) for rougher functions, higher \\(\\nu\\) for smoother.</p> </li> <li> <p>Rational Quadratic: Use when you suspect the function has varying smoothness or both short- and long-range correlations.</p> </li> </ul> <p>Tips:</p> <ul> <li> <p>If unsure, start with Matern (\\(\\nu=2.5\\) or \\(1.5\\)) or RBF.</p> </li> <li> <p>Try different kernels and compare cross-validation metrics (RMSE, MAE, etc.).</p> </li> <li> <p>Use ARD to let the model determine variable relevance.</p> </li> </ul>"},{"location":"background/kernels/#further-reading","title":"Further Reading","text":"<ul> <li>scikit-learn Gaussian Process kernels documentation</li> <li>scikit-optimize kernels</li> <li>BoTorch kernels</li> </ul>"},{"location":"modeling/botorch/","title":"BoTorch Backend","text":""},{"location":"modeling/botorch/#botorch-backend","title":"BoTorch Backend","text":"<p>The BoTorch backend in ALchemist allows you to train a Gaussian Process (GP) surrogate model using the BoTorch library, which is built on PyTorch and designed for scalable Bayesian optimization. BoTorch provides advanced kernel options and efficient handling of both continuous and categorical variables.</p>"},{"location":"modeling/botorch/#what-is-botorch","title":"What is BoTorch?","text":"<p>BoTorch is a flexible, research-oriented library for Bayesian optimization built on PyTorch. It was developed by Meta (Facebook) and serves as the underlying optimization engine for Ax, a high-level adaptive experimentation platform. BoTorch supports advanced features such as anisotropic kernels (automatic relevance determination, ARD) and mixed-variable spaces, and is tightly integrated with GPyTorch for scalable Gaussian process inference.</p>"},{"location":"modeling/botorch/#training-a-model-with-botorch-backend","title":"Training a Model with BoTorch Backend","text":"<p>When you select the botorch backend in the Model panel, you are training a GP model using BoTorch's <code>SingleTaskGP</code> (for continuous variables) or <code>MixedSingleTaskGP</code> (for mixed continuous/categorical variables). The workflow and options are as follows:</p>"},{"location":"modeling/botorch/#1-kernel-selection","title":"1. Kernel Selection","text":"<p>You can choose the kernel type for the continuous variables:</p> <ul> <li> <p>Matern: Default, with a tunable smoothness parameter (<code>nu</code>).</p> </li> <li> <p>RBF: Radial Basis Function kernel.</p> </li> </ul> <p>For the Matern kernel, you can select the <code>nu</code> parameter (0.5, 1.5, or 2.5), which controls the smoothness of the function.</p> <p>Note: BoTorch uses anisotropic (ARD) kernels by default, so each variable can have its own learned lengthscale. This helps preserve the physical meaning of each variable and enables automatic relevance detection. For more details, see the Kernel Deep Dive in the Educational Resources section.</p>"},{"location":"modeling/botorch/#2-categorical-variables","title":"2. Categorical Variables","text":"<ul> <li> <p>Categorical variables are automatically detected and encoded.</p> </li> <li> <p>BoTorch uses the <code>MixedSingleTaskGP</code> model to handle mixed spaces, encoding categorical variables as required.</p> </li> </ul>"},{"location":"modeling/botorch/#3-noise-handling","title":"3. Noise Handling","text":"<ul> <li> <p>If your experimental data includes a <code>Noise</code> column, these values are used for regularization.</p> </li> <li> <p>If not, the model uses its internal noise estimation.</p> </li> </ul>"},{"location":"modeling/botorch/#4-model-training-and-evaluation","title":"4. Model Training and Evaluation","text":"<ul> <li> <p>The model is trained on your current experiment data.</p> </li> <li> <p>Cross-validation is performed to estimate model performance (RMSE, MAE, MAPE, R\u00b2).</p> </li> <li> <p>Learned kernel hyperparameters (lengthscales, outputscale, etc.) are displayed after training.</p> </li> </ul>"},{"location":"modeling/botorch/#5-input-and-output-transforms","title":"5. Input and Output Transforms","text":"<p>Smart Defaults (v0.3.0+):</p> <p>BoTorch models in ALchemist now automatically apply input normalization and output standardization for improved performance:</p> <ul> <li> <p>Input Normalization: Scales inputs to [0, 1] range, improving numerical stability</p> </li> <li> <p>Output Standardization: Centers outputs to zero mean and unit variance, helping with optimization</p> </li> </ul> <p>These transforms are applied automatically unless explicitly overridden. This typically improves model R\u00b2 from ~0.0001 to 0.3-0.9 for typical problems.</p> <p>Manual Override:</p> <p>You can explicitly specify transform types when needed:</p> <ul> <li> <p><code>input_transform_type</code>: \"normalize\" (default), \"standardize\", or \"none\"</p> </li> <li> <p><code>output_transform_type</code>: \"standardize\" (default) or \"none\"</p> </li> </ul>"},{"location":"modeling/botorch/#6-advanced-options","title":"6. Advanced Options","text":"<ul> <li> <p>You can select the kernel type and Matern <code>nu</code> parameter in the Model panel.</p> </li> <li> <p>BoTorch uses sensible defaults for training iterations and transforms.</p> </li> <li> <p>ARD lengthscales are extracted and displayed after training for feature importance analysis.</p> </li> </ul>"},{"location":"modeling/botorch/#how-it-works","title":"How It Works","text":"<ul> <li> <p>The model uses your variable space and experiment data to fit a GP regression model using PyTorch.</p> </li> <li> <p>Input/output transforms are automatically applied for better performance.</p> </li> <li> <p>The trained model is used for Bayesian optimization, suggesting new experiments via acquisition functions.</p> </li> <li> <p>All preprocessing (encoding, transforms, noise handling) is handled automatically.</p> </li> </ul>"},{"location":"modeling/botorch/#references","title":"References","text":"<ul> <li>BoTorch documentation</li> <li>BoTorch SingleTaskGP</li> <li>BoTorch MixedSingleTaskGP</li> <li>Ax platform documentation</li> <li>GPyTorch documentation</li> </ul> <p>For a deeper explanation of kernel selection, anisotropic kernels, and ARD, see Kernel Deep Dive in the Educational Resources section.</p> <p>For details on using the scikit-optimize backend, see the previous section.</p>"},{"location":"modeling/performance/","title":"Model Performance","text":""},{"location":"modeling/performance/#model-performance","title":"Model Performance","text":"<p>Evaluating the performance of your surrogate model is a critical step in the active learning workflow. ALchemist provides several tools and visualizations to help you assess model quality and guide your next steps.</p>"},{"location":"modeling/performance/#cross-validation-and-error-metrics","title":"Cross-Validation and Error Metrics","text":"<p>After training a model, ALchemist automatically computes cross-validation metrics such as RMSE, MAE, MAPE, and R\u00b2. These metrics are visualized in the Visualizations dialog, where you can see how model error changes as more data points are added.</p> <p>General expectation: </p> <ul> <li>As you increase the number of observations, cross-validation error (e.g., RMSE) should generally decrease. This indicates that the model is learning from the data and improving its predictions.</li> </ul>"},{"location":"modeling/performance/#what-if-error-doesnt-decrease","title":"What if Error Doesn't Decrease?","text":"<p>If you notice that error metrics do not decrease with more data, consider the following:</p> <ol> <li> <p>Small Data Regime (&lt;10 points):    With very few data points, high error or flat trends are common. This is not necessarily a problem\u2014acquisition functions will naturally suggest new experiments in regions of high uncertainty, helping the model converge as more data is collected.</p> </li> <li> <p>Try a Different Backend:    Switch between the scikit-optimize and BoTorch backends. Sometimes one backend may fit your data better, especially depending on the variable types and dimensionality.</p> </li> <li> <p>Tweak the Kernel:    Experiment with different kernel types (RBF, Matern, RationalQuadratic) or adjust the Matern <code>nu</code> parameter. The choice of kernel can significantly affect model flexibility and fit.</p> </li> </ol>"},{"location":"modeling/performance/#additional-tips-and-considerations","title":"Additional Tips and Considerations","text":"<ul> <li> <p>Overfitting:   Overfitting may appear as jagged or unrealistic response surfaces in contour plots. If you see this, try increasing regularization (e.g., by specifying higher noise values) or collecting more data.</p> </li> <li> <p>Data Quality:   Poor model performance can result from poor data quality. Check for outliers or inconsistent measurements. Consider populating the <code>Noise</code> column with an appropriate metric (such as variance or signal-to-noise ratio) to help regularize the model. See the BoTorch Backend page for details on noise handling.</p> </li> <li> <p>Model Diagnostics:   Use parity plots and error metric trends to diagnose underfitting, overfitting, or data issues. Ideally, parity plots should show points close to the diagonal (y = x), indicating good agreement between predicted and actual values.</p> </li> <li> <p>Variable Importance:   Both backends use anisotropic (ARD) kernels by default, allowing the model to learn a separate lengthscale for each variable. This can help identify which variables are most relevant to the output.</p> </li> </ul>"},{"location":"modeling/performance/#summary","title":"Summary","text":"<ul> <li> <p>Expect error to decrease as more data is added.</p> </li> <li> <p>Use backend and kernel options to improve fit.</p> </li> <li> <p>Watch for signs of overfitting or poor data quality.</p> </li> <li> <p>Use regularization and noise estimates to stabilize the model.</p> </li> </ul> <p>For more on error metrics and visualization, see the Visualizations section.</p>"},{"location":"modeling/skopt/","title":"scikit-optimize Backend","text":""},{"location":"modeling/skopt/#scikit-learn-backend","title":"scikit-learn Backend","text":"<p>The sklearn backend in ALchemist allows you to train a Gaussian Process (GP) surrogate model using scikit-learn's <code>GaussianProcessRegressor</code>, wrapped by scikit-optimize for Bayesian optimization workflows.</p>"},{"location":"modeling/skopt/#what-is-the-sklearn-backend","title":"What is the sklearn backend?","text":"<p>ALchemist's sklearn backend uses scikit-optimize (skopt) which builds on scikit-learn's Gaussian Process implementation. This provides a lightweight, CPU-only option for Bayesian optimization that works well for moderate-sized datasets.</p>"},{"location":"modeling/skopt/#training-a-model-with-sklearn-backend","title":"Training a Model with sklearn Backend","text":"<p>When you select the sklearn backend in the Model panel (or specify <code>backend='sklearn'</code> in code), you are training a Gaussian Process model using the scikit-learn/scikit-optimize stack. The workflow and options are as follows:</p>"},{"location":"modeling/skopt/#1-kernel-selection","title":"1. Kernel Selection","text":"<p>You can choose from several kernel types for the GP:</p> <ul> <li> <p>RBF (Radial Basis Function): Default, smooth kernel.</p> </li> <li> <p>Matern: Flexible kernel with a tunable smoothness parameter (<code>nu</code>).</p> </li> <li> <p>RationalQuadratic: Mixture of RBF kernels with varying length scales.</p> </li> </ul> <p>For the Matern kernel, you can select the <code>nu</code> parameter (0.5, 1.5, 2.5, or \u221e), which controls the smoothness of the function.</p> <p>Note: ALchemist uses anisotropic (dimension-wise) kernels by default, so each variable can have its own learned lengthscale. This helps preserve the physical meaning of each variable and enables automatic relevance detection (ARD). For more details, see the Kernel Deep Dive in the Educational Resources section.</p>"},{"location":"modeling/skopt/#2-optimizer","title":"2. Optimizer","text":"<p>You can select the optimizer used for hyperparameter tuning:</p> <ul> <li> <p>L-BFGS-B (default)</p> </li> <li> <p>CG</p> </li> <li> <p>BFGS</p> </li> <li> <p>TNC</p> </li> </ul> <p>These control how the kernel hyperparameters are optimized during model fitting.</p>"},{"location":"modeling/skopt/#3-advanced-options","title":"3. Advanced Options","text":"<p>Enable advanced options to customize kernel and optimizer settings. By default, kernel hyperparameters are automatically optimized.</p>"},{"location":"modeling/skopt/#4-noise-handling","title":"4. Noise Handling","text":"<p>If your experimental data includes a <code>Noise</code> column, these values are used for regularization (<code>alpha</code> parameter in scikit-learn). If not, the model uses its default regularization.</p>"},{"location":"modeling/skopt/#5-one-hot-encoding","title":"5. One-Hot Encoding","text":"<p>Categorical variables are automatically one-hot encoded for compatibility with scikit-learn.</p>"},{"location":"modeling/skopt/#6-input-and-output-transforms","title":"6. Input and Output Transforms","text":"<p>ALchemist supports optional input and output scaling for scikit-learn models:</p> <ul> <li> <p>Input scaling: \"minmax\" (scale to [0,1]), \"standard\" (zero mean, unit variance), \"robust\" (median and IQR), or \"none\"</p> </li> <li> <p>Output scaling: \"minmax\", \"standard\", \"robust\", or \"none\"</p> </li> </ul> <p>While not applied by default like BoTorch, transforms can improve model performance for data with varying scales or outliers.</p>"},{"location":"modeling/skopt/#7-model-training-and-evaluation","title":"7. Model Training and Evaluation","text":"<ul> <li> <p>The model is trained on your current experiment data.</p> </li> <li> <p>Cross-validation is performed to estimate model performance (RMSE, MAE, MAPE, R\u00b2).</p> </li> <li> <p>Learned kernel hyperparameters are displayed after training.</p> </li> <li> <p>ARD lengthscales can be extracted for feature importance analysis.</p> </li> </ul>"},{"location":"modeling/skopt/#how-it-works","title":"How It Works","text":"<ul> <li> <p>The model uses your variable space and experiment data to fit a GP regression model.</p> </li> <li> <p>Optional transforms are applied based on configuration.</p> </li> <li> <p>The trained model is used for Bayesian optimization, suggesting new experiments via acquisition functions.</p> </li> <li> <p>All preprocessing (encoding, transforms, noise handling) is handled automatically.</p> </li> </ul>"},{"location":"modeling/skopt/#references","title":"References","text":"<ul> <li>scikit-learn GaussianProcessRegressor documentation</li> <li>scikit-optimize GaussianProcessRegressor documentation</li> </ul> <p>For a deeper explanation of kernel selection, anisotropic kernels, and ARD, see Kernel Deep Dive in the Educational Resources section.</p> <p>For details on using the BoTorch backend, see the next section.</p>"},{"location":"reproducibility/audit_logs/","title":"Audit Logs","text":""},{"location":"reproducibility/audit_logs/#audit-logs","title":"Audit Logs","text":"<p>Audit logs track your optimization decisions for reproducible research. The log records which data you used, which models you trained, and which experiments you decided to run\u2014creating a complete, verifiable history of your project.</p>"},{"location":"reproducibility/audit_logs/#using-audit-logs-in-the-application","title":"Using Audit Logs in the Application","text":""},{"location":"reproducibility/audit_logs/#the-workflow","title":"The Workflow","text":"<p>1. Train your model:</p> <ul> <li> <p>Add your experimental data</p> </li> <li> <p>Train a Gaussian Process model</p> </li> <li> <p>Check diagnostics (parity plot, Q-Q plot, etc.)</p> </li> </ul> <p>2. Run acquisition:</p> <ul> <li> <p>Choose your acquisition strategy (Expected Improvement, UCB, etc.)</p> </li> <li> <p>Generate candidate experiments</p> </li> <li> <p>Review the suggestions</p> </li> </ul> <p>3. Add to audit log:</p> <ul> <li> <p>Desktop App: When you're happy with a suggestion, click \"Log to Audit Trail\" in the notification window</p> </li> <li> <p>Web App: Click \"Stage to Audit Log\" in the Acquisition panel</p> </li> <li> <p>This marks it as a pending experiment you plan to run</p> </li> <li> <p>The log records: model used, acquisition settings, suggested point</p> </li> </ul> <p>4. Add the experiment:</p> <ul> <li> <p>Run the experiment in your lab</p> </li> <li> <p>Use \"Add Point\" dialog to enter the results</p> </li> <li> <p>Or import from CSV</p> </li> </ul> <p>5. Save your session:</p> <ul> <li> <p>The audit log is saved with your session</p> </li> <li> <p>Complete record of your optimization decisions</p> </li> </ul>"},{"location":"reproducibility/audit_logs/#exporting-the-audit-log","title":"Exporting the Audit Log","text":"<p>The audit log is stored in your session file. To review or share it:</p> <p>Desktop App:</p> <ul> <li> <p>File \u2192 Export Audit Log</p> </li> <li> <p>Saves as a Markdown file with a readable report</p> </li> </ul> <p>Web App:</p> <ul> <li> <p>Click the export icon in the top toolbar</p> </li> <li> <p>Downloads audit log as Markdown</p> </li> </ul> <p>What's included:</p> <ul> <li> <p>Timeline of all decisions</p> </li> <li> <p>Model configurations used</p> </li> <li> <p>Acquisition strategies and parameters</p> </li> <li> <p>Suggested experiments and results</p> </li> <li> <p>Notes you've added along the way</p> </li> </ul>"},{"location":"reproducibility/audit_logs/#why-use-audit-logs","title":"Why Use Audit Logs?","text":"<p>For reproducible research:</p> <ul> <li> <p>Prove you followed your methodology</p> </li> <li> <p>Show reviewers exactly what you did</p> </li> <li> <p>Document decisions weren't cherry-picked</p> </li> <li> <p>Support publications with verifiable records</p> </li> </ul> <p>For your records:</p> <ul> <li> <p>Remember what worked and what didn't</p> </li> <li> <p>Track multiple optimization attempts</p> </li> <li> <p>Share methodology with colleagues</p> </li> <li> <p>Keep lab notebook records</p> </li> </ul> <p>For collaboration:</p> <ul> <li> <p>Share decision rationale with team members</p> </li> <li> <p>Document protocol compliance</p> </li> <li> <p>Enable others to reproduce your work</p> </li> </ul>"},{"location":"reproducibility/audit_logs/#advanced-programmatic-access","title":"Advanced: Programmatic Access","text":"<p>For users writing Python scripts:</p> <pre><code>from alchemist_core import OptimizationSession\n\nsession = OptimizationSession()\n\n# Add data and train model\nsession.add_experiment({'temp': 60, 'pressure': 5}, output=75.3)\nsession.train_model(backend='botorch')\n\n# Stage acquisition to audit log\ncandidates = session.suggest_next(strategy='EI', n_candidates=5)\nsession.lock_acquisition(\n    strategy='EI',\n    candidates=candidates,\n    notes=\"Batch 3 - targeting optimal region\"\n)\n\n# Access audit log entries\nfor entry in session.audit_log.entries:\n    print(f\"{entry.timestamp}: {entry.entry_type}\")\n\n# Export audit log\nsession.audit_log.export('audit_log.json')\n</code></pre> <p>Audit logs are automatically saved with your session file.</p>"},{"location":"reproducibility/sessions/","title":"Sessions","text":""},{"location":"reproducibility/sessions/#session-management","title":"Session Management","text":"<p>Sessions are your optimization project containers. Each session stores your variables, experimental data, trained models, and complete history. Sessions can be saved, loaded, and shared across the desktop and web applications.</p>"},{"location":"reproducibility/sessions/#using-sessions-in-the-application","title":"Using Sessions in the Application","text":""},{"location":"reproducibility/sessions/#creating-a-new-session","title":"Creating a New Session","text":"<p>Desktop or Web App:</p> <ol> <li>Click \"New Session\" (or File \u2192 New Session on desktop)</li> <li>Enter a project name (e.g., \"Catalyst Screening Dec 2025\")</li> <li>Optionally add a description</li> <li>Click Create</li> </ol> <p>Your session is now ready. Start by defining your variables.</p>"},{"location":"reproducibility/sessions/#opening-an-existing-session","title":"Opening an Existing Session","text":"<p>Desktop App:</p> <ul> <li> <p>File \u2192 Open Session \u2192 Select your <code>.json</code> file</p> </li> <li> <p>Or File \u2192 Recent Sessions to quickly reopen</p> </li> </ul> <p>Web App:</p> <ul> <li> <p>Click \"Load Session\"</p> </li> <li> <p>Select from your available sessions</p> </li> <li> <p>Session loads with all your data intact</p> </li> </ul>"},{"location":"reproducibility/sessions/#saving-your-session","title":"Saving Your Session","text":"<p>Desktop App:</p> <ul> <li> <p>File \u2192 Save Session (or Cmd+S / Ctrl+S)</p> </li> <li> <p>Save after adding data, training models, or generating suggestions</p> </li> </ul> <p>Web App:</p> <ul> <li> <p>Sessions auto-save as you work</p> </li> <li> <p>No manual save needed</p> </li> </ul> <p>Where sessions are stored: <code>cache/sessions/</code> directory as <code>.json</code> files</p>"},{"location":"reproducibility/sessions/#sharing-sessions","title":"Sharing Sessions","text":"<p>Sessions are just JSON files you can share with colleagues:</p> <ol> <li>Find your session file in <code>cache/sessions/</code></li> <li>Share it via email, Dropbox, Git, etc.</li> <li>Colleague places it in their <code>cache/sessions/</code> folder</li> <li>They can open it in desktop or web app</li> </ol> <p>Sessions work across both apps - create in web, open in desktop, or vice versa.</p>"},{"location":"reproducibility/sessions/#advanced-programmatic-session-management","title":"Advanced: Programmatic Session Management","text":"<p>For users writing Python scripts or integrating ALchemist into analysis pipelines:</p>"},{"location":"reproducibility/sessions/#creating-sessions-programmatically","title":"Creating Sessions Programmatically","text":"<pre><code>from alchemist_core import OptimizationSession\n\n# Create new session\nsession = OptimizationSession()\nsession.metadata.name = \"Catalyst Screening\"\nsession.metadata.description = \"Pd catalyst optimization\"\n\n# Define search space\nsession.add_variable('temperature', 'real', bounds=(20, 100), unit='\u00b0C')\nsession.add_variable('catalyst_loading', 'real', bounds=(0.1, 5.0), unit='mol%')\nsession.add_variable('solvent', 'categorical', categories=['THF', 'DMF', 'toluene'])\n\n# Add experimental data\nsession.add_experiment({\n    'temperature': 60,\n    'catalyst_loading': 2.5,\n    'solvent': 'THF'\n}, output=85.3)\n\n# Save session\nsession.save_session('cache/sessions/')\n</code></pre>"},{"location":"reproducibility/sessions/#loading-and-using-sessions","title":"Loading and Using Sessions","text":"<pre><code># Load existing session\nsession = OptimizationSession.load_session('cache/sessions/abc123.json')\n\n# Access session state\nprint(f\"Session: {session.metadata.name}\")\nprint(f\"Variables: {len(session.search_space.variables)}\")\nprint(f\"Experiments: {len(session.experiment_manager.data)}\")\n\n# Continue optimization\nsession.train_model(backend='botorch')\nnext_point = session.suggest_next(strategy='EI', goal='maximize')\n\n# Save updates\nsession.save_session()\n</code></pre>"},{"location":"reproducibility/sessions/#session-file-structure","title":"Session File Structure","text":"<p>Sessions are stored as JSON files with this structure:</p> <pre><code>{\n  \"metadata\": {\n    \"session_id\": \"abc123-def456-...\",\n    \"name\": \"Catalyst Screening\",\n    \"created_at\": \"2025-12-12T10:00:00\",\n    \"last_modified\": \"2025-12-12T15:30:00\"\n  },\n  \"search_space\": {\n    \"variables\": [...]\n  },\n  \"experiments\": {\n    \"data\": [...]\n  },\n  \"model\": {\n    \"backend\": \"botorch\",\n    \"hyperparameters\": {...}\n  },\n  \"audit_log\": {\n    \"entries\": [...]\n  }\n}\n</code></pre> <p>The JSON format is human-readable and can be inspected with any text editor git commit -m \"Add catalyst screening session\" git push</p>"},{"location":"reproducibility/sessions/#collaborator-pulls","title":"Collaborator pulls","text":"<p>git pull</p>"},{"location":"reproducibility/sessions/#session-now-available-in-their-alchemist","title":"Session now available in their ALchemist","text":"<pre><code>### Best Practices\n\n**Version control**:\n\n- Use Git LFS for large session files (&gt; 1 MB)\n\n- Commit session files at milestones\n\n- Tag important versions\n\n**Naming conventions**:\n\n- Use descriptive names: \"Catalyst_Screen_2025-12\"\n\n- Avoid special characters\n\n- Include date or version in name\n\n**Documentation**:\n\n- Use description field extensively\n\n- Add tags for organization\n\n- Include contact info in author field\n\n---\n\n## Session Organization\n\n### Tagging System\n\n**Purpose**: Organize sessions by project, topic, date, or priority\n\n**Examples**:\n\n- Project tags: `\"project-alpha\"`, `\"project-beta\"`\n\n- Topic tags: `\"catalysis\"`, `\"materials\"`, `\"process-optimization\"`\n\n- Date tags: `\"2025\"`, `\"Q4-2025\"`\n\n- Status tags: `\"active\"`, `\"completed\"`, `\"on-hold\"`\n\n- Priority tags: `\"high-priority\"`, `\"exploratory\"`\n\n**Usage**:\n```python\nsession.metadata.tags = [\n    \"catalysis\",\n    \"suzuki\",\n    \"2025\",\n    \"high-priority\"\n]\n</code></pre>"},{"location":"reproducibility/sessions/#directory-organization","title":"Directory Organization","text":"<p>Recommended Structure: </p><pre><code>cache/sessions/\n\u251c\u2500\u2500 active/\n\u2502   \u251c\u2500\u2500 catalyst_screen_2025-12.json\n\u2502   \u2514\u2500\u2500 process_optimization_2025-11.json\n\u251c\u2500\u2500 completed/\n\u2502   \u251c\u2500\u2500 materials_discovery_2025-10.json\n\u2502   \u2514\u2500\u2500 kinetics_study_2025-09.json\n\u2514\u2500\u2500 archive/\n    \u251c\u2500\u2500 old_project_2024.json\n    \u2514\u2500\u2500 failed_experiment_2024.json\n</code></pre><p></p> <p>Implementation: </p><pre><code># Save to organized directory\nsession.save_session('cache/sessions/active/')\n</code></pre><p></p>"},{"location":"reproducibility/sessions/#session-state-management","title":"Session State Management","text":""},{"location":"reproducibility/sessions/#whats-saved","title":"What's Saved","text":"<p>Always Saved:</p> <ul> <li> <p>Variable space definition</p> </li> <li> <p>All experimental data</p> </li> <li> <p>Session metadata</p> </li> <li> <p>Audit log entries</p> </li> </ul> <p>Optionally Saved:</p> <ul> <li> <p>Trained model state (if trained)</p> </li> <li> <p>Model hyperparameters</p> </li> <li> <p>Training metrics (CV results)</p> </li> <li> <p>Acquisition suggestions (last batch)</p> </li> </ul>"},{"location":"reproducibility/sessions/#whats-not-saved","title":"What's Not Saved","text":"<p>Excluded from session files:</p> <ul> <li> <p>Visualization plots (regenerated on demand)</p> </li> <li> <p>Large intermediate arrays</p> </li> <li> <p>UI preferences (stored separately)</p> </li> <li> <p>Temporary scratch data</p> </li> </ul> <p>Why: Keep session files lightweight and portable</p>"},{"location":"reproducibility/sessions/#state-consistency","title":"State Consistency","text":"<p>Validation on load:</p> <ul> <li> <p>Variable space matches experimental data</p> </li> <li> <p>Data integrity checks</p> </li> <li> <p>Model compatibility verification</p> </li> <li> <p>Audit log hash validation</p> </li> </ul> <p>Error Handling: </p><pre><code>try:\n    session = OptimizationSession.load_session('path/to/session.json')\nexcept ValueError as e:\n    print(f\"Session validation failed: {e}\")\n    # Handle corrupted or incompatible session\n</code></pre><p></p>"},{"location":"setup/data_visualization/","title":"Visualizing Data","text":""},{"location":"setup/data_visualization/#data-visualization","title":"Data Visualization","text":"<p>The Visualization panel in ALchemist displays a 2D scatter plot of your experiment data and search space.</p> <ul> <li> <p>Variable Selection:   Use the dropdown menus above the plot to choose which variables to display on the x and y axes.</p> </li> <li> <p>Pool Points:   Light blue, semi-transparent dots represent the pool of potential experiment points generated from your variable space.</p> </li> <li> <p>Sampled Points:   Green dots show the experiments you have already run or loaded.</p> </li> <li> <p>Suggested Next Point:   A blue diamond marker will appear after you train a model and run an acquisition function, indicating the next recommended experiment.</p> </li> </ul> <p>Currently, only 2D scatter plots are supported. Future updates may add more visualization options, such as 3D plots or advanced data views.</p>"},{"location":"setup/initial_sampling/","title":"Generating Initial Experiments","text":""},{"location":"setup/initial_sampling/#generating-initial-experiments","title":"Generating Initial Experiments","text":"<p>When starting an active learning workflow, it's important to generate an initial set of experiments that cover your variable space efficiently. This is especially useful if you have no prior experimental data, or if your existing data was not collected with surrogate modeling in mind. Well-chosen initial points help ensure that your model converges efficiently and avoids bias from poor coverage.</p>"},{"location":"setup/initial_sampling/#why-generate-initial-points","title":"Why Generate Initial Points?","text":"<ul> <li> <p>No Prior Data: If you are starting from scratch, you need a set of initial experiments to train your first surrogate model.</p> </li> <li> <p>Supplement Existing Data: If you have some data, but it is sparse or not well-distributed, you can generate additional points to improve coverage.</p> </li> <li> <p>Efficient Model Convergence: Good initial coverage of the variable space helps the model learn faster and reduces the risk of missing important regions.</p> </li> </ul>"},{"location":"setup/initial_sampling/#how-to-generate-initial-points","title":"How to Generate Initial Points","text":"<ol> <li> <p>Load or Define Your Variable Space:    Make sure you have set up your variable space using the Variable Space Setup dialog.</p> </li> <li> <p>Open the Initial Sampling Dialog:    In the main application window, click Generate Initial Points in the Experiment Data panel.</p> </li> <li> <p>Choose a Sampling Strategy:    Select from several strategies:</p> </li> <li>Random: Uniformly samples points at random.</li> <li>LHS (Latin Hypercube Sampling): Ensures each variable is sampled evenly across its range.</li> <li> <p>Sobol, Halton, Hammersly: Quasi-random low-discrepancy sequences for more uniform coverage in high dimensions.</p> </li> <li> <p>Set the Number of Points:    Enter how many initial experiments you want to generate.</p> </li> <li> <p>Generate and Review:    Click Generate. The new points will appear in your experiment table, ready for export or further editing.</p> </li> </ol>"},{"location":"setup/initial_sampling/#tips","title":"Tips","text":"<ul> <li> <p>Coverage Matters: More points give better coverage, but also require more experiments. Balance your resources and modeling needs.</p> </li> <li> <p>Quasi-Random vs. Random: Quasi-random methods (LHS, Sobol, etc.) are generally preferred for initial sampling, especially in higher dimensions.</p> </li> <li> <p>Supplementing Data: You can generate initial points even if you already have some data, to fill gaps or improve distribution.</p> </li> </ul>"},{"location":"setup/initial_sampling/#example-workflow","title":"Example Workflow","text":"<ol> <li>Define your variable space (e.g., 3 variables: temperature, pressure, catalyst).</li> <li>Click Generate Initial Points.</li> <li>Choose LHS and set Number of Points to 10.</li> <li>Click Generate.</li> <li>Review the generated points in the experiment table and save to file if desired.</li> </ol> <p>For more details on experiment management and data loading, see the next section of the workflow documentation.</p>"},{"location":"setup/load_data/","title":"Loading Experimental Data","text":""},{"location":"setup/load_data/#loading-experimental-data","title":"Loading Experimental Data","text":"<p>The Experiment Data panel in ALchemist lets you load, view, and manage your experimental results. This is a key step before training surrogate models or running active learning.</p>"},{"location":"setup/load_data/#loading-data-from-file","title":"Loading Data from File","text":"<ol> <li>Click \"Load Experiments\":    In the Experiment Data panel, click the Load Experiments button.</li> <li>Select Your File:    Choose a <code>.csv</code> file containing your experimental data. The file should have columns for each variable (matching your variable space) and an <code>Output</code> column for the measured result. Optionally, you can include a <code>Noise</code> column to specify measurement uncertainty for each point.</li> <li>Data Appears in the Table:    The loaded data will be displayed in the table. If a <code>Noise</code> column is present, it will be used for model regularization.</li> </ol> <p>Tip: If your data columns do not match the variable names or required format, you may see an error. Make sure your CSV headers match your variable space exactly.</p>"},{"location":"setup/load_data/#adding-a-new-experiment-point","title":"Adding a New Experiment Point","text":"<p>You can add a new experiment directly from the UI:</p> <ol> <li>Click \"Add Point\":    Opens a dialog where you can enter values for each variable, the output, and (optionally) the noise.</li> <li>Fill in the Fields:    Enter values for all variables and the output. If you know the measurement uncertainty, enter it in the Noise field.</li> <li>Save &amp; Close:    Click Save &amp; Close to add the point to your experiment table. You can also choose to save the updated data to file and retrain the model immediately by checking the corresponding boxes.</li> </ol> <p>Note: </p> <ul> <li> <p>There may be issues with type compatibility (e.g., numbers being saved as strings). If you encounter problems, check your CSV file and ensure numeric columns are formatted correctly.</p> </li> <li> <p>Sometimes, changes made directly in the table (tksheet widget) may not update the internal experiment data until you save or reload. Use the provided dialogs for best results.</p> </li> </ul>"},{"location":"setup/load_data/#saving-your-data","title":"Saving Your Data","text":"<ul> <li> <p>Click \"Save Experiments\":   Saves the current experiment table to a <code>.csv</code> file.  </p> </li> <li> <p>Tip:   Always save your data before closing the application to avoid losing changes.</p> </li> </ul>"},{"location":"setup/load_data/#retraining-the-model","title":"Retraining the Model","text":"<ul> <li> <p>When adding a new point, you can check Retrain model to automatically update the surrogate model with the new data.</p> </li> <li> <p>If retraining does not seem to trigger, you may need to retrain manually from the model panel.</p> </li> </ul>"},{"location":"setup/load_data/#known-issues-tips","title":"Known Issues &amp; Tips","text":"<ul> <li> <p>Type Compatibility:   Data entered via the table or add-point dialog may sometimes be interpreted as strings. If you see errors or unexpected behavior, check your data types in the CSV file.</p> </li> <li> <p>Table Edits:   Editing data directly in the table does not always update the internal experiment manager. For reliable results, use the add-point dialog or reload your data after editing.</p> </li> <li> <p>Noise Column:   The noise column is optional. If present, it should be numeric. You can toggle its visibility in the Preferences menu.</p> </li> </ul> <p>For more details on managing experiments and troubleshooting, see the rest of the workflow documentation.</p>"},{"location":"setup/variable_space/","title":"Setting Up Variable Space","text":""},{"location":"setup/variable_space/#setting-up-the-variable-space","title":"Setting Up the Variable Space","text":"<p>The Variable Space Setup window allows you to define the variables that make up your search space for active learning experiments. This user guide explains each part of the dialog and how to use it.</p>"},{"location":"setup/variable_space/#opening-the-variable-space-setup","title":"Opening the Variable Space Setup","text":"<ul> <li>Click the Create Variables File button in the main application window to open the setup dialog.</li> </ul>"},{"location":"setup/variable_space/#window-overview","title":"Window Overview","text":"<p>The window is divided into two main sections:</p> <ul> <li> <p>Left Panel: Displays a list of variables you have defined.</p> </li> <li> <p>Right Panel: Contains control buttons for managing variables.</p> </li> </ul>"},{"location":"setup/variable_space/#adding-and-editing-variables","title":"Adding and Editing Variables","text":"<p>Each variable is represented as a row with the following fields:</p> <ul> <li> <p>Variable Name: Enter a unique name for your variable.</p> </li> <li> <p>Type: Choose the variable type from the dropdown:</p> </li> <li><code>Real</code>: For continuous variables (e.g., floating-point numbers).</li> <li><code>Integer</code>: For discrete variables (e.g., whole numbers).</li> <li><code>Categorical</code>: For variables with a set of named categories.</li> </ul> <p>Depending on the type selected:</p> <ul> <li> <p>Real/Integer: Enter minimum and maximum values in the <code>Min</code> and <code>Max</code> fields.</p> </li> <li> <p>Categorical: Click Edit Values to open a dialog where you can enter possible category values (one per row). Click Save in the dialog to confirm.</p> </li> </ul>"},{"location":"setup/variable_space/#managing-variable-rows","title":"Managing Variable Rows","text":"<p>Use the buttons on the right panel to manage your variable list:</p> <ul> <li> <p>Add Variable: Add a new variable row.</p> </li> <li> <p>Delete Row: Remove the currently selected variable.</p> </li> <li> <p>Clear Row: Clear all fields in the selected row.</p> </li> <li> <p>Move Up/Down: Change the order of variables by moving the selected row up or down.</p> </li> </ul> <p>Click on a row to select it; the selected row is highlighted.</p>"},{"location":"setup/variable_space/#saving-and-loading","title":"Saving and Loading","text":"<ul> <li> <p>Save to File: Save your variable definitions to a <code>.json</code> or <code>.csv</code> file.</p> </li> <li> <p>Load from File: Load variable definitions from a <code>.json</code> or <code>.csv</code> file.</p> </li> <li> <p>Save &amp; Close: Save your current variable space and close the window. This also updates the application's internal search space.</p> </li> </ul>"},{"location":"setup/variable_space/#tips","title":"Tips","text":"<ul> <li> <p>All fields must be filled out for a variable to be valid.</p> </li> <li> <p>For categorical variables, you must specify at least one value.</p> </li> <li> <p>You can reorder variables to control their order in the search space.</p> </li> </ul>"},{"location":"setup/variable_space/#example","title":"Example","text":"Variable Name Type Min Max Values temperature Real 20 100 batch_size Integer 1 10 catalyst Categorical A, B, C, D <p>For more details on how the variable space is used, see the rest of the workflow documentation.</p>"},{"location":"setup/web_app/","title":"Web Application","text":""},{"location":"setup/web_app/#getting-started","title":"Getting Started","text":"<p>ALchemist provides both a web application (browser-based) and desktop application (native GUI). Both interfaces offer the same functionality with nearly identical layouts and workflows. Choose whichever fits your needs - sessions are fully interoperable between both.</p>"},{"location":"setup/web_app/#launching-alchemist","title":"Launching ALchemist","text":""},{"location":"setup/web_app/#web-application","title":"Web Application","text":"<pre><code>alchemist-web\n</code></pre> <p>Open your browser to <code>http://localhost:8000</code></p>"},{"location":"setup/web_app/#desktop-application","title":"Desktop Application","text":"<pre><code>alchemist\n</code></pre> <p>The desktop GUI launches directly.</p>"},{"location":"setup/web_app/#basic-workflow","title":"Basic Workflow","text":"<p>All Bayesian optimization projects in ALchemist follow the same basic steps:</p> <ol> <li>Create or load a session - Your optimization project container</li> <li>Define variables - Set up your search space (see Variable Space)</li> <li>Add initial data - Import experiments or generate space-filling designs</li> <li>Train a model - Fit Gaussian Process to your data (see Modeling)</li> <li>View diagnostics - Check model quality (see Visualizations)</li> <li>Get suggestions - Run acquisition functions for next experiments (see Acquisition)</li> <li>Repeat - Add new data and iterate steps 4-6</li> </ol>"},{"location":"setup/web_app/#key-features","title":"Key Features","text":"<p>Session Management: Create, save, and load optimization projects. Sessions include your variable definitions, experimental data, trained models, and full audit logs.</p> <p>Flexible Variable Types: Define continuous (bounded ranges), discrete (specific values), or categorical (named options) variables.</p> <p>Data Import/Export: Add experiments manually or import from CSV. Export suggestions for lab execution.</p> <p>Model Training: Choose between BoTorch (advanced, GPU-ready) or scikit-learn (lightweight) backends with automatic hyperparameter optimization.</p> <p>Visualizations: Parity plots, Q-Q plots, calibration curves, and metrics evolution help you understand model performance.</p> <p>Acquisition Strategies: Expected Improvement, Upper Confidence Bound, Probability of Improvement, and Thompson Sampling.</p>"},{"location":"setup/web_app/#whats-different-between-web-and-desktop","title":"What's Different Between Web and Desktop?","text":"<p>Very little - they share the same core functionality:</p> Feature Web App Desktop App User Interface Browser-based Native GUI Functionality Identical Identical Sessions Fully compatible Fully compatible Offline Use Requires backend server Works fully offline <p>Choose the web app for remote access or team collaboration. Choose the desktop app for offline work or if you prefer native applications.</p>"},{"location":"setup/web_app/#where-to-go-next","title":"Where to Go Next","text":"<p>This guide covers launching ALchemist and the basic workflow. For detailed information on each step:</p> <ul> <li> <p>Variable Space Setup - Define your optimization problem</p> </li> <li> <p>Model Training - Configure and train Gaussian Processes</p> </li> <li> <p>Visualizations - Interpret model diagnostics</p> </li> <li> <p>Acquisition Functions - Choose how to explore your space</p> </li> <li> <p>Session Management - Save, load, and share your work</p> </li> <li> <p>REST API - Programmatic access for automation</p> </li> </ul>"},{"location":"visualizations/calibration_curve/","title":"Calibration curve","text":""},{"location":"visualizations/calibration_curve/#calibration-curve","title":"Calibration Curve","text":"<p>The calibration curve in ALchemist measures whether your Gaussian Process model's predicted confidence intervals have the correct coverage. It answers the question: \"When the model says a measurement is 90% likely to fall within an interval, does it actually fall within that interval 90% of the time?\"</p>"},{"location":"visualizations/calibration_curve/#what-the-calibration-curve-shows","title":"What the Calibration Curve Shows","text":"<p>X-axis: Expected confidence level (0 to 1, or 0% to 100%) Y-axis: Observed coverage from cross-validation predictions</p> <p>Key elements:</p> <ul> <li> <p>Blue curve: Actual coverage at each confidence level</p> </li> <li> <p>Diagonal line: Perfect calibration reference (y = x)</p> </li> <li> <p>Shaded regions: 95% and 68% confidence bands (Clopper-Pearson)</p> </li> <li> <p>Diagnostic text: Summary statistics and calibration status</p> </li> </ul>"},{"location":"visualizations/calibration_curve/#quick-interpretation-guide","title":"Quick Interpretation Guide","text":"Pattern Shape Status What It Means On diagonal y \u2248 x \u2713 Well-calibrated Coverage matches expectations Below diagonal y &lt; x Over-confident Intervals too narrow, poor coverage Above diagonal y &gt; x Under-confident Intervals too wide, conservative Matches band Within shaded area \u2713 Acceptable Within statistical uncertainty"},{"location":"visualizations/calibration_curve/#understanding-coverage-calibration","title":"Understanding Coverage Calibration","text":"<p>For each confidence level \u03b1 (e.g., 0.90 for 90%), the coverage is:</p> \\[ \\text{Coverage}(\\alpha) = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{1}\\left[y_i \\in [\\hat{\\mu}_i - z_{\\alpha}\\hat{\\sigma}_i,\\ \\hat{\\mu}_i + z_{\\alpha}\\hat{\\sigma}_i]\\right] \\] <p>Where:</p> <ul> <li> <p>\\(y_i\\) = true experimental value</p> </li> <li> <p>\\(\\hat{\\mu}_i\\) = predicted mean from cross-validation</p> </li> <li> <p>\\(\\hat{\\sigma}_i\\) = predicted standard deviation</p> </li> <li> <p>\\(z_{\\alpha}\\) = z-score for confidence level \u03b1 (e.g., 1.96 for 95%)</p> </li> <li> <p>\\(\\mathbb{1}[\\cdot]\\) = indicator function (1 if true, 0 if false)</p> </li> </ul> <p>Perfect calibration: Coverage(\u03b1) = \u03b1 for all \u03b1 \u2208 [0, 1]</p>"},{"location":"visualizations/calibration_curve/#when-to-use-the-calibration-curve","title":"When to Use the Calibration Curve","text":""},{"location":"visualizations/calibration_curve/#essential-situations","title":"Essential Situations","text":"<p>Before making optimization decisions:</p> <ul> <li> <p>Verify confidence intervals are trustworthy</p> </li> <li> <p>Assess risk tolerance (safety-critical applications)</p> </li> <li> <p>Validate uncertainty-based acquisition functions</p> </li> </ul> <p>After model training:</p> <ul> <li> <p>Check calibration across all confidence levels</p> </li> <li> <p>Compare different modeling backends</p> </li> <li> <p>Evaluate kernel choices</p> </li> </ul> <p>During active learning:</p> <ul> <li> <p>Monitor if calibration degrades as data grows</p> </li> <li> <p>Ensure reliability of new predictions</p> </li> <li> <p>Detect if recalibration is needed</p> </li> </ul>"},{"location":"visualizations/calibration_curve/#combined-with-other-diagnostics","title":"Combined with Other Diagnostics","text":"<p>Use calibration curve alongside:</p> <ul> <li> <p>Q-Q plot: Check z-score distribution (Std(z) \u2248 1)</p> </li> <li> <p>Parity plot: Assess prediction accuracy</p> </li> <li> <p>Metrics plot: Track overall performance</p> </li> </ul>"},{"location":"visualizations/calibration_curve/#accessing-the-calibration-curve","title":"Accessing the Calibration Curve","text":""},{"location":"visualizations/calibration_curve/#in-web-application","title":"In Web Application","text":"<ol> <li>Train a model in the GPR Panel</li> <li>Click \"Show Model Visualizations\"</li> <li>Select \"Calibration Curve\" from plot type buttons</li> <li>Toggle between calibrated/uncalibrated results</li> </ol>"},{"location":"visualizations/calibration_curve/#in-desktop-application","title":"In Desktop Application","text":"<ol> <li>Train model in Model panel</li> <li>Open Visualizations dialog</li> <li>Calibration curve available in visualization options</li> <li>Customize and export for reports</li> </ol>"},{"location":"visualizations/calibration_curve/#interpreting-calibration-patterns","title":"Interpreting Calibration Patterns","text":""},{"location":"visualizations/calibration_curve/#perfect-calibration","title":"Perfect Calibration","text":"<pre><code>Curve follows diagonal within confidence bands\nAll predicted confidence levels match observed coverage\n</code></pre> Example: 95% intervals contain true value 94-96% of the time Action: Model is ready for optimization, no changes needed"},{"location":"visualizations/calibration_curve/#over-confident-model","title":"Over-Confident Model","text":"<pre><code>Curve below diagonal across multiple confidence levels\nObserved coverage &lt; expected confidence level\n</code></pre> Example: 90% intervals only contain true value 75% of the time Impact: <ul> <li> <p>Higher risk of missing optimal regions</p> </li> <li> <p>Acquisition functions overly exploitative</p> </li> <li> <p>May converge prematurely</p> </li> </ul> <p>Actions:</p> <ol> <li>Use automatic calibration (enabled by default)</li> <li>Increase noise parameter if applicable</li> <li>Try more flexible kernel (Matern \u03bd=1.5)</li> <li>Collect more diverse training data</li> </ol>"},{"location":"visualizations/calibration_curve/#under-confident-model","title":"Under-Confident Model","text":"<pre><code>Curve above diagonal across multiple confidence levels\nObserved coverage &gt; expected confidence level\n</code></pre> Example: 90% intervals contain true value 98% of the time Impact: <ul> <li> <p>Wasted experimental budget (over-exploration)</p> </li> <li> <p>Slower convergence to optimum</p> </li> <li> <p>Conservative but safer</p> </li> </ul> <p>Actions:</p> <ol> <li>Often acceptable (conservative is safer than aggressive)</li> <li>If inefficient, reduce explicit noise values</li> <li>Try less flexible kernel (Matern \u03bd=2.5, RBF)</li> <li>Check that lengthscales aren't manually fixed too large</li> </ol>"},{"location":"visualizations/calibration_curve/#confidence-bands-statistical-uncertainty","title":"Confidence Bands (Statistical Uncertainty)","text":"<p>The shaded regions show expected variability due to finite sample size.</p>"},{"location":"visualizations/calibration_curve/#clopper-pearson-intervals","title":"Clopper-Pearson Intervals","text":"<p>For each confidence level \u03b1 with n samples and k successes:</p> \\[ \\text{Lower bound} = \\text{Beta}^{-1}\\left(\\frac{\\alpha_{\\text{band}}}{2}; k, n-k+1\\right) \\] \\[ \\text{Upper bound} = \\text{Beta}^{-1}\\left(1 - \\frac{\\alpha_{\\text{band}}}{2}; k+1, n-k\\right) \\] <p>Shaded regions:</p> <ul> <li> <p>Dark band: 68% confidence (\u22481\u03c3)</p> </li> <li> <p>Light band: 95% confidence (\u22482\u03c3)</p> </li> </ul> <p>Interpretation:</p> <ul> <li> <p>If curve is within bands: Deviations likely due to chance</p> </li> <li> <p>If curve is outside bands: Genuine calibration issue</p> </li> </ul>"},{"location":"visualizations/calibration_curve/#sample-size-considerations","title":"Sample Size Considerations","text":""},{"location":"visualizations/calibration_curve/#small-datasets-n-30","title":"Small Datasets (N &lt; 30)","text":"<ul> <li> <p>Very wide confidence bands</p> </li> <li> <p>High variability expected</p> </li> <li> <p>Difficult to distinguish poor calibration from sampling noise</p> </li> <li> <p>Focus on overall trend, don't over-interpret</p> </li> </ul>"},{"location":"visualizations/calibration_curve/#medium-datasets-30-n-100","title":"Medium Datasets (30 &lt; N &lt; 100)","text":"<ul> <li> <p>Moderate confidence bands</p> </li> <li> <p>Systematic deviations become detectable</p> </li> <li> <p>Curves outside 95% band indicate real issues</p> </li> <li> <p>Sufficient for calibration assessment</p> </li> </ul>"},{"location":"visualizations/calibration_curve/#large-datasets-n-100","title":"Large Datasets (N &gt; 100)","text":"<ul> <li> <p>Narrow confidence bands</p> </li> <li> <p>High confidence in calibration assessment</p> </li> <li> <p>Even small deviations from diagonal are meaningful</p> </li> <li> <p>Clear detection of calibration problems</p> </li> </ul>"},{"location":"visualizations/calibration_curve/#automatic-calibration-in-alchemist","title":"Automatic Calibration in ALchemist","text":"<p>When miscalibration is detected, ALchemist automatically applies correction:</p>"},{"location":"visualizations/calibration_curve/#calibration-method","title":"Calibration Method","text":"<ol> <li>Compute standardized residuals from cross-validation: \\(z_i = \\frac{y_i - \\hat{\\mu}_i}{\\hat{\\sigma}_i}\\)</li> <li>Calculate empirical standard deviation: \\(\\text{Std}(z)\\)</li> <li>Apply scaling to future predictions: \\(\\sigma_{\\text{calibrated}} = \\sigma_{\\text{raw}} \\times \\text{Std}(z)\\)</li> </ol>"},{"location":"visualizations/calibration_curve/#effect-on-calibration-curve","title":"Effect on Calibration Curve","text":"<p>Over-confident (Std(z) &gt; 1):</p> <ul> <li> <p>Raw curve below diagonal</p> </li> <li> <p>Calibrated curve shifts upward toward diagonal</p> </li> <li> <p>Intervals widened by factor Std(z)</p> </li> </ul> <p>Under-confident (Std(z) &lt; 1):</p> <ul> <li> <p>Raw curve above diagonal</p> </li> <li> <p>Calibrated curve shifts downward toward diagonal</p> </li> <li> <p>Intervals narrowed by factor Std(z)</p> </li> </ul>"},{"location":"visualizations/calibration_curve/#verification","title":"Verification","text":"<p>Toggle between calibrated/uncalibrated views to see:</p> <ul> <li> <p>Raw model performance</p> </li> <li> <p>Impact of automatic correction</p> </li> <li> <p>Improvement in coverage</p> </li> </ul>"},{"location":"visualizations/calibration_curve/#relationship-to-q-q-plot","title":"Relationship to Q-Q Plot","text":"<p>Calibration curve and Q-Q plot are complementary:</p> Diagnostic What It Checks Key Metric Q-Q Plot Distribution of z-scores Std(z) \u2248 1.0 Calibration Curve Coverage at confidence levels Coverage(\u03b1) \u2248 \u03b1 <p>Connection:</p> <ul> <li> <p>If Std(z) = 1.0 \u2192 Calibration curve should be near diagonal</p> </li> <li> <p>If Std(z) &gt; 1.0 \u2192 Calibration curve below diagonal (over-confident)</p> </li> <li> <p>If Std(z) &lt; 1.0 \u2192 Calibration curve above diagonal (under-confident)</p> </li> </ul> <p>Why use both?</p> <ul> <li> <p>Q-Q plot: Global assessment, single metrics</p> </li> <li> <p>Calibration curve: Level-specific assessment, shows where issues occur</p> </li> </ul>"},{"location":"visualizations/calibration_curve/#integration-with-bayesian-optimization","title":"Integration with Bayesian Optimization","text":"<p>Calibration directly impacts optimization efficiency:</p>"},{"location":"visualizations/calibration_curve/#expected-improvement-ei","title":"Expected Improvement (EI)","text":"<ul> <li> <p>Relies on correct \u03c3 for exploration/exploitation balance</p> </li> <li> <p>Poor calibration \u2192 suboptimal decisions</p> </li> </ul>"},{"location":"visualizations/calibration_curve/#upper-confidence-bound-ucb","title":"Upper Confidence Bound (UCB)","text":"<ul> <li> <p>Formula: \\(\\text{UCB} = \\mu + \\kappa \\sigma\\)</p> </li> <li> <p>Miscalibrated \u03c3 \u2192 wrong balance between mean and uncertainty</p> </li> </ul>"},{"location":"visualizations/calibration_curve/#safety-constrained-optimization","title":"Safety-Constrained Optimization","text":"<ul> <li> <p>Often requires 95% or 99% confidence intervals</p> </li> <li> <p>Poor calibration at high confidence levels \u2192 safety violations or excessive conservatism</p> </li> </ul> <p>Bottom line: Well-calibrated intervals are critical for successful and safe optimization.</p>"},{"location":"visualizations/calibration_curve/#interpreting-specific-confidence-levels","title":"Interpreting Specific Confidence Levels","text":""},{"location":"visualizations/calibration_curve/#low-confidence-50-70","title":"Low Confidence (50%-70%)","text":"<p>Region: Central part of curve Importance: Typical working range for acquisition functions Good calibration here: Essential for efficient exploration</p>"},{"location":"visualizations/calibration_curve/#medium-confidence-80-90","title":"Medium Confidence (80%-90%)","text":"<p>Region: Upper-middle of curve Importance: Safety margins for constraints Deviations: Impact risk assessment in constrained optimization</p>"},{"location":"visualizations/calibration_curve/#high-confidence-95-99","title":"High Confidence (95%-99%)","text":"<p>Region: Far right of curve Importance: Critical for safety-critical applications Statistical note: Fewer samples at extremes, wider confidence bands</p>"},{"location":"visualizations/calibration_curve/#common-calibration-issues","title":"Common Calibration Issues","text":""},{"location":"visualizations/calibration_curve/#issue-curve-below-diagonal-at-all-levels","title":"Issue: Curve Below Diagonal at All Levels","text":"<p>Diagnosis: Systematically over-confident Root causes:</p> <ul> <li> <p>Insufficient training data diversity</p> </li> <li> <p>Overfitting to training data</p> </li> <li> <p>Noise parameter too small</p> </li> <li> <p>Overly complex kernel</p> </li> </ul> <p>Solutions:</p> <ol> <li>Use automatic calibration</li> <li>Collect more varied training data</li> <li>Increase noise constraints</li> <li>Simplify kernel or regularize hyperparameters</li> </ol>"},{"location":"visualizations/calibration_curve/#issue-curve-above-diagonal-at-all-levels","title":"Issue: Curve Above Diagonal at All Levels","text":"<p>Diagnosis: Systematically under-confident Root causes:</p> <ul> <li> <p>Noise parameter too large</p> </li> <li> <p>Overly conservative kernel</p> </li> <li> <p>Lengthscales fixed too large</p> </li> </ul> <p>Solutions:</p> <ol> <li>Assess if this is acceptable (conservative is safer)</li> <li>Reduce explicit noise if set manually</li> <li>Allow lengthscale optimization</li> <li>Try less flexible kernel</li> </ol>"},{"location":"visualizations/calibration_curve/#issue-good-at-center-poor-at-extremes","title":"Issue: Good at Center, Poor at Extremes","text":"<p>Diagnosis: Non-uniform calibration Root causes:</p> <ul> <li> <p>Sample size effects (fewer points at extremes)</p> </li> <li> <p>Non-Gaussian error distribution</p> </li> <li> <p>Heteroscedastic noise</p> </li> </ul> <p>Solutions:</p> <ol> <li>Check if deviations are within confidence bands (may be statistical noise)</li> <li>Try output transforms (log, Box-Cox)</li> <li>Consider heteroscedastic GP if available</li> <li>Collect more data to reduce uncertainty</li> </ol>"},{"location":"visualizations/calibration_curve/#issue-sudden-jumps-or-non-monotonic-curve","title":"Issue: Sudden Jumps or Non-Monotonic Curve","text":"<p>Diagnosis: Small sample size or data artifacts Root causes:</p> <ul> <li> <p>Insufficient cross-validation samples</p> </li> <li> <p>Outliers or data quality issues</p> </li> <li> <p>Too few unique predictions</p> </li> </ul> <p>Solutions:</p> <ol> <li>Increase dataset size</li> <li>Check for and address outliers</li> <li>Verify data quality and preprocessing</li> <li>Use smoothing or binning for visualization only</li> </ol>"},{"location":"visualizations/calibration_curve/#practical-guidelines","title":"Practical Guidelines","text":""},{"location":"visualizations/calibration_curve/#acceptable-calibration","title":"Acceptable Calibration","text":"<p>Strict (safety-critical):</p> <ul> <li> <p>Curve within 68% confidence band at all levels</p> </li> <li> <p>Maximum deviation &lt; 5% from diagonal</p> </li> </ul> <p>Moderate (standard optimization):</p> <ul> <li> <p>Curve within 95% confidence band at most levels</p> </li> <li> <p>Maximum deviation &lt; 10% from diagonal</p> </li> </ul> <p>Relaxed (exploratory):</p> <ul> <li> <p>Overall trend near diagonal</p> </li> <li> <p>No systematic bias &gt; 15%</p> </li> </ul>"},{"location":"visualizations/calibration_curve/#when-to-recalibrate","title":"When to Recalibrate","text":"<p>During active learning:</p> <ul> <li> <p>After adding 20-30% more data</p> </li> <li> <p>If acquisition functions seem unreliable</p> </li> <li> <p>When optimization stagnates unexpectedly</p> </li> </ul> <p>After model changes:</p> <ul> <li> <p>Switching kernels or backends</p> </li> <li> <p>Changing hyperparameter constraints</p> </li> <li> <p>Applying new preprocessing</p> </li> </ul>"},{"location":"visualizations/calibration_curve/#advanced-topics","title":"Advanced Topics","text":""},{"location":"visualizations/calibration_curve/#coverage-vs-sharpness-trade-off","title":"Coverage vs. Sharpness Trade-off","text":"<p>Coverage: Frequency of intervals containing true value Sharpness: Width of confidence intervals</p> <p>Ideal: High coverage with narrow intervals Trade-off: Can always increase coverage by widening intervals, but this reduces utility</p> <p>ALchemist approach:</p> <ol> <li>Optimize model for best predictions (sharpness)</li> <li>Apply calibration to ensure correct coverage</li> <li>Balance achieved automatically</li> </ol>"},{"location":"visualizations/calibration_curve/#bayesian-confidence-intervals","title":"Bayesian Confidence Intervals","text":"<p>GP predictions naturally provide Bayesian credible intervals:</p> \\[ y_{\\text{new}} \\sim \\mathcal{N}(\\mu_*, \\sigma_*^2) \\] <p>Interpretation:</p> <ul> <li> <p>95% credible interval: \\([\\mu_* - 1.96\\sigma_*, \\mu_* + 1.96\\sigma_*]\\)</p> </li> <li> <p>Probability that true value is in interval (given model assumptions)</p> </li> </ul> <p>Calibration check: Do these intervals have frequentist coverage?</p>"},{"location":"visualizations/calibration_curve/#troubleshooting","title":"Troubleshooting","text":"<p>If calibration is poor, ALchemist's automatic calibration (enabled by default) will adjust confidence intervals. For persistent issues, try different kernels (Matern \u03bd=1.5, \u03bd=2.5, RBF) or collect more diverse data. Check the Q-Q plot and parity plot for additional diagnostics.</p>"},{"location":"visualizations/calibration_curve/#further-reading","title":"Further Reading","text":"<ul> <li>Interpreting Calibration Curves (Educational Guide) - Comprehensive theory and examples</li> <li>Q-Q Plot - Complementary z-score distribution diagnostic</li> <li>Parity Plot - Prediction accuracy assessment</li> <li>Model Performance - Overall model quality guide</li> </ul> <p>Key Takeaway: The calibration curve tells you whether you can trust your model's confidence intervals. Well-calibrated uncertainties enable confident decision-making and efficient Bayesian optimization.</p>"},{"location":"visualizations/contour_plot/","title":"Contour plot","text":""},{"location":"visualizations/contour_plot/#contour-plot-visualization","title":"Contour Plot Visualization","text":"<p>The Contour Plot feature in ALchemist lets you visualize your surrogate model\u2019s predicted output as a 2D contour plot, providing insight into the model\u2019s response surface across your variable space. This tool is designed for interpreting model predictions, identifying trends, and creating publication-quality figures.</p>"},{"location":"visualizations/contour_plot/#how-to-create-a-contour-plot","title":"How to Create a Contour Plot","text":"<ol> <li> <p>Open the Visualizations Dialog:    After training a model, open the Visualizations dialog from the main application window.</p> </li> <li> <p>Choose X and Y Axes:    Use the dropdown menus in the \"Contour Plot Options\" panel to select which two real-valued variables to display on the X and Y axes.</p> </li> <li> <p>Set Fixed Values for Other Variables:    Set a value for each remaining variable using the provided controls. This lets you view a \u201cslice\u201d of the model\u2019s prediction at a specific cross-section.</p> </li> <li> <p>Plot the Contour:    Click Plot Contour to generate the plot for your selected axes and fixed values. The predicted output will be shown as filled contours.</p> </li> </ol>"},{"location":"visualizations/contour_plot/#customization-and-export","title":"Customization and Export","text":"<ul> <li> <p>Customize Appearance:   Click Customize Plot to adjust the plot title, axis labels, colormap, font, font size, axis limits, font weight, tick style, and whether to show experimental data points (white circles) and the next suggested point (red diamond).</p> </li> <li> <p>Save Figures:   Use the Matplotlib toolbar below the plot to save your figure as a high-resolution image (PNG, SVG, PDF, etc.) for publication or presentations.</p> </li> </ul>"},{"location":"visualizations/contour_plot/#additional-features","title":"Additional Features","text":"<ul> <li> <p>Interactive Controls:   Changing the X or Y axis or adjusting fixed values for other variables will update the plot in real time.</p> </li> <li> <p>Legend:   The plot includes a legend for experimental points and the next suggested point if displayed.</p> </li> <li> <p>Hyperparameters Display:   The bottom of the Visualizations dialog shows the learned kernel hyperparameters from the trained model.</p> </li> </ul>"},{"location":"visualizations/contour_plot/#web-application","title":"Web Application","text":"<p>The contour plot is also available in the ALchemist web application:</p> <ol> <li>Navigate to GPR Panel</li> <li>Train a model</li> <li>Click Show Model Visualizations</li> <li>Select Contour Plot from plot type buttons</li> <li>Use dropdown menus to select X and Y axis variables</li> <li>Adjust sliders for fixed values (if &gt; 2 variables)</li> <li>Plot updates dynamically</li> </ol> <p>Web UI features:</p> <ul> <li> <p>Interactive variable selection</p> </li> <li> <p>Real-time updates</p> </li> <li> <p>Responsive design</p> </li> <li> <p>Hover tooltips showing values</p> </li> <li> <p>Download button for export</p> </li> </ul>"},{"location":"visualizations/contour_plot/#tips","title":"Tips","text":"<ul> <li> <p>Use customization options to match your figure style to journal or presentation requirements.</p> </li> <li> <p>Generate multiple contour plots for different variable pairs and fixed values to explore the model's predictions.</p> </li> <li> <p>Contour plots are useful for diagnosing model behavior, visualizing optima, and communicating results.</p> </li> <li> <p>In web UI, create multiple plots to compare different variable combinations side-by-side.</p> </li> </ul>"},{"location":"visualizations/contour_plot/#further-reading","title":"Further Reading","text":"<ul> <li>Parity Plot - Verify model prediction accuracy</li> <li>Q-Q Plot - Check model calibration</li> <li>Model Performance - Comprehensive model evaluation</li> <li>Web Application Guide - Complete web UI documentation</li> </ul> <p>For more on model evaluation and error metrics, see the Metrics Evolution section.</p>"},{"location":"visualizations/metrics_plot/","title":"Metrics evolution","text":""},{"location":"visualizations/metrics_plot/#metrics-evolution-plot","title":"Metrics Evolution Plot","text":"<p>The metrics evolution plot in ALchemist tracks how your Gaussian Process model's predictive performance changes as you collect more experimental data during the active learning loop. It provides a visual record of model improvement and helps you decide when to stop optimization.</p>"},{"location":"visualizations/metrics_plot/#what-the-metrics-plot-shows","title":"What the Metrics Plot Shows","text":"<p>X-axis: Number of observations (training data points) Y-axis: Performance metric value(s)</p> <p>Key elements:</p> <ul> <li> <p>Line plot(s): Evolution of selected metric(s) over data collection</p> </li> <li> <p>Metric options: RMSE, MAE, MAPE, R\u00b2</p> </li> <li> <p>Multiple metrics: Can display several metrics simultaneously</p> </li> <li> <p>Trends: Visual indication of convergence or degradation</p> </li> </ul>"},{"location":"visualizations/metrics_plot/#available-metrics","title":"Available Metrics","text":""},{"location":"visualizations/metrics_plot/#root-mean-squared-error-rmse","title":"Root Mean Squared Error (RMSE)","text":"\\[ \\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2} \\] <p>Interpretation:</p> <ul> <li> <p>Units match your response variable</p> </li> <li> <p>Lower is better (0 = perfect predictions)</p> </li> <li> <p>Penalizes large errors more than small ones</p> </li> <li> <p>Most common metric for regression</p> </li> </ul> <p>Typical values:</p> <ul> <li> <p>RMSE &lt; 5% of response range: Excellent</p> </li> <li> <p>RMSE 5-10% of response range: Good</p> </li> <li> <p>RMSE &gt; 20% of response range: Poor or insufficient data</p> </li> </ul>"},{"location":"visualizations/metrics_plot/#mean-absolute-error-mae","title":"Mean Absolute Error (MAE)","text":"\\[ \\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i| \\] <p>Interpretation:</p> <ul> <li> <p>Units match your response variable</p> </li> <li> <p>Lower is better (0 = perfect predictions)</p> </li> <li> <p>Less sensitive to outliers than RMSE</p> </li> <li> <p>Average magnitude of errors</p> </li> </ul> <p>Comparison to RMSE:</p> <ul> <li> <p>MAE &lt; RMSE always (equality only if all errors identical)</p> </li> <li> <p>RMSE/MAE ratio indicates error distribution</p> </li> <li> <p>Ratio \u2248 1: Uniform errors</p> </li> <li> <p>Ratio &gt; 1.5: Some large errors (outliers)</p> </li> </ul>"},{"location":"visualizations/metrics_plot/#mean-absolute-percentage-error-mape","title":"Mean Absolute Percentage Error (MAPE)","text":"\\[ \\text{MAPE} = \\frac{100\\%}{n}\\sum_{i=1}^{n}\\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right| \\] <p>Interpretation:</p> <ul> <li> <p>Percentage units (scale-independent)</p> </li> <li> <p>Lower is better (0% = perfect predictions)</p> </li> <li> <p>Useful for comparing across different response ranges</p> </li> <li> <p>Can be unstable if \\(y_i\\) near zero</p> </li> </ul> <p>Typical values:</p> <ul> <li> <p>MAPE &lt; 5%: Excellent</p> </li> <li> <p>MAPE 5-10%: Good</p> </li> <li> <p>MAPE 10-20%: Acceptable</p> </li> <li> <p>MAPE &gt; 20%: Poor</p> </li> </ul> <p>Warning: Undefined if any true value is exactly zero. ALchemist skips MAPE calculation in this case.</p>"},{"location":"visualizations/metrics_plot/#coefficient-of-determination-r2","title":"Coefficient of Determination (R\u00b2)","text":"\\[ R^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2} \\] <p>Where \\(\\bar{y}\\) is the mean of observed values.</p> <p>Interpretation:</p> <ul> <li> <p>Dimensionless (0 to 1 for good models)</p> </li> <li> <p>Higher is better (1 = perfect predictions)</p> </li> <li> <p>Proportion of variance explained by model</p> </li> <li> <p>Can be negative for very poor models</p> </li> </ul> <p>Typical values:</p> <ul> <li> <p>R\u00b2 &gt; 0.95: Excellent</p> </li> <li> <p>R\u00b2 0.90-0.95: Good</p> </li> <li> <p>R\u00b2 0.80-0.90: Acceptable</p> </li> <li> <p>R\u00b2 &lt; 0.80: Poor or insufficient data</p> </li> </ul> <p>Note: R\u00b2 from cross-validation (ALchemist default) is more reliable than training R\u00b2.</p>"},{"location":"visualizations/metrics_plot/#when-to-use-the-metrics-plot","title":"When to Use the Metrics Plot","text":""},{"location":"visualizations/metrics_plot/#during-active-learning","title":"During Active Learning","text":"<p>Essential for:</p> <ul> <li> <p>Monitoring model improvement as data accumulates</p> </li> <li> <p>Deciding when to stop optimization</p> </li> <li> <p>Detecting convergence or plateaus</p> </li> <li> <p>Identifying data quality issues</p> </li> </ul> <p>Check metrics plot:</p> <ul> <li> <p>After each batch of experiments</p> </li> <li> <p>Before requesting new candidates</p> </li> <li> <p>When considering stopping criteria</p> </li> </ul>"},{"location":"visualizations/metrics_plot/#comparing-models","title":"Comparing Models","text":"<p>Use metrics plot to:</p> <ul> <li> <p>Compare different kernels (Matern \u03bd=1.5 vs RBF)</p> </li> <li> <p>Evaluate different backends (sklearn vs BoTorch)</p> </li> <li> <p>Test impact of transforms</p> </li> <li> <p>Assess hyperparameter choices</p> </li> </ul>"},{"location":"visualizations/metrics_plot/#diagnosing-issues","title":"Diagnosing Issues","text":"<p>Metrics plot reveals:</p> <ul> <li> <p>Degrading performance (possible overfitting)</p> </li> <li> <p>Stagnant metrics (plateau reached)</p> </li> <li> <p>Erratic behavior (data quality problems)</p> </li> <li> <p>Unexpected trends (check preprocessing)</p> </li> </ul>"},{"location":"visualizations/metrics_plot/#accessing-the-metrics-plot","title":"Accessing the Metrics Plot","text":""},{"location":"visualizations/metrics_plot/#in-web-application","title":"In Web Application","text":"<ol> <li>Train a model in the GPR Panel</li> <li>Click \"Show Model Visualizations\"</li> <li>Select \"Metrics Plot\" from plot type buttons</li> <li>Choose which metrics to display (checkboxes)</li> </ol>"},{"location":"visualizations/metrics_plot/#in-desktop-application","title":"In Desktop Application","text":"<ol> <li>Train model in Model panel</li> <li>Open Visualizations dialog</li> <li>Metrics evolution available in plot options</li> <li>Customize display and export</li> </ol>"},{"location":"visualizations/metrics_plot/#interpreting-common-patterns","title":"Interpreting Common Patterns","text":""},{"location":"visualizations/metrics_plot/#ideal-pattern-steady-improvement","title":"Ideal Pattern: Steady Improvement","text":"<pre><code>Metrics improve (RMSE/MAE/MAPE decrease, R\u00b2 increases) as data accumulates\nRate of improvement slows as model converges\nEventually plateaus at acceptable performance\n</code></pre> <p>What this means: Active learning is working as expected Action: Continue until plateau, then stop optimization</p> <p>Example:</p> <ul> <li> <p>Start: RMSE = 15, R\u00b2 = 0.60 (10 samples)</p> </li> <li> <p>Mid: RMSE = 8, R\u00b2 = 0.88 (25 samples)</p> </li> <li> <p>End: RMSE = 5, R\u00b2 = 0.94 (40 samples) \u2192 Plateau</p> </li> </ul>"},{"location":"visualizations/metrics_plot/#warning-pattern-degrading-performance","title":"Warning Pattern: Degrading Performance","text":"<pre><code>Metrics worsen (RMSE/MAE increase, R\u00b2 decreases) as data accumulates\nPerformance peaks early then declines\nMay indicate overfitting or data issues\n</code></pre> <p>What this means: Problem with data quality or model Actions:</p> <ol> <li>Check for outliers in recent data</li> <li>Verify experimental measurements</li> <li>Inspect data preprocessing</li> <li>Try different kernel or regularization</li> <li>Check if hyperparameter optimization failing</li> </ol>"},{"location":"visualizations/metrics_plot/#warning-pattern-no-improvement","title":"Warning Pattern: No Improvement","text":"<pre><code>Metrics flat or erratic across all data sizes\nNo clear trend of improvement\nHigh variance between evaluations\n</code></pre> <p>What this means: Model not learning from data Actions:</p> <ol> <li>Verify variable space covers response range</li> <li>Check data is actually varying (not constant)</li> <li>Ensure preprocessing appropriate</li> <li>Try different kernel family</li> <li>Inspect initial sampling distribution</li> </ol>"},{"location":"visualizations/metrics_plot/#expected-pattern-early-volatility","title":"Expected Pattern: Early Volatility","text":"<pre><code>Metrics fluctuate significantly with very few samples (&lt; 15)\nBehavior stabilizes as data accumulates\nTrends become clear after 20-30 samples\n</code></pre> <p>What this means: Normal statistical noise with small samples Action: Don't over-interpret early behavior, wait for more data</p>"},{"location":"visualizations/metrics_plot/#deciding-when-to-stop","title":"Deciding When to Stop","text":""},{"location":"visualizations/metrics_plot/#convergence-criteria","title":"Convergence Criteria","text":"<p>Metrics-based:</p> <ul> <li> <p>RMSE/MAE plateau (&lt; 5% change over 10 samples)</p> </li> <li> <p>R\u00b2 &gt; target threshold (e.g., 0.90 or 0.95)</p> </li> <li> <p>Absolute performance acceptable for application</p> </li> </ul> <p>Practical:</p> <ul> <li> <p>Budget exhausted (time, cost, materials)</p> </li> <li> <p>Acceptable optimum found (target performance reached)</p> </li> <li> <p>Diminishing returns (effort exceeds benefit)</p> </li> </ul>"},{"location":"visualizations/metrics_plot/#common-stopping-rules","title":"Common Stopping Rules","text":"<p>Conservative:</p> <ul> <li> <p>R\u00b2 &gt; 0.95 AND no improvement in last 15 samples</p> </li> <li> <p>RMSE &lt; 2% of response range for 20 consecutive samples</p> </li> <li> <p>Validation metrics stable across 3 CV folds</p> </li> </ul> <p>Moderate:</p> <ul> <li> <p>R\u00b2 &gt; 0.90 AND plateau for 10 samples</p> </li> <li> <p>RMSE &lt; 5% of response range</p> </li> <li> <p>Acquisition function values &lt; threshold</p> </li> </ul> <p>Aggressive:</p> <ul> <li> <p>R\u00b2 &gt; 0.85 achieved</p> </li> <li> <p>RMSE better than baseline/literature</p> </li> <li> <p>Optimization objective reached</p> </li> </ul>"},{"location":"visualizations/metrics_plot/#understanding-cross-validation-metrics","title":"Understanding Cross-Validation Metrics","text":""},{"location":"visualizations/metrics_plot/#k-fold-cross-validation","title":"K-Fold Cross-Validation","text":"<p>ALchemist uses k-fold cross-validation (default: 5 folds) for all datasets:</p> <p>Process:</p> <ol> <li>Split data into k groups (typically 5)</li> <li>For each group:</li> <li>Train on other k-1 groups</li> <li>Predict on held-out group</li> <li>Aggregate predictions across all folds</li> </ol> <p>Advantages:</p> <ul> <li> <p>Good balance of bias/variance</p> </li> <li> <p>Computationally efficient</p> </li> <li> <p>Standard practice in machine learning</p> </li> <li> <p>Reliable estimates across dataset sizes</p> </li> </ul> <p>Interpretation:</p> <ul> <li> <p>Reflects generalization to new data</p> </li> <li> <p>More pessimistic than training metrics</p> </li> <li> <p>More realistic for decision-making</p> </li> </ul>"},{"location":"visualizations/metrics_plot/#metric-selection-guidelines","title":"Metric Selection Guidelines","text":""},{"location":"visualizations/metrics_plot/#use-rmse-when","title":"Use RMSE when:","text":"<ul> <li> <p>Response units meaningful (e.g., temperature \u00b0C, yield %)</p> </li> <li> <p>Large errors particularly problematic</p> </li> <li> <p>Standard metric expected in your field</p> </li> <li> <p>Comparing models on same response</p> </li> </ul>"},{"location":"visualizations/metrics_plot/#use-mae-when","title":"Use MAE when:","text":"<ul> <li> <p>Outliers present (more robust)</p> </li> <li> <p>All errors treated equally important</p> </li> <li> <p>Easier interpretation needed (average error)</p> </li> <li> <p>RMSE vs MAE comparison informative (error distribution)</p> </li> </ul>"},{"location":"visualizations/metrics_plot/#use-mape-when","title":"Use MAPE when:","text":"<ul> <li> <p>Comparing across different response scales</p> </li> <li> <p>Percentage errors more interpretable</p> </li> <li> <p>Response values far from zero (avoid division issues)</p> </li> <li> <p>Scale-independent comparison needed</p> </li> </ul>"},{"location":"visualizations/metrics_plot/#use-r2-when","title":"Use R\u00b2 when:","text":"<ul> <li> <p>Variance explanation important</p> </li> <li> <p>Comparing to baseline (R\u00b2 = 0)</p> </li> <li> <p>Standard metric in your field (common in chemistry/materials)</p> </li> <li> <p>Want single dimensionless metric</p> </li> </ul>"},{"location":"visualizations/metrics_plot/#display-multiple-metrics-when","title":"Display multiple metrics when:","text":"<ul> <li> <p>Want comprehensive view</p> </li> <li> <p>Different stakeholders prefer different metrics</p> </li> <li> <p>Checking consistency across metrics</p> </li> <li> <p>Diagnosing specific issues</p> </li> </ul>"},{"location":"visualizations/metrics_plot/#integration-with-optimization","title":"Integration with Optimization","text":"<p>Metrics evolution informs optimization strategy:</p>"},{"location":"visualizations/metrics_plot/#acquisition-function-choice","title":"Acquisition Function Choice","text":"<p>Poor metrics (R\u00b2 &lt; 0.75):</p> <ul> <li> <p>Favor exploration (UCB with high \u03ba)</p> </li> <li> <p>Collect diverse data first</p> </li> <li> <p>Consider space-filling designs</p> </li> </ul> <p>Good metrics (R\u00b2 &gt; 0.90):</p> <ul> <li> <p>Allow exploitation (EI, PI)</p> </li> <li> <p>Trust model predictions</p> </li> <li> <p>Focus on promising regions</p> </li> </ul>"},{"location":"visualizations/metrics_plot/#batch-size-decisions","title":"Batch Size Decisions","text":"<p>Rapidly improving metrics:</p> <ul> <li> <p>Smaller batches (adapt quickly)</p> </li> <li> <p>Re-train frequently</p> </li> <li> <p>Stay responsive to learning</p> </li> </ul> <p>Plateaued metrics:</p> <ul> <li> <p>Larger batches acceptable</p> </li> <li> <p>Less frequent re-training</p> </li> <li> <p>Efficiency over responsiveness</p> </li> </ul>"},{"location":"visualizations/metrics_plot/#stopping-criteria","title":"Stopping Criteria","text":"<p>Metrics-driven stopping:</p> <ul> <li> <p>Set R\u00b2 or RMSE threshold</p> </li> <li> <p>Monitor plateau duration</p> </li> <li> <p>Balance performance vs cost</p> </li> </ul>"},{"location":"visualizations/metrics_plot/#practical-tips","title":"Practical Tips","text":""},{"location":"visualizations/metrics_plot/#displaying-multiple-metrics","title":"Displaying Multiple Metrics","text":"<p>Recommended combinations:</p> <ul> <li> <p>Standard: RMSE + R\u00b2 (accuracy and variance explained)</p> </li> <li> <p>Comprehensive: RMSE + MAE + R\u00b2 (multiple perspectives)</p> </li> <li> <p>Scale-independent: MAPE + R\u00b2 (for broad comparisons)</p> </li> </ul> <p>Avoid:</p> <ul> <li> <p>Too many metrics (cluttered plot)</p> </li> <li> <p>Redundant pairs (RMSE + MAE without reason)</p> </li> </ul>"},{"location":"visualizations/metrics_plot/#interpreting-trends","title":"Interpreting Trends","text":"<p>Smooth trends: Good model stability Erratic jumps: Check data quality or hyperparameter optimization Sudden drops: Possible outlier added or CV issue Linear improvement: Still learning, more data beneficial</p>"},{"location":"visualizations/metrics_plot/#comparing-sessions","title":"Comparing Sessions","text":"<p>When comparing:</p> <ul> <li> <p>Use same metrics across sessions</p> </li> <li> <p>Account for different data sizes</p> </li> <li> <p>Consider response scale differences</p> </li> <li> <p>Check cross-validation method consistency</p> </li> </ul>"},{"location":"visualizations/metrics_plot/#troubleshooting","title":"Troubleshooting","text":"<p>If metrics aren't improving, check the parity plot for systematic bias and try different kernels. If metrics worsen, inspect recent data for outliers. High variance is expected with small datasets (n &lt; 25). For persistent issues, see Model Performance.</p>"},{"location":"visualizations/metrics_plot/#further-reading","title":"Further Reading","text":"<ul> <li>Parity Plot - Visual accuracy assessment</li> <li>Model Performance - Comprehensive model evaluation</li> <li>Q-Q Plot - Uncertainty calibration diagnostic</li> <li>Calibration Curve - Coverage verification</li> </ul> <p>Key Takeaway: The metrics evolution plot is your guide to tracking model improvement during active learning. Use it to decide when your model is good enough and when to stop collecting data.</p>"},{"location":"visualizations/parity_plot/","title":"Parity plot","text":""},{"location":"visualizations/parity_plot/#parity-plot","title":"Parity Plot","text":"<p>The parity plot (also called actual vs. predicted plot) is a fundamental diagnostic tool for evaluating your surrogate model's prediction accuracy. It shows how well the model's predictions match the actual experimental measurements.</p>"},{"location":"visualizations/parity_plot/#what-the-parity-plot-shows","title":"What the Parity Plot Shows","text":"<p>X-axis: Actual (true) experimental values Y-axis: Model predicted values</p> <p>Perfect predictions: All points lie on the diagonal line (y = x), indicating predictions exactly match observations.</p> <p>Additional information displayed:</p> <ul> <li> <p>Error bars: Optional uncertainty visualization (\u00b11\u03c3, \u00b11.96\u03c3, \u00b12\u03c3, \u00b12.58\u03c3, or \u00b13\u03c3)</p> </li> <li> <p>Performance metrics: RMSE, MAE, and R\u00b2 displayed in the plot title</p> </li> <li> <p>Parity line: Diagonal reference (y = x) for perfect predictions</p> </li> </ul>"},{"location":"visualizations/parity_plot/#interpreting-the-plot","title":"Interpreting the Plot","text":""},{"location":"visualizations/parity_plot/#excellent-model-fit","title":"Excellent Model Fit","text":"<p>What it looks like:</p> <ul> <li> <p>Points tightly clustered along diagonal line</p> </li> <li> <p>Minimal scatter</p> </li> <li> <p>R\u00b2 &gt; 0.9</p> </li> <li> <p>RMSE and MAE are small relative to output range</p> </li> </ul> <p>What it means:</p> <ul> <li> <p>Model predictions are highly accurate</p> </li> <li> <p>Strong confidence in optimization decisions</p> </li> <li> <p>Safe to trust acquisition function suggestions</p> </li> </ul>"},{"location":"visualizations/parity_plot/#good-model-fit","title":"Good Model Fit \ud83d\udc4d","text":"<p>What it looks like:</p> <ul> <li> <p>Points generally follow diagonal with moderate scatter</p> </li> <li> <p>R\u00b2 between 0.7-0.9</p> </li> <li> <p>No systematic bias</p> </li> </ul> <p>What it means:</p> <ul> <li> <p>Model captures main trends</p> </li> <li> <p>Acceptable for optimization</p> </li> <li> <p>Some uncertainty in predictions</p> </li> </ul>"},{"location":"visualizations/parity_plot/#poor-model-fit","title":"Poor Model Fit","text":"<p>What it looks like:</p> <ul> <li> <p>Large scatter around diagonal</p> </li> <li> <p>R\u00b2 &lt; 0.5</p> </li> <li> <p>High RMSE relative to output range</p> </li> </ul> <p>What it means:</p> <ul> <li> <p>Model has difficulty predicting outcomes</p> </li> <li> <p>Consider collecting more data</p> </li> <li> <p>Try different kernel or backend</p> </li> <li> <p>Check data quality</p> </li> </ul>"},{"location":"visualizations/parity_plot/#systematic-bias","title":"Systematic Bias \ud83d\udd34","text":"<p>What it looks like:</p> <ul> <li> <p>Points systematically above or below diagonal</p> </li> <li> <p>Clear pattern rather than random scatter</p> </li> </ul> <p>What it means:</p> <ul> <li> <p>Above diagonal: Model consistently under-predicts (predicts lower than actual)</p> </li> <li> <p>Below diagonal: Model consistently over-predicts (predicts higher than actual)</p> </li> <li> <p>Check data preprocessing and transforms</p> </li> <li> <p>May indicate model misspecification</p> </li> </ul>"},{"location":"visualizations/parity_plot/#cross-validation-approach","title":"Cross-Validation Approach","text":"<p>ALchemist's parity plot uses k-fold cross-validation to provide unbiased estimates:</p> <ol> <li>Data is split into k folds (typically 5)</li> <li>For each fold:</li> <li>Train model on remaining k-1 folds</li> <li>Predict on held-out fold</li> <li>Aggregate all predictions for complete dataset coverage</li> </ol> <p>Benefits:</p> <ul> <li> <p>Predictions for every point without using that point in training</p> </li> <li> <p>Unbiased estimate of generalization performance</p> </li> <li> <p>More reliable than training set predictions</p> </li> </ul>"},{"location":"visualizations/parity_plot/#error-bars-and-uncertainty","title":"Error Bars and Uncertainty","text":""},{"location":"visualizations/parity_plot/#selecting-confidence-intervals","title":"Selecting Confidence Intervals","text":"<p>Choose from standard statistical confidence intervals:</p> <ul> <li> <p>\u00b11\u03c3 (68%): Standard deviation, 68% of true values should fall within</p> </li> <li> <p>\u00b11.96\u03c3 (95%): Most common, 95% confidence interval</p> </li> <li> <p>\u00b12\u03c3 (95.4%): Approximately 2-sigma interval</p> </li> <li> <p>\u00b12.58\u03c3 (99%): High confidence, 99% of true values</p> </li> <li> <p>\u00b13\u03c3 (99.7%): Very high confidence, three-sigma interval</p> </li> </ul>"},{"location":"visualizations/parity_plot/#interpreting-error-bars","title":"Interpreting Error Bars","text":"<p>Well-calibrated uncertainty:</p> <ul> <li> <p>Error bars cross the diagonal line for most points</p> </li> <li> <p>About 68% of points within \u00b11\u03c3, 95% within \u00b12\u03c3</p> </li> </ul> <p>Under-confident predictions:</p> <ul> <li> <p>Error bars are much larger than actual deviations</p> </li> <li> <p>Most points fall well within error bars</p> </li> <li> <p>Model is too cautious</p> </li> </ul> <p>Over-confident predictions:</p> <ul> <li> <p>Error bars are smaller than actual deviations</p> </li> <li> <p>Many points fall outside error bars</p> </li> <li> <p>Model underestimates uncertainty (see Q-Q plot for more)</p> </li> </ul>"},{"location":"visualizations/parity_plot/#calibrated-vs-uncalibrated-results","title":"Calibrated vs. Uncalibrated Results","text":"<p>ALchemist provides both calibrated and uncalibrated predictions:</p>"},{"location":"visualizations/parity_plot/#uncalibrated-raw-model-output","title":"Uncalibrated (Raw Model Output)","text":"<ul> <li> <p>Direct predictions from Gaussian Process</p> </li> <li> <p>May have over/under-confident uncertainty estimates</p> </li> <li> <p>Useful for comparing with calibrated results</p> </li> </ul>"},{"location":"visualizations/parity_plot/#calibrated-adjusted-uncertainty","title":"Calibrated (Adjusted Uncertainty)","text":"<ul> <li> <p>Uncertainty scaled based on cross-validation residuals</p> </li> <li> <p>Corrects systematic over/under-confidence</p> </li> <li> <p>Recommended for decision-making</p> </li> <li> <p>Toggle available in visualization panel</p> </li> </ul> <p>For more on calibration, see Interpreting Calibration Curves.</p>"},{"location":"visualizations/parity_plot/#performance-metrics","title":"Performance Metrics","text":""},{"location":"visualizations/parity_plot/#rmse-root-mean-squared-error","title":"RMSE (Root Mean Squared Error)","text":"\\[ \\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2} \\] <ul> <li> <p>Measures average prediction error magnitude</p> </li> <li> <p>Same units as output variable</p> </li> <li> <p>Sensitive to large errors (squared term)</p> </li> <li> <p>Lower is better</p> </li> </ul> <p>Interpretation:</p> <ul> <li> <p>RMSE = 0: Perfect predictions</p> </li> <li> <p>RMSE &lt;&lt; output range: Excellent fit</p> </li> <li> <p>RMSE \u2248 output std dev: Poor fit (no better than mean prediction)</p> </li> </ul>"},{"location":"visualizations/parity_plot/#mae-mean-absolute-error","title":"MAE (Mean Absolute Error)","text":"\\[ \\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i| \\] <ul> <li> <p>Average absolute difference between predictions and actual</p> </li> <li> <p>Same units as output variable</p> </li> <li> <p>Less sensitive to outliers than RMSE</p> </li> <li> <p>Lower is better</p> </li> </ul> <p>Interpretation:</p> <ul> <li> <p>MAE typically &lt; RMSE (due to no squaring)</p> </li> <li> <p>If MAE \u2248 RMSE: Errors are consistently sized</p> </li> <li> <p>If MAE &lt;&lt; RMSE: Some large outlier errors</p> </li> </ul>"},{"location":"visualizations/parity_plot/#r2-coefficient-of-determination","title":"R\u00b2 (Coefficient of Determination)","text":"\\[ R^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2} \\] <ul> <li> <p>Fraction of variance explained by model</p> </li> <li> <p>Dimensionless (0 to 1 for good models)</p> </li> <li> <p>Can be negative for very poor fits</p> </li> </ul> <p>Interpretation:</p> <ul> <li> <p>R\u00b2 = 1.0: Perfect predictions</p> </li> <li> <p>R\u00b2 &gt; 0.9: Excellent fit</p> </li> <li> <p>R\u00b2 = 0.7-0.9: Good fit</p> </li> <li> <p>R\u00b2 = 0.5-0.7: Moderate fit</p> </li> <li> <p>R\u00b2 &lt; 0.5: Poor fit</p> </li> <li> <p>R\u00b2 &lt; 0: Model worse than predicting mean</p> </li> </ul>"},{"location":"visualizations/parity_plot/#practical-guidelines","title":"Practical Guidelines","text":""},{"location":"visualizations/parity_plot/#when-to-be-satisfied","title":"When to Be Satisfied","text":"<p>Proceed with optimization if:</p> <ul> <li> <p>R\u00b2 &gt; 0.7 with no systematic bias</p> </li> <li> <p>Error bars reasonable (not too wide or narrow)</p> </li> <li> <p>No obvious outliers or patterns</p> </li> <li> <p>Metrics improve as data is added</p> </li> </ul>"},{"location":"visualizations/parity_plot/#when-to-improve-model","title":"When to Improve Model","text":"<p>Take action if:</p> <ul> <li> <p>R\u00b2 &lt; 0.5 or negative</p> </li> <li> <p>Clear systematic bias visible</p> </li> <li> <p>Many points outside error bars (over-confident)</p> </li> <li> <p>Error bars much wider than scatter (under-confident)</p> </li> </ul>"},{"location":"visualizations/parity_plot/#remediation-strategies","title":"Remediation Strategies","text":"<p>For poor R\u00b2: 1. Collect more training data 2. Try different kernel (RBF \u2194 Matern, adjust \u03bd) 3. Switch backend (sklearn \u2194 BoTorch) 4. Check data quality (outliers, measurement errors) 5. Apply input/output transforms</p> <p>For systematic bias: 1. Check data preprocessing 2. Verify units and scales 3. Try different kernel 4. Check for missing variables or physical constraints</p> <p>For miscalibrated uncertainty: 1. Use calibration feature (automatic in ALchemist) 2. Adjust noise parameter 3. See Q-Q plot for diagnosis</p>"},{"location":"visualizations/parity_plot/#using-the-parity-plot-in-workflows","title":"Using the Parity Plot in Workflows","text":""},{"location":"visualizations/parity_plot/#during-initial-modeling","title":"During Initial Modeling","text":"<ul> <li> <p>Generate after first model training</p> </li> <li> <p>Check R\u00b2 &gt; 0.5 before proceeding</p> </li> <li> <p>Identify if more initial data needed</p> </li> </ul>"},{"location":"visualizations/parity_plot/#during-active-learning","title":"During Active Learning","text":"<ul> <li> <p>Monitor after each iteration</p> </li> <li> <p>Watch for degradation (may indicate overfitting)</p> </li> <li> <p>R\u00b2 should generally improve with more data</p> </li> </ul>"},{"location":"visualizations/parity_plot/#before-final-optimization","title":"Before Final Optimization","text":"<ul> <li> <p>Ensure R\u00b2 &gt; 0.7</p> </li> <li> <p>Verify calibration quality</p> </li> <li> <p>Confirm no systematic bias</p> </li> <li> <p>Check that best experiments are well-predicted</p> </li> </ul>"},{"location":"visualizations/parity_plot/#desktop-vs-web-ui","title":"Desktop vs. Web UI","text":"<p>Desktop Application:</p> <ul> <li> <p>Access via Visualizations dialog after training model</p> </li> <li> <p>Full Matplotlib controls for zoom, pan, save</p> </li> <li> <p>Customization options for publication-quality figures</p> </li> </ul> <p>Web Application:</p> <ul> <li> <p>Embedded in visualizations panel</p> </li> <li> <p>Interactive Recharts visualization</p> </li> <li> <p>Theme-aware (light/dark mode)</p> </li> <li> <p>Select error bar confidence levels</p> </li> <li> <p>Toggle calibrated/uncalibrated results</p> </li> </ul>"},{"location":"visualizations/parity_plot/#example-interpretations","title":"Example Interpretations","text":""},{"location":"visualizations/parity_plot/#case-1-excellent-fit","title":"Case 1: Excellent Fit","text":"<pre><code>R\u00b2 = 0.94, RMSE = 1.2, MAE = 0.9\nPoints tightly along diagonal, error bars appropriate\n\u2192 Model ready for optimization, trust suggestions\n</code></pre>"},{"location":"visualizations/parity_plot/#case-2-under-predicting","title":"Case 2: Under-Predicting","text":"<pre><code>R\u00b2 = 0.72, RMSE = 3.1, MAE = 2.8\nPoints systematically above diagonal\n\u2192 Check data units, try transforms, more data needed\n</code></pre>"},{"location":"visualizations/parity_plot/#case-3-high-uncertainty","title":"Case 3: High Uncertainty","text":"<pre><code>R\u00b2 = 0.81, RMSE = 2.0, MAE = 1.5\nLarge error bars, but points within them\n\u2192 Under-confident, consider calibration or tighter kernel\n</code></pre>"},{"location":"visualizations/parity_plot/#case-4-poor-fit","title":"Case 4: Poor Fit","text":"<pre><code>R\u00b2 = 0.28, RMSE = 8.5, MAE = 7.2\nLarge scatter, no clear pattern\n\u2192 Collect more data, check data quality, try different kernel\n</code></pre>"},{"location":"visualizations/parity_plot/#further-reading","title":"Further Reading","text":"<ul> <li> <p>Metrics Evolution - Track RMSE, MAE, R\u00b2 over iterations</p> </li> <li> <p>Q-Q Plot - Uncertainty calibration diagnostic</p> </li> <li> <p>Calibration Curve - Coverage assessment</p> </li> <li> <p>Model Performance - Troubleshooting poor fits</p> </li> </ul> <p>The parity plot is your primary tool for assessing model quality. Combined with Q-Q plots and calibration curves, you get a complete picture of both prediction accuracy and uncertainty calibration.</p>"},{"location":"visualizations/qq_plot/","title":"Q-Q plot","text":""},{"location":"visualizations/qq_plot/#q-q-plot","title":"Q-Q Plot","text":"<p>The Q-Q plot (quantile-quantile plot) in ALchemist is a specialized diagnostic tool that helps you assess whether your Gaussian Process model's uncertainty estimates are well-calibrated. It compares the distribution of standardized residuals from cross-validation against the theoretical normal distribution.</p>"},{"location":"visualizations/qq_plot/#what-the-q-q-plot-shows","title":"What the Q-Q Plot Shows","text":"<p>X-axis: Theoretical quantiles from standard normal distribution \\(\\mathcal{N}(0,1)\\) Y-axis: Observed standardized residuals (z-scores) from cross-validation predictions</p> <p>Key elements:</p> <ul> <li> <p>Scatter points: Each point represents one cross-validation prediction</p> </li> <li> <p>Diagonal line: Perfect calibration reference (y = x)</p> </li> <li> <p>Confidence band: Expected deviation range for finite samples (shown when N &lt; 100)</p> </li> <li> <p>Diagnostic text: Mean(z) and Std(z) with calibration status</p> </li> </ul>"},{"location":"visualizations/qq_plot/#quick-interpretation-guide","title":"Quick Interpretation Guide","text":"Pattern Mean(z) Std(z) Status What It Means Points on diagonal \u22480 \u22481.0 \u2713 Well-calibrated Uncertainties are accurate Points above diagonal \u22480 &gt;1.0 Over-confident Intervals too narrow Points below diagonal \u22480 &lt;1.0 Under-confident Intervals too wide Shifted upward &gt;0 any \ud83d\udd34 Under-predicting Systematic bias Shifted downward &lt;0 any \ud83d\udd34 Over-predicting Systematic bias"},{"location":"visualizations/qq_plot/#understanding-standardized-residuals","title":"Understanding Standardized Residuals","text":"<p>For each cross-validation prediction, the z-score is:</p> \\[ z_i = \\frac{y_i^{\\text{true}} - y_i^{\\text{pred}}}{\\sigma_i} \\] <p>Where:</p> <ul> <li> <p>\\(y_i^{\\text{true}}\\) = actual experimental value</p> </li> <li> <p>\\(y_i^{\\text{pred}}\\) = model prediction</p> </li> <li> <p>\\(\\sigma_i\\) = predicted standard deviation</p> </li> </ul> <p>If well-calibrated: z-scores should follow \\(\\mathcal{N}(0,1)\\) distribution</p>"},{"location":"visualizations/qq_plot/#when-to-use-the-q-q-plot","title":"When to Use the Q-Q Plot","text":""},{"location":"visualizations/qq_plot/#essential-situations","title":"Essential Situations","text":"<p>Before optimization decisions:</p> <ul> <li> <p>Verify uncertainty estimates are reliable</p> </li> <li> <p>Check if acquisition functions can be trusted</p> </li> <li> <p>Assess risk of over-confident predictions</p> </li> </ul> <p>After model training:</p> <ul> <li> <p>Initial calibration check</p> </li> <li> <p>Compare different backends (sklearn vs BoTorch)</p> </li> <li> <p>Evaluate impact of kernel choices</p> </li> </ul> <p>During active learning:</p> <ul> <li> <p>Monitor calibration as data accumulates</p> </li> <li> <p>Detect if model becomes over/under-confident</p> </li> <li> <p>Ensure continued reliability</p> </li> </ul>"},{"location":"visualizations/qq_plot/#combined-with-other-diagnostics","title":"Combined with Other Diagnostics","text":"<p>Use Q-Q plot alongside:</p> <ul> <li> <p>Parity plot: Check prediction accuracy (R\u00b2, RMSE)</p> </li> <li> <p>Calibration curve: Verify coverage at confidence levels</p> </li> <li> <p>Metrics plot: Monitor performance trends</p> </li> </ul>"},{"location":"visualizations/qq_plot/#accessing-the-q-q-plot","title":"Accessing the Q-Q Plot","text":""},{"location":"visualizations/qq_plot/#in-web-application","title":"In Web Application","text":"<ol> <li>Train a model in the GPR Panel</li> <li>Click \"Show Model Visualizations\"</li> <li>Select \"Q-Q Plot\" from plot type buttons</li> <li>Toggle between calibrated/uncalibrated results</li> </ol>"},{"location":"visualizations/qq_plot/#in-desktop-application","title":"In Desktop Application","text":"<ol> <li>Train model in Model panel</li> <li>Open Visualizations dialog</li> <li>Q-Q plot available in visualization options</li> <li>Can customize and save for publications</li> </ol>"},{"location":"visualizations/qq_plot/#interpreting-diagnostic-metrics","title":"Interpreting Diagnostic Metrics","text":""},{"location":"visualizations/qq_plot/#meanz-bias-assessment","title":"Mean(z): Bias Assessment","text":"\\[ \\text{Mean}(z) = \\frac{1}{n}\\sum_{i=1}^{n} z_i \\] <p>Ideal: Mean(z) \u2248 0 (within \u00b10.1)</p> <p>Problematic:</p> <ul> <li> <p>Mean(z) &gt; 0.3: Model consistently under-predicts</p> </li> <li> <p>Mean(z) &lt; -0.3: Model consistently over-predicts</p> </li> </ul> <p>Actions:</p> <ul> <li> <p>Check data preprocessing and units</p> </li> <li> <p>Verify output transforms are appropriate</p> </li> <li> <p>Try different kernel or backend</p> </li> <li> <p>Investigate data quality issues</p> </li> </ul>"},{"location":"visualizations/qq_plot/#stdz-calibration-assessment","title":"Std(z): Calibration Assessment","text":"\\[ \\text{Std}(z) = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}(z_i - \\bar{z})^2} \\] <p>Ideal: Std(z) \u2248 1.0 (within 0.9-1.1)</p> <p>Over-confident (Std(z) &gt; 1.1):</p> <ul> <li> <p>Model uncertainties too small</p> </li> <li> <p>Actual errors larger than predicted</p> </li> <li> <p>Risk of over-exploitation in optimization</p> </li> </ul> <p>Under-confident (Std(z) &lt; 0.9):</p> <ul> <li> <p>Model uncertainties too large</p> </li> <li> <p>Actual errors smaller than predicted</p> </li> <li> <p>Risk of over-exploration, wasted experiments</p> </li> </ul>"},{"location":"visualizations/qq_plot/#calibration-status-messages","title":"Calibration Status Messages","text":"<p>ALchemist automatically interprets Q-Q plot results:</p>"},{"location":"visualizations/qq_plot/#well-calibrated","title":"\u2713 Well-Calibrated","text":"<pre><code>Mean(z) = 0.02, Std(z) = 0.98\nStatus: \u2713 Well-calibrated uncertainties\n</code></pre> Action: None needed, model is ready for optimization"},{"location":"visualizations/qq_plot/#over-confident","title":"Over-Confident","text":"<pre><code>Mean(z) = -0.05, Std(z) = 1.45\nStatus: Over-confident (model uncertainties too small)\n</code></pre> Actions: <ul> <li> <p>Apply automatic calibration (built-in)</p> </li> <li> <p>Increase noise parameter</p> </li> <li> <p>Try more flexible kernel (Matern \u03bd=1.5)</p> </li> <li> <p>Collect more training data</p> </li> </ul>"},{"location":"visualizations/qq_plot/#under-confident","title":"Under-Confident","text":"<pre><code>Mean(z) = 0.08, Std(z) = 0.72\nStatus: Under-confident (model uncertainties too large)\n</code></pre> Actions: <ul> <li> <p>May be acceptable (conservative is safe)</p> </li> <li> <p>Try less flexible kernel (Matern \u03bd=2.5, RBF)</p> </li> <li> <p>Reduce explicit noise values</p> </li> <li> <p>Optimize kernel hyperparameters more aggressively</p> </li> </ul>"},{"location":"visualizations/qq_plot/#systematic-bias","title":"\ud83d\udd34 Systematic Bias","text":"<pre><code>Mean(z) = 0.45, Std(z) = 1.02\nStatus: \ud83d\udd34 Systematic bias (consistent under-prediction)\n</code></pre> Actions: <ul> <li> <p>Critical issue requiring attention</p> </li> <li> <p>Check data units and scaling</p> </li> <li> <p>Verify preprocessing steps</p> </li> <li> <p>Consider different kernel family</p> </li> <li> <p>Investigate data quality</p> </li> </ul>"},{"location":"visualizations/qq_plot/#sample-size-considerations","title":"Sample Size Considerations","text":""},{"location":"visualizations/qq_plot/#small-datasets-n-30","title":"Small Datasets (N &lt; 30)","text":"<ul> <li> <p>High variability expected</p> </li> <li> <p>Wider confidence bands</p> </li> <li> <p>Don't over-interpret moderate deviations</p> </li> <li> <p>Focus on overall trend rather than exact values</p> </li> </ul>"},{"location":"visualizations/qq_plot/#medium-datasets-30-n-100","title":"Medium Datasets (30 &lt; N &lt; 100)","text":"<ul> <li> <p>Moderate reliability</p> </li> <li> <p>Confidence bands still shown</p> </li> <li> <p>Deviations &gt;0.2 in Std(z) indicate issues</p> </li> <li> <p>Patterns become meaningful</p> </li> </ul>"},{"location":"visualizations/qq_plot/#large-datasets-n-100","title":"Large Datasets (N &gt; 100)","text":"<ul> <li> <p>High confidence in assessment</p> </li> <li> <p>No confidence bands (not needed)</p> </li> <li> <p>Even small deviations meaningful</p> </li> <li> <p>Std(z) should be within 0.95-1.05</p> </li> </ul>"},{"location":"visualizations/qq_plot/#automatic-calibration-in-alchemist","title":"Automatic Calibration in ALchemist","text":"<p>When miscalibration is detected, ALchemist automatically applies correction:</p> <p>Calibration Process:</p> <ol> <li>Calculate Std(z) from cross-validation</li> <li>Use as scaling factor: \\(\\sigma_{\\text{calibrated}} = \\sigma_{\\text{raw}} \\times \\text{Std}(z)\\)</li> <li>Apply to future predictions</li> </ol> <p>Effect:</p> <ul> <li> <p>Std(z) = 1.5 \u2192 Future uncertainties scaled up 1.5\u00d7</p> </li> <li> <p>Std(z) = 0.7 \u2192 Future uncertainties scaled down 0.7\u00d7</p> </li> <li> <p>Brings model toward better calibration</p> </li> </ul> <p>Toggle:</p> <ul> <li> <p>Compare calibrated vs uncalibrated in visualization panel</p> </li> <li> <p>See immediate impact of calibration</p> </li> <li> <p>Verify improvement in Q-Q plot</p> </li> </ul>"},{"location":"visualizations/qq_plot/#common-patterns-and-solutions","title":"Common Patterns and Solutions","text":""},{"location":"visualizations/qq_plot/#pattern-s-curve-sigmoid-shape","title":"Pattern: S-curve (Sigmoid Shape)","text":"<p>What it means: Heavier tails than normal distribution Actions: Check for outliers, consider robust scaling</p>"},{"location":"visualizations/qq_plot/#pattern-points-fan-out-at-extremes","title":"Pattern: Points Fan Out at Extremes","text":"<p>What it means: Heteroscedastic errors (variance changes) Actions: Try log transform on outputs, check data range</p>"},{"location":"visualizations/qq_plot/#pattern-multiple-clusters","title":"Pattern: Multiple Clusters","text":"<p>What it means: Multiple modes or subpopulations Actions: Check for categorical effects, investigate data stratification</p>"},{"location":"visualizations/qq_plot/#pattern-systematic-curve-but-stdz-1","title":"Pattern: Systematic Curve but Std(z) \u2248 1","text":"<p>What it means: Non-normal but correct variance Actions: Usually acceptable, functional form is more important</p>"},{"location":"visualizations/qq_plot/#integration-with-bayesian-optimization","title":"Integration with Bayesian Optimization","text":"<p>Q-Q plot calibration directly impacts optimization:</p>"},{"location":"visualizations/qq_plot/#expected-improvement-ei","title":"Expected Improvement (EI)","text":"<ul> <li> <p>Relies on \u03c3 for exploration/exploitation balance</p> </li> <li> <p>Over-confident \u2192 premature convergence</p> </li> <li> <p>Under-confident \u2192 excessive exploration</p> </li> </ul>"},{"location":"visualizations/qq_plot/#upper-confidence-bound-ucb","title":"Upper Confidence Bound (UCB)","text":"<ul> <li> <p>Uses \u03c3 directly in formula: UCB = \u03bc + \u03ba\u03c3</p> </li> <li> <p>Miscalibration affects all decisions</p> </li> <li> <p>Calibrated \u03c3 ensures optimal trade-off</p> </li> </ul>"},{"location":"visualizations/qq_plot/#probability-of-improvement-pi","title":"Probability of Improvement (PI)","text":"<ul> <li> <p>Depends on \u03c3 for probability calculation</p> </li> <li> <p>Correct calibration critical for thresholds</p> </li> </ul> <p>Bottom line: Well-calibrated uncertainty is essential for efficient optimization.</p>"},{"location":"visualizations/qq_plot/#troubleshooting","title":"Troubleshooting","text":"<p>ALchemist's automatic calibration (enabled by default) handles most calibration issues. For over-confident models (Std(z) &gt; 1.3), try a more flexible kernel like Matern \u03bd=1.5. For under-confident models (Std(z) &lt; 0.7), this is often acceptable as it's conservative. If Mean(z) shows significant bias, check the parity plot for systematic patterns.</p>"},{"location":"visualizations/qq_plot/#further-reading","title":"Further Reading","text":"<ul> <li>Interpreting Q-Q Plots (Educational Guide) - Comprehensive theory and examples</li> <li>Calibration Curve - Complementary coverage diagnostic</li> <li>Parity Plot - Prediction accuracy assessment</li> <li>Model Performance - Overall model quality guide</li> </ul> <p>Key Takeaway: The Q-Q plot reveals whether your model \"knows what it doesn't know.\" Well-calibrated uncertainty is as important as accurate predictions for successful Bayesian optimization.</p>"}]}