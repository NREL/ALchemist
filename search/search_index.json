{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to ALchemist","text":"<p>ALchemist is a GUI-based active learning and Bayesian optimization toolkit for chemical and materials researchers.</p> <p>Use the sidebar to navigate through the documentation.</p>"},{"location":"modeling/botorch/","title":"BoTorch Backend","text":"<p>The BoTorch backend in ALchemist allows you to train a Gaussian Process (GP) surrogate model using the BoTorch library, which is built on PyTorch and designed for scalable Bayesian optimization. BoTorch provides advanced kernel options and efficient handling of both continuous and categorical variables.</p>"},{"location":"modeling/botorch/#what-is-botorch","title":"What is BoTorch?","text":"<p>BoTorch is a flexible, research-oriented library for Bayesian optimization built on PyTorch. It was developed by Meta (Facebook) and serves as the underlying optimization engine for Ax, a high-level adaptive experimentation platform. BoTorch supports advanced features such as anisotropic kernels (automatic relevance determination, ARD) and mixed-variable spaces, and is tightly integrated with GPyTorch for scalable Gaussian process inference.</p>"},{"location":"modeling/botorch/#training-a-model-with-botorch-backend","title":"Training a Model with BoTorch Backend","text":"<p>When you select the botorch backend in the Model panel, you are training a GP model using BoTorch's <code>SingleTaskGP</code> (for continuous variables) or <code>MixedSingleTaskGP</code> (for mixed continuous/categorical variables). The workflow and options are as follows:</p>"},{"location":"modeling/botorch/#1-kernel-selection","title":"1. Kernel Selection","text":"<p>You can choose the kernel type for the continuous variables:</p> <ul> <li>Matern: Default, with a tunable smoothness parameter (<code>nu</code>).</li> <li>RBF: Radial Basis Function kernel.</li> </ul> <p>For the Matern kernel, you can select the <code>nu</code> parameter (0.5, 1.5, or 2.5), which controls the smoothness of the function.</p> <p>Note: BoTorch uses anisotropic (ARD) kernels by default, so each variable can have its own learned lengthscale. This helps preserve the physical meaning of each variable and enables automatic relevance detection. For more details, see the Kernel Deep Dive in the Educational Resources section.</p>"},{"location":"modeling/botorch/#2-categorical-variables","title":"2. Categorical Variables","text":"<ul> <li>Categorical variables are automatically detected and encoded.</li> <li>BoTorch uses the <code>MixedSingleTaskGP</code> model to handle mixed spaces, encoding categorical variables as required.</li> </ul>"},{"location":"modeling/botorch/#3-noise-handling","title":"3. Noise Handling","text":"<ul> <li>If your experimental data includes a <code>Noise</code> column, these values are used for regularization.</li> <li>If not, the model uses its internal noise estimation.</li> </ul>"},{"location":"modeling/botorch/#4-model-training-and-evaluation","title":"4. Model Training and Evaluation","text":"<ul> <li>The model is trained on your current experiment data.</li> <li>Cross-validation is performed to estimate model performance (RMSE, MAE, MAPE, R\u00b2).</li> <li>Learned kernel hyperparameters (lengthscales, outputscale, etc.) are displayed after training.</li> </ul>"},{"location":"modeling/botorch/#5-advanced-options","title":"5. Advanced Options","text":"<ul> <li>You can select the kernel type and Matern <code>nu</code> parameter in the Model panel.</li> <li>BoTorch uses sensible defaults for other training parameters.</li> </ul>"},{"location":"modeling/botorch/#how-it-works","title":"How It Works","text":"<ul> <li>The model uses your variable space and experiment data to fit a GP regression model using PyTorch.</li> <li>The trained model is used for Bayesian optimization, suggesting new experiments via acquisition functions.</li> <li>All preprocessing (encoding, noise handling) is handled automatically.</li> </ul>"},{"location":"modeling/botorch/#references","title":"References","text":"<ul> <li>BoTorch documentation</li> <li>BoTorch SingleTaskGP</li> <li>BoTorch MixedSingleTaskGP</li> <li>Ax platform documentation</li> <li>GPyTorch documentation</li> </ul> <p>For a deeper explanation of kernel selection, anisotropic kernels, and ARD, see Kernel Deep Dive in the Educational Resources section.</p> <p>For details on using the scikit-optimize backend, see the previous section.</p>"},{"location":"modeling/performance/","title":"Model Performance","text":"<p>Evaluating the performance of your surrogate model is a critical step in the active learning workflow. ALchemist provides several tools and visualizations to help you assess model quality and guide your next steps.</p>"},{"location":"modeling/performance/#cross-validation-and-error-metrics","title":"Cross-Validation and Error Metrics","text":"<p>After training a model, ALchemist automatically computes cross-validation metrics such as RMSE, MAE, MAPE, and R\u00b2. These metrics are visualized in the Visualizations dialog, where you can see how model error changes as more data points are added.</p> <p>General expectation: - As you increase the number of observations, cross-validation error (e.g., RMSE) should generally decrease. This indicates that the model is learning from the data and improving its predictions.</p>"},{"location":"modeling/performance/#what-if-error-doesnt-decrease","title":"What if Error Doesn't Decrease?","text":"<p>If you notice that error metrics do not decrease with more data, consider the following:</p> <ol> <li> <p>Small Data Regime (&lt;10 points):    With very few data points, high error or flat trends are common. This is not necessarily a problem\u2014acquisition functions will naturally suggest new experiments in regions of high uncertainty, helping the model converge as more data is collected.</p> </li> <li> <p>Try a Different Backend:    Switch between the scikit-optimize and BoTorch backends. Sometimes one backend may fit your data better, especially depending on the variable types and dimensionality.</p> </li> <li> <p>Tweak the Kernel:    Experiment with different kernel types (RBF, Matern, RationalQuadratic) or adjust the Matern <code>nu</code> parameter. The choice of kernel can significantly affect model flexibility and fit.</p> </li> </ol>"},{"location":"modeling/performance/#additional-tips-and-considerations","title":"Additional Tips and Considerations","text":"<ul> <li> <p>Overfitting:   Overfitting may appear as jagged or unrealistic response surfaces in contour plots. If you see this, try increasing regularization (e.g., by specifying higher noise values) or collecting more data.</p> </li> <li> <p>Data Quality:   Poor model performance can result from poor data quality. Check for outliers or inconsistent measurements. Consider populating the <code>Noise</code> column with an appropriate metric (such as variance or signal-to-noise ratio) to help regularize the model. See the Regularization page for more details.</p> </li> <li> <p>Model Diagnostics:   Use parity plots and error metric trends to diagnose underfitting, overfitting, or data issues. Ideally, parity plots should show points close to the diagonal (y = x), indicating good agreement between predicted and actual values.</p> </li> <li> <p>Variable Importance:   Both backends use anisotropic (ARD) kernels by default, allowing the model to learn a separate lengthscale for each variable. This can help identify which variables are most relevant to the output.</p> </li> </ul>"},{"location":"modeling/performance/#summary","title":"Summary","text":"<ul> <li>Expect error to decrease as more data is added.</li> <li>Use backend and kernel options to improve fit.</li> <li>Watch for signs of overfitting or poor data quality.</li> <li>Use regularization and noise estimates to stabilize the model.</li> </ul> <p>For more on error metrics and visualization, see the Visualizations section.</p>"},{"location":"modeling/skopt/","title":"scikit-optimize Backend","text":"<p>The scikit-optimize backend in ALchemist allows you to train a Gaussian Process (GP) surrogate model using the <code>skopt.learning.GaussianProcessRegressor</code>, which is a wrapper around <code>sklearn.gaussian_process.GaussianProcessRegressor</code> designed specifically for Bayesian optimization workflows.</p>"},{"location":"modeling/skopt/#what-is-scikit-optimize","title":"What is scikit-optimize?","text":"<p>scikit-optimize (skopt) is a library built on top of scikit-learn that provides tools for sequential model-based optimization, commonly used in Bayesian optimization. In ALchemist, the skopt backend leverages this framework to efficiently model your experimental data and suggest new experiments.</p>"},{"location":"modeling/skopt/#training-a-model-with-scikit-learn-backend","title":"Training a Model with scikit-learn Backend","text":"<p>When you select the scikit-learn backend in the Model panel, you are training a Gaussian Process model using the skopt/scikit-learn stack. The workflow and options are as follows:</p>"},{"location":"modeling/skopt/#1-kernel-selection","title":"1. Kernel Selection","text":"<p>You can choose from several kernel types for the GP:</p> <ul> <li>RBF (Radial Basis Function): Default, smooth kernel.</li> <li>Matern: Flexible kernel with a tunable smoothness parameter (<code>nu</code>).</li> <li>RationalQuadratic: Mixture of RBF kernels with varying length scales.</li> </ul> <p>For the Matern kernel, you can select the <code>nu</code> parameter (0.5, 1.5, 2.5, or \u221e), which controls the smoothness of the function.</p> <p>Note: ALchemist uses anisotropic (dimension-wise) kernels by default, so each variable can have its own learned lengthscale. This helps preserve the physical meaning of each variable and enables automatic relevance detection (ARD). For more details, see the Kernel Deep Dive in the Educational Resources section.</p>"},{"location":"modeling/skopt/#2-optimizer","title":"2. Optimizer","text":"<p>You can select the optimizer used for hyperparameter tuning:</p> <ul> <li>L-BFGS-B (default)</li> <li>CG</li> <li>BFGS</li> <li>TNC</li> </ul> <p>These control how the kernel hyperparameters are optimized during model fitting.</p>"},{"location":"modeling/skopt/#3-advanced-options","title":"3. Advanced Options","text":"<p>Enable advanced options to customize kernel and optimizer settings. By default, kernel hyperparameters are automatically optimized.</p>"},{"location":"modeling/skopt/#4-noise-handling","title":"4. Noise Handling","text":"<p>If your experimental data includes a <code>Noise</code> column, these values are used for regularization (<code>alpha</code> parameter in scikit-learn). If not, the model uses its default regularization.</p>"},{"location":"modeling/skopt/#5-one-hot-encoding","title":"5. One-Hot Encoding","text":"<p>Categorical variables are automatically one-hot encoded for compatibility with scikit-learn.</p>"},{"location":"modeling/skopt/#6-model-training-and-evaluation","title":"6. Model Training and Evaluation","text":"<ul> <li>The model is trained on your current experiment data.</li> <li>Cross-validation is performed to estimate model performance (RMSE, MAE, MAPE, R\u00b2).</li> <li>Learned kernel hyperparameters are displayed after training.</li> </ul>"},{"location":"modeling/skopt/#how-it-works","title":"How It Works","text":"<ul> <li>The model uses your variable space and experiment data to fit a GP regression model.</li> <li>The trained model is used for Bayesian optimization, suggesting new experiments via acquisition functions.</li> <li>All preprocessing (encoding, noise handling) is handled automatically.</li> </ul>"},{"location":"modeling/skopt/#references","title":"References","text":"<ul> <li>scikit-learn GaussianProcessRegressor documentation</li> <li>scikit-optimize GaussianProcessRegressor documentation</li> </ul> <p>For a deeper explanation of kernel selection, anisotropic kernels, and ARD, see Kernel Deep Dive in the Educational Resources section.</p> <p>For details on using the BoTorch backend, see the next section.</p>"},{"location":"visualizations/contour_plot/","title":"Contour Plot Visualization","text":"<p>The Contour Plot feature in ALchemist lets you visualize your surrogate model\u2019s predicted output as a 2D contour plot, providing insight into the model\u2019s response surface across your variable space. This tool is designed for interpreting model predictions, identifying trends, and creating publication-quality figures.</p>"},{"location":"visualizations/contour_plot/#how-to-create-a-contour-plot","title":"How to Create a Contour Plot","text":"<ol> <li> <p>Open the Visualizations Dialog:    After training a model, open the Visualizations dialog from the main application window.</p> </li> <li> <p>Choose X and Y Axes:    Use the dropdown menus in the \"Contour Plot Options\" panel to select which two real-valued variables to display on the X and Y axes.</p> </li> <li> <p>Set Fixed Values for Other Variables:    Set a value for each remaining variable using the provided controls. This lets you view a \u201cslice\u201d of the model\u2019s prediction at a specific cross-section.</p> </li> <li> <p>Plot the Contour:    Click Plot Contour to generate the plot for your selected axes and fixed values. The predicted output will be shown as filled contours.</p> </li> </ol>"},{"location":"visualizations/contour_plot/#customization-and-export","title":"Customization and Export","text":"<ul> <li> <p>Customize Appearance:   Click Customize Plot to adjust the plot title, axis labels, colormap, font, font size, axis limits, font weight, tick style, and whether to show experimental data points (white circles) and the next suggested point (red diamond).</p> </li> <li> <p>Save Figures:   Use the Matplotlib toolbar below the plot to save your figure as a high-resolution image (PNG, SVG, PDF, etc.) for publication or presentations.</p> </li> </ul>"},{"location":"visualizations/contour_plot/#additional-features","title":"Additional Features","text":"<ul> <li>Interactive Controls:   Changing the X or Y axis or adjusting fixed values for other variables will update the plot in real time.</li> <li>Legend:   The plot includes a legend for experimental points and the next suggested point if displayed.</li> <li>Hyperparameters Display:   The bottom of the Visualizations dialog shows the learned kernel hyperparameters from the trained model.</li> </ul>"},{"location":"visualizations/contour_plot/#tips","title":"Tips","text":"<ul> <li>Use customization options to match your figure style to journal or presentation requirements.</li> <li>Generate multiple contour plots for different variable pairs and fixed values to explore the model\u2019s predictions.</li> <li>Contour plots are useful for diagnosing model behavior, visualizing optima, and communicating results.</li> </ul> <p>For more on model evaluation and error metrics, see the Error Metrics Visualization section.</p>"},{"location":"visualizations/error_metrics/","title":"Error Metrics Visualization","text":"<p>The Visualizations dialog in ALchemist provides several tools to help you evaluate your surrogate model's performance using different error metrics. After training a model, you can access these visualizations to better understand how your model is performing as you add more experimental data.</p>"},{"location":"visualizations/error_metrics/#available-error-metrics","title":"Available Error Metrics","text":"<p>You can select and plot the following metrics in the Visualizations dialog:</p> <ul> <li>RMSE (Root Mean Squared Error):   Measures the average magnitude of prediction errors. Lower RMSE indicates better fit. Sensitive to large errors.</li> </ul> \\[ \\mathrm{RMSE} = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 } \\] <ul> <li>MAE (Mean Absolute Error):   The average of absolute differences between predicted and actual values. Less sensitive to outliers than RMSE.</li> </ul> \\[ \\mathrm{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| \\] <ul> <li>MAPE (Mean Absolute Percentage Error):   Expresses prediction error as a percentage of the actual values. Useful for comparing errors across different scales, but can be unstable if actual values are near zero.</li> </ul> \\[ \\mathrm{MAPE} = \\frac{100\\%}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| \\] <ul> <li>\\(R^2\\) (Coefficient of Determination):   Indicates how well the model explains the variance in the data. Values closer to 1 mean better fit; values near 0 or negative indicate poor fit.</li> </ul> \\[ R^2 = 1 - \\frac{ \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 }{ \\sum_{i=1}^{n} (y_i - \\bar{y})^2 } \\] <p>Where: - \\(y_i\\) = true value - \\(\\hat{y}_i\\) = predicted value - \\(\\bar{y}\\) = mean of true values - \\(n\\) = number of data points</p>"},{"location":"visualizations/error_metrics/#using-the-visualizations-dialog","title":"Using the Visualizations Dialog","text":"<ul> <li> <p>Plotting Metrics:   Use the dropdown menu at the top of the Visualizations dialog to select which error metric to plot. Click \"Plot Metrics\" to see how the chosen metric changes as more data points are added.</p> </li> <li> <p>Interpreting Trends:   Ideally, error metrics (RMSE, MAE, MAPE) should decrease as you add more observations, and \\(R^2\\) should increase. Flat or increasing error trends may indicate issues with model fit, data quality, or kernel choice.</p> </li> <li> <p>Cross-Validation:   All metrics are computed using cross-validation, providing a robust estimate of model performance on unseen data.</p> </li> <li> <p>Parity Plots:   In addition to error metrics, you can generate parity plots to visually compare predicted vs. actual values. Points close to the diagonal indicate good agreement.</p> </li> </ul>"},{"location":"visualizations/error_metrics/#general-considerations","title":"General Considerations","text":"<ul> <li>Use error metric trends to monitor model improvement as you collect more data.</li> <li>Compare different metrics for a more complete picture of model performance.</li> <li>If you see unexpected trends, refer to the Model Performance page for troubleshooting tips and deeper guidance.</li> </ul> <p>For more on interpreting these metrics and improving model performance, see the Model Performance section.</p>"},{"location":"workflow/data_visualization/","title":"Data Visualization","text":"<p>The Visualization panel in ALchemist displays a 2D scatter plot of your experiment data and search space.</p> <ul> <li> <p>Variable Selection:   Use the dropdown menus above the plot to choose which variables to display on the x and y axes.</p> </li> <li> <p>Pool Points:   Light blue, semi-transparent dots represent the pool of potential experiment points generated from your variable space.</p> </li> <li> <p>Sampled Points:   Green dots show the experiments you have already run or loaded.</p> </li> <li> <p>Suggested Next Point:   A blue diamond marker will appear after you train a model and run an acquisition function, indicating the next recommended experiment.</p> </li> </ul> <p>Currently, only 2D scatter plots are supported. Future updates may add more visualization options, such as 3D plots or advanced data views.</p>"},{"location":"workflow/initial_sampling/","title":"Generating Initial Experiments","text":"<p>When starting an active learning workflow, it's important to generate an initial set of experiments that cover your variable space efficiently. This is especially useful if you have no prior experimental data, or if your existing data was not collected with surrogate modeling in mind. Well-chosen initial points help ensure that your model converges efficiently and avoids bias from poor coverage.</p>"},{"location":"workflow/initial_sampling/#why-generate-initial-points","title":"Why Generate Initial Points?","text":"<ul> <li>No Prior Data: If you are starting from scratch, you need a set of initial experiments to train your first surrogate model.</li> <li>Supplement Existing Data: If you have some data, but it is sparse or not well-distributed, you can generate additional points to improve coverage.</li> <li>Efficient Model Convergence: Good initial coverage of the variable space helps the model learn faster and reduces the risk of missing important regions.</li> </ul>"},{"location":"workflow/initial_sampling/#how-to-generate-initial-points","title":"How to Generate Initial Points","text":"<ol> <li> <p>Load or Define Your Variable Space:    Make sure you have set up your variable space using the Variable Space Setup dialog.</p> </li> <li> <p>Open the Initial Sampling Dialog:    In the main application window, click Generate Initial Points in the Experiment Data panel.</p> </li> <li> <p>Choose a Sampling Strategy:    Select from several strategies:</p> </li> <li>Random: Uniformly samples points at random.</li> <li>LHS (Latin Hypercube Sampling): Ensures each variable is sampled evenly across its range.</li> <li> <p>Sobol, Halton, Hammersly: Quasi-random low-discrepancy sequences for more uniform coverage in high dimensions.</p> </li> <li> <p>Set the Number of Points:    Enter how many initial experiments you want to generate.</p> </li> <li> <p>Generate and Review:    Click Generate. The new points will appear in your experiment table, ready for export or further editing.</p> </li> </ol>"},{"location":"workflow/initial_sampling/#tips","title":"Tips","text":"<ul> <li>Coverage Matters: More points give better coverage, but also require more experiments. Balance your resources and modeling needs.</li> <li>Quasi-Random vs. Random: Quasi-random methods (LHS, Sobol, etc.) are generally preferred for initial sampling, especially in higher dimensions.</li> <li>Supplementing Data: You can generate initial points even if you already have some data, to fill gaps or improve distribution.</li> </ul>"},{"location":"workflow/initial_sampling/#example-workflow","title":"Example Workflow","text":"<ol> <li>Define your variable space (e.g., 3 variables: temperature, pressure, catalyst).</li> <li>Click Generate Initial Points.</li> <li>Choose LHS and set Number of Points to 10.</li> <li>Click Generate.</li> <li>Review the generated points in the experiment table and save to file if desired.</li> </ol> <p>For more details on experiment management and data loading, see the next section of the workflow documentation.</p>"},{"location":"workflow/load_data/","title":"Loading Experimental Data","text":"<p>The Experiment Data panel in ALchemist lets you load, view, and manage your experimental results. This is a key step before training surrogate models or running active learning.</p>"},{"location":"workflow/load_data/#loading-data-from-file","title":"Loading Data from File","text":"<ol> <li>Click \"Load Experiments\":    In the Experiment Data panel, click the Load Experiments button.</li> <li>Select Your File:    Choose a <code>.csv</code> file containing your experimental data. The file should have columns for each variable (matching your variable space) and an <code>Output</code> column for the measured result. Optionally, you can include a <code>Noise</code> column to specify measurement uncertainty for each point.</li> <li>Data Appears in the Table:    The loaded data will be displayed in the table. If a <code>Noise</code> column is present, it will be used for model regularization.</li> </ol> <p>Tip: If your data columns do not match the variable names or required format, you may see an error. Make sure your CSV headers match your variable space exactly.</p>"},{"location":"workflow/load_data/#adding-a-new-experiment-point","title":"Adding a New Experiment Point","text":"<p>You can add a new experiment directly from the UI:</p> <ol> <li>Click \"Add Point\":    Opens a dialog where you can enter values for each variable, the output, and (optionally) the noise.</li> <li>Fill in the Fields:    Enter values for all variables and the output. If you know the measurement uncertainty, enter it in the Noise field.</li> <li>Save &amp; Close:    Click Save &amp; Close to add the point to your experiment table. You can also choose to save the updated data to file and retrain the model immediately by checking the corresponding boxes.</li> </ol> <p>Note: - There may be issues with type compatibility (e.g., numbers being saved as strings). If you encounter problems, check your CSV file and ensure numeric columns are formatted correctly. - Sometimes, changes made directly in the table (tksheet widget) may not update the internal experiment data until you save or reload. Use the provided dialogs for best results.</p>"},{"location":"workflow/load_data/#saving-your-data","title":"Saving Your Data","text":"<ul> <li>Click \"Save Experiments\":   Saves the current experiment table to a <code>.csv</code> file.  </li> <li>Tip:   Always save your data before closing the application to avoid losing changes.</li> </ul>"},{"location":"workflow/load_data/#retraining-the-model","title":"Retraining the Model","text":"<ul> <li>When adding a new point, you can check Retrain model to automatically update the surrogate model with the new data.</li> <li>If retraining does not seem to trigger, you may need to retrain manually from the model panel.</li> </ul>"},{"location":"workflow/load_data/#known-issues-tips","title":"Known Issues &amp; Tips","text":"<ul> <li>Type Compatibility:   Data entered via the table or add-point dialog may sometimes be interpreted as strings. If you see errors or unexpected behavior, check your data types in the CSV file.</li> <li>Table Edits:   Editing data directly in the table does not always update the internal experiment manager. For reliable results, use the add-point dialog or reload your data after editing.</li> <li>Noise Column:   The noise column is optional. If present, it should be numeric. You can toggle its visibility in the Preferences menu.</li> </ul> <p>For more details on managing experiments and troubleshooting, see the rest of the workflow documentation.</p>"},{"location":"workflow/variable_space/","title":"Setting Up the Variable Space","text":"<p>The Variable Space Setup window allows you to define the variables that make up your search space for active learning experiments. This user guide explains each part of the dialog and how to use it.</p>"},{"location":"workflow/variable_space/#opening-the-variable-space-setup","title":"Opening the Variable Space Setup","text":"<ul> <li>Click the Define Search Space button in the main application window to open the setup dialog.</li> </ul>"},{"location":"workflow/variable_space/#window-overview","title":"Window Overview","text":"<p>The window is divided into two main sections:</p> <ul> <li>Left Panel: Displays a list of variables you have defined.</li> <li>Right Panel: Contains control buttons for managing variables.</li> </ul>"},{"location":"workflow/variable_space/#adding-and-editing-variables","title":"Adding and Editing Variables","text":"<p>Each variable is represented as a row with the following fields:</p> <ul> <li>Variable Name: Enter a unique name for your variable.</li> <li>Type: Choose the variable type from the dropdown:</li> <li><code>Real</code>: For continuous variables (e.g., floating-point numbers).</li> <li><code>Integer</code>: For discrete variables (e.g., whole numbers).</li> <li><code>Categorical</code>: For variables with a set of named categories.</li> </ul> <p>Depending on the type selected: - Real/Integer: Enter minimum and maximum values in the <code>Min</code> and <code>Max</code> fields. - Categorical: Click Edit Values to open a dialog where you can enter possible category values (one per row). Click Save in the dialog to confirm.</p>"},{"location":"workflow/variable_space/#managing-variable-rows","title":"Managing Variable Rows","text":"<p>Use the buttons on the right panel to manage your variable list:</p> <ul> <li>Add Variable: Add a new variable row.</li> <li>Delete Row: Remove the currently selected variable.</li> <li>Clear Row: Clear all fields in the selected row.</li> <li>Move Up/Down: Change the order of variables by moving the selected row up or down.</li> </ul> <p>Click on a row to select it; the selected row is highlighted.</p>"},{"location":"workflow/variable_space/#saving-and-loading","title":"Saving and Loading","text":"<ul> <li>Save to File: Save your variable definitions to a <code>.json</code> or <code>.csv</code> file.</li> <li>Load from File: Load variable definitions from a <code>.json</code> or <code>.csv</code> file.</li> <li>Save &amp; Close: Save your current variable space and close the window. This also updates the application's internal search space.</li> </ul>"},{"location":"workflow/variable_space/#tips","title":"Tips","text":"<ul> <li>All fields must be filled out for a variable to be valid.</li> <li>For categorical variables, you must specify at least one value.</li> <li>You can reorder variables to control their order in the search space.</li> </ul>"},{"location":"workflow/variable_space/#example","title":"Example","text":"Variable Name Type Min Max Values temperature Real 20 100 batch_size Integer 1 10 catalyst Categorical A, B, C, D <p>For more details on how the variable space is used, see the rest of the workflow documentation.</p>"}]}